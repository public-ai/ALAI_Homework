{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AfterTrasfer_model_useCollection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5DbTV6Y8_fm"
      },
      "source": [
        "## CIFAR-10, 100 학습시키기\n",
        "\n",
        "## Objective\n",
        "\n",
        "1.[CIFAR -10 Data](https://www.cs.toronto.edu/~kriz/cifar.html) 을 Convolution Neural Network 을 이용해 학습해봅니다.\n",
        "----\n",
        "![Imgur](https://i.imgur.com/yy09iLz.png)\n",
        "\n",
        "\n",
        "- loss 가 가장 작은 model 을 저장합니다.\n",
        "- 목표 accuracy 는 75% 입니다. \n",
        "​\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-syRx_WBWebm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDkOxMoyFrpv",
        "colab_type": "code",
        "outputId": "4fef2775-8bac-4134-c78f-46d1057c26f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from tensorflow.python.training import checkpoint_utils as cp\n",
        "# var_names = cp.list_variables('./ttee/model-0')\n",
        "# print(var_names)\n",
        "# print(len(var_names))\n",
        "# block_4_kernel = cp.load_variable('./ttee/model-0' , 'VGG_block-4/conv/kernel')\n",
        "# block_4_kernel_1 = cp.load_variable('./ttee/model-0' , 'VGG_block-4/conv_1/kernel')\n",
        "# print(type(block_4_kernel_1))\n",
        "\n",
        "sess2 = tf.Session()\n",
        "saver_var = tf.train.import_meta_graph('./model-16600.meta')\n",
        "graph_var = tf.get_default_graph()\n",
        "col_var = graph_var.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "print(len(col_var)) #20개나옴\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr3zu7rGUWNc",
        "colab_type": "code",
        "outputId": "ca0b6db6-2225-4a98-eba8-8f9ac4d9eb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "saver_var.restore(sess2, './model-16600')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 07:38:22.172179 139769143691136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gEk4kz-WUzV",
        "colab_type": "code",
        "outputId": "aeef6e22-1fbf-40f0-87c7-7cf1ef3d5114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "#제대로 weight들이 들어있나 확인\n",
        "for tensor_var in col_var :\n",
        "    print(tensor_var)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'VGG_block-1/conv/kernel:0' shape=(2, 2, 3, 10) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-1/conv/bias:0' shape=(10,) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-2/conv/kernel:0' shape=(2, 2, 10, 50) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-2/conv/bias:0' shape=(50,) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-3/conv/kernel:0' shape=(2, 2, 50, 100) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-3/conv/bias:0' shape=(100,) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-3/conv_1/kernel:0' shape=(2, 2, 100, 100) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-3/conv_1/bias:0' shape=(100,) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-4/conv/kernel:0' shape=(2, 2, 100, 200) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-4/conv/bias:0' shape=(200,) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-4/conv_1/kernel:0' shape=(2, 2, 200, 200) dtype=float32_ref>\n",
            "<tf.Variable 'VGG_block-4/conv_1/bias:0' shape=(200,) dtype=float32_ref>\n",
            "<tf.Variable 'FC1/kernel:0' shape=(800, 2000) dtype=float32_ref>\n",
            "<tf.Variable 'FC1/bias:0' shape=(2000,) dtype=float32_ref>\n",
            "<tf.Variable 'FC2/kernel:0' shape=(2000, 500) dtype=float32_ref>\n",
            "<tf.Variable 'FC2/bias:0' shape=(500,) dtype=float32_ref>\n",
            "<tf.Variable 'FC3/kernel:0' shape=(500, 100) dtype=float32_ref>\n",
            "<tf.Variable 'FC3/bias:0' shape=(100,) dtype=float32_ref>\n",
            "<tf.Variable 'FC4/kernel:0' shape=(100, 10) dtype=float32_ref>\n",
            "<tf.Variable 'FC4/bias:0' shape=(10,) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsWdZJPQRZ6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.constant_initializer(var_list[0]) 사용 하기위해 값 추출\n",
        "var_list = sess2.run(col_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okPBT4DkqPmu"
      },
      "source": [
        "# Load Cifar-10 dataset \n",
        " - cifar 10 dataset 을 다운로드 합니다. \n",
        " - normalize 을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8KwcN7baxvN",
        "colab_type": "code",
        "outputId": "34628bbf-51bd-4c9e-9a81-20de41c78a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "import tensorboardcolab\n",
        "#content/tensorboard\n",
        "#tbc=tensorboardcolab.TensorBoardColab(graph_path='./tensorboard')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ApD4EzRiqOGj",
        "outputId": "3e5b90d1-3de9-4116-8b14-18ad41697f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# load cifar10 dataset \n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# reshape (None, 1) -> (None)\n",
        "y_train = np.reshape(y_train, (-1))\n",
        "y_test = np.reshape(y_test, (-1))\n",
        "\n",
        "# normalization \n",
        "x_train, x_test = x_train/255. , x_test/255.\n",
        "\n",
        "# N class\n",
        "n_classes = 10\n",
        "print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n",
        "print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n",
        "print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n",
        "print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "image shape : (50000, 32, 32, 3), label shape : (50000,) \n",
            "image shape : (10000, 32, 32, 3), label shape : (10000,) \n",
            "train minimun : 0.0, train_maximum : 1.0 \n",
            "tests minimun : 0.0, test_maximum : 1.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb-_UZEUQ_N",
        "colab_type": "text"
      },
      "source": [
        "# DataProvider "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmE4_GikBI6I",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "np.random.seed(0)\n",
        "class DataProvider(object):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_fix = images # fix data저장\n",
        "        self.labels_fix = labels # fix data저장\n",
        "        self.len_ = self.images.shape[0] # 총 이미지 갯수저장\n",
        "        self.len_fix = copy.deepcopy(self.len_)  # fix길이 저장\n",
        "        self.ind_range = self.images.shape[0] # index\n",
        "        self.ind = [ x for x in range(self.ind_range)]\n",
        "        np.random.shuffle(self.ind) # index shuffle\n",
        "        self.images = self.images[self.ind, :] # shuffle 수행\n",
        "        self.labels = self.labels[self.ind]\n",
        "        self.images = list(self.images) #list화 시킴(del() 등 list연산 사용필요)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        #fix me#\n",
        "        if self.len_ <= batch_size :\n",
        "            ### 해당 epoch의 마지막 batch case ###\n",
        "            # 1.나머지 모두 내보냄\n",
        "            out_batch_image = self.images[:][:]\n",
        "            out_batch_labels = self.labels[:]\n",
        "            del(self.images[:])\n",
        "            del(self.labels[:])\n",
        "            \n",
        "            # 2.다음 epoch의 shuffle 수행\n",
        "            self.len_ = self.len_fix\n",
        "            self.images = self.images_fix\n",
        "            self.labels = self.labels_fix\n",
        "            self.ind = [x for x in range(self.ind_range)]\n",
        "            np.random.shuffle(self.ind)\n",
        "            self.images = self.images[self.ind,:] # shuffle 수행\n",
        "            self.labels = self.labels[self.ind]\n",
        "            self.images = list(self.images)\n",
        "            self.labels = list(self.labels)\n",
        "        else : \n",
        "            # 일반 batch수행\n",
        "            out_batch_image, out_batch_labels = self.images[:batch_size][:], self.labels[:batch_size] # slice함\n",
        "            del(self.images[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            del(self.labels[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            self.len_ = self.len_ - batch_size # 길이줄임\n",
        "            out_batch_labels = np.array(out_batch_labels)\n",
        "            out_batch_image = np.array(out_batch_image)\n",
        "        return out_batch_image, out_batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b0Hku8a98_fo"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "설계한 모델을 표로 작성합니다. \n",
        "\n",
        "- 목표 Receptive Field : ? <br>\n",
        "- Convolution Phase 후  출력 크기  :  ? <br>\n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| ? |?| ?|? |?  | ? |?| ?|\n",
        "\n",
        "\n",
        "- 모델 설계가 끝나면 간단한 그림을 작성해 아래에 붙여주세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDhKG-rSUQ_P",
        "colab_type": "text"
      },
      "source": [
        "예시1) \n",
        "\n",
        "\n",
        "- 목표 Receptive Field : 28 <br>\n",
        "- Convolution Phase 후  출력 크기  :  4 <br>\n",
        "- Regularization  : L2 \n",
        "- Batch size : 120\n",
        "- Learning rate : 0.0001 \n",
        "- Data normalization : min max normalization \n",
        "- Standardization : None \n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| c1 |conv| 64| 3x3| 1  | SAME | None| 32x32 |\n",
        "| s2 |max-pooling| None| 3x3| 2  | SAME | None|16x16 | \n",
        "| c3 |conv| 128| 3x3| 2  | SAME |NOne |16x16 | \n",
        "| s4 |max-pooling| None| 3x3| 2  | SAME | None|8 x8 | \n",
        "| c5 |conv| 128| 3x3| 2  | SAME | None |8 x8 | \n",
        "| s6 |conv| 256| 3x3| 2  | SAME | None |4 x 4 | \n",
        "| c7 |conv| 256| 1x1| 2  | SAME | None |4 x 4 | \n",
        "| f8 ||| | FC 256  | |  || \n",
        "| f8 ||| | Dropout 0.7 | |  || \n",
        "| f9 ||| | FC 256  | |  || \n",
        "| f9 ||| | Dropout 0.6 | |  || \n",
        "| f10||| | FC 10   | |  || \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Imgur](https://i.imgur.com/yqrIm5u.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2qtnd7UQ_Q",
        "colab_type": "text"
      },
      "source": [
        "# Convolution layer\n",
        "- convolution layer helper function 을 정의합니다.\n",
        "- 위 설계한 convolution layer 을 구현합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rzr0by6Kyv6J",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# convolution helper function\n",
        "def conv(input_xs ,units, k, s, padding, activation, ker_init, bias_init, name):\n",
        "    layer = tf.layers.Conv2D(filters = units, kernel_size = k, strides = s,\n",
        "                             padding = padding, activation = activation, use_bias=True,\n",
        "                             kernel_initializer=ker_init, bias_initializer = bias_init, name = name )(input_xs)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmKroxF-CDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "93ab3a1c-0e08-40c9-f1ce-3231b303d7d4"
      },
      "source": [
        "# define input placeholder \n",
        "xs = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 3])\n",
        "ys = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "#ys_one_hot = tf.one_hot()\n",
        "lr = tf.placeholder(dtype = tf.float32, shape = ())\n",
        "phase_train = tf.placeholder(tf.bool, shape = (), name = 'phase_train')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9670fb600e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#ys_one_hot = tf.one_hot()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mphase_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'phase_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AJcmfqHJ9lLm",
        "outputId": "f6765116-5566-456a-82e0-d311d0d7df57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Model implementation \n",
        "# convolution Neural Network \n",
        "# 자신이 설계한 모형을 구현해주세요.\n",
        "with tf.variable_scope('VGG_block-1') :\n",
        "    layer = conv(xs, 10, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[0]), tf.constant_initializer(var_list[1]), 'conv' )\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-2') :\n",
        "    layer = conv(pooling, 50, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[2]), tf.constant_initializer(var_list[3]), 'conv')\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-3') :\n",
        "    layer = conv(pooling, 100, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[4]), tf.constant_initializer(var_list[5]) ,'conv')\n",
        "    layer = conv(layer, 100, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[6]), tf.constant_initializer(var_list[7]), 'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-4') :\n",
        "    layer = conv(pooling, 200, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[8]), tf.constant_initializer(var_list[9]) ,'conv')\n",
        "    layer = conv(layer, 200, (2,2), (1,1), 'SAME', tf.nn.relu, tf.constant_initializer(var_list[10]), tf.constant_initializer(var_list[11]),'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "# with tf.variable_scope('VGG_block-5') :\n",
        "#     layer = conv(pooling, 400, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "#     layer = conv(layer,400, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')    \n",
        "#     #pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "top_conv = tf.identity(pooling, 'top_conv') # 마지막 layer 을 top conv 에 넣습니다.\n",
        "tf.shape(top_conv)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Shape:0' shape=(4,) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRukJjRZUQ_V",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layer\n",
        "- 설계한 fully connected layer 을 구현합니다.\n",
        "- dropout 을 적용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNx29qxUQ_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(flat_layer, units, initializer_k, initializer_b, layer_name):\n",
        "    dense = tf.layers.Dense(units = units, activation = tf.nn.relu,use_bias=True,\n",
        "                            kernel_initializer = initializer_k, bias_initializer = initializer_b, name = layer_name)(flat_layer)\n",
        "    return dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL1M4gvUQ_X",
        "colab_type": "code",
        "outputId": "563a5c0b-392a-42be-ba31-2cd871716a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# flat layer \n",
        "flatten_layer = tf.layers.flatten(top_conv)\n",
        "#print(\"tf.shape(flat_layer) :\", tf.shape(flatten_layer))\n",
        "\n",
        "# fully connected layer 1\n",
        "#fc_initializer = tf.initializers.he_normal() #<--여기 하는중, 여기부터 하라,, 또한 model의 가중치값을 불러오던지 하라..\n",
        "\n",
        "fc_layer_1 = fc(flat_layer = flatten_layer, units = 2000, initializer_k = tf.constant_initializer(var_list[12]), initializer_b = tf.constant_initializer(var_list[13]) , layer_name = \"FC1\" )\n",
        "fc_layer_1 = tf.layers.dropout(fc_layer_1,rate=0.5, training=phase_train)\n",
        "\n",
        "# fix me # 자신이 설계한 fully connected layer 을 구현합니다.\n",
        "fc_layer_2 = fc(flat_layer = fc_layer_1, units = 500, initializer_k = tf.constant_initializer(var_list[14]), initializer_b = tf.constant_initializer(var_list[15]), layer_name = \"FC2\" )\n",
        "fc_layer_2 = tf.layers.dropout(fc_layer_2,rate=0.5, training=phase_train)\n",
        "\n",
        "fc_layer_3 = fc(flat_layer = fc_layer_2, units = 100, initializer_k = tf.constant_initializer(var_list[16]), initializer_b = tf.constant_initializer(var_list[17]), layer_name = \"FC3\" )\n",
        "\n",
        "fc_layer_4 = fc(flat_layer = fc_layer_3, units = 10, initializer_k = tf.constant_initializer(var_list[18]), initializer_b = tf.constant_initializer(var_list[19]), layer_name = \"FC4\" )\n",
        "\n",
        "logits= tf.identity(fc_layer_4, 'logits')\n",
        "\n",
        "ys = tf.cast(ys, tf.int32)\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(ys, logits)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0618 07:38:39.977081 139769143691136 deprecation.py:323] From <ipython-input-13-3c36e0e311d8>:1: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0618 07:38:40.567987 139769143691136 deprecation.py:323] From <ipython-input-13-3c36e0e311d8>:8: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0618 07:38:40.736547 139769143691136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-625ULtUQ_Z",
        "colab_type": "text"
      },
      "source": [
        "#  Loss function \n",
        "- loss function 을 정의합니다. L2 regularization 을 사용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2DkXG-UQ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_reg = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n",
        "l2_beta = 5e-4\n",
        "\n",
        "#loss \n",
        "# L2 reularization \n",
        "loss = loss + (l2_beta * l2_reg)\n",
        "loss = tf.identity(loss, name = 'loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Oke8OgUQ_e",
        "colab_type": "text"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_TMXVtX1pXK",
        "colab": {}
      },
      "source": [
        "# metric\n",
        "pred = tf.nn.softmax(logits)\n",
        "one_hot_label = tf.one_hot(ys, 10)\n",
        "pred_arg = tf.argmax(pred, axis = 1)\n",
        "label_arg = tf.argmax(one_hot_label, axis = 1)\n",
        "eq = tf.cast(tf.equal(pred_arg, label_arg), dtype = tf.float32)\n",
        "acc = tf.reduce_mean(eq, axis =0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfoWSrLUQ_h",
        "colab_type": "text"
      },
      "source": [
        "# Add tensor to Tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqUXVsUTUQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add accuracy to tensorboard nodes \n",
        "#fix me #\n",
        "acc_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "# add loss to tensorboard nodes \n",
        "#fix me #\n",
        "loss_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "\n",
        "#merge all tensorboard nodes \n",
        "#fix me #\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGeaVRCOUQ_j",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83C5s7mfUQ_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_op : adamoptimizer \n",
        "#train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
        "#train_op = tf.train.MomentumOptimizer(lr, momentum=0.9).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6N1fBH2BBfX6"
      },
      "source": [
        "# Session open "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wiz-2rkLBdTw",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "optimizer = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "#초기학습\n",
        "init_g = tf.global_variables_initializer() # : globalal initializer\n",
        "init_l = tf.local_variables_initializer() # : local initializer\n",
        "#sess.run(init_g)\n",
        "sess.run([init_l,init_g])\n",
        "#sess.run(tf.variables_initializer(optimizer.variables()))\n",
        "# saver \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "#Weight Transfer / restore\n",
        "#saver.restore(sess, './ttee/model-16600')\n",
        "# optimizer = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
        "# train_op = optimizer.minimize(loss)\n",
        "#sess.run(tf.variables_initializer(optimizer.variables()))\n",
        "#print(sess.run(optimizer.variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFmQDdJDUQ_o",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard Filewriter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPH7CLomUQ_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorboard \n",
        "train_writer=tf.summary.FileWriter(logdir='./tensorboard/train')\n",
        "\n",
        "test_writer=tf.summary.FileWriter(logdir='./tensorboard/test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P88ef2rRUQ_q",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yv99AR4A_Y7P",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6250
        },
        "outputId": "205ad1e1-4e9e-4901-d53c-289e986677a8"
      },
      "source": [
        "dataprovider = DataProvider(images=x_train, labels=y_train)\n",
        "#save_root_folder = #fix me # : models saved folder \n",
        "\n",
        "# hparam \n",
        "batch_size = 100\n",
        "min_loss = 1000000.0\n",
        "learning_rate = 0.00001 # val loss 발산때마다 1/10씩 줄여나감\n",
        "\n",
        "np.random.seed(0)\n",
        "#local variable initialize\n",
        "for i in range(50000):\n",
        "    batch_xs, batch_ys = dataprovider.next_batch(batch_size)\n",
        "    # training \n",
        "    _= sess.run(train_op, feed_dict = {xs : batch_xs,\n",
        "                                        ys : batch_ys,\n",
        "                                        lr : learning_rate,\n",
        "                                        phase_train : True})\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        # Validate validation dataset \n",
        "        fetches=[loss, acc, merged]\n",
        "        val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n",
        "                                                                      ys : y_test,\n",
        "                                                                      phase_train : False})\n",
        "\n",
        "        # Validate train dataset : extract randomly 10000 samples from train dataset \n",
        "        ran = [ x for x in range(0, 50000)]\n",
        "        nansu = np.random.choice(ran, size = 10000, replace=False)\n",
        "        train_xtest, train_ytest = x_train[nansu], y_train[nansu]\n",
        "        train_loss, train_acc, train_merged = sess.run([loss, acc, merged], feed_dict = { xs : train_xtest,\n",
        "                                                                                          ys : train_ytest,\n",
        "                                                                                        phase_train : False})\n",
        "       \n",
        "        print('step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'.\\\n",
        "        format(i, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "        # Save Model \n",
        "        if val_loss < min_loss : #fix me # : when val_loss < min_loss \n",
        "            min_loss = val_loss\n",
        "            save_path = './ttee/model'\n",
        "            saver.save(sess, save_path, global_step=i)\n",
        "            print('model save!')\n",
        "            \n",
        "        # Add values to tensorboard \n",
        "        train_writer.add_summary(train_merged, i)\n",
        "        test_writer.add_summary(val_merged, i)\n",
        "        train_writer.flush()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 0 train loss : 0.7688 acc : 0.8141 | Val loss : 1.0729 acc : 0.7032\n",
            "model save!\n",
            "step : 100 train loss : 0.7560 acc : 0.8116 | Val loss : 1.0626 acc : 0.7078\n",
            "model save!\n",
            "step : 200 train loss : 0.7674 acc : 0.8130 | Val loss : 1.0612 acc : 0.7091\n",
            "model save!\n",
            "step : 300 train loss : 0.7458 acc : 0.8220 | Val loss : 1.0609 acc : 0.7091\n",
            "model save!\n",
            "step : 400 train loss : 0.7588 acc : 0.8115 | Val loss : 1.0614 acc : 0.7074\n",
            "step : 500 train loss : 0.7639 acc : 0.8127 | Val loss : 1.0612 acc : 0.7077\n",
            "step : 600 train loss : 0.7505 acc : 0.8167 | Val loss : 1.0592 acc : 0.7091\n",
            "model save!\n",
            "step : 700 train loss : 0.7479 acc : 0.8182 | Val loss : 1.0602 acc : 0.7093\n",
            "step : 800 train loss : 0.7431 acc : 0.8211 | Val loss : 1.0596 acc : 0.7095\n",
            "step : 900 train loss : 0.7519 acc : 0.8163 | Val loss : 1.0593 acc : 0.7110\n",
            "step : 1000 train loss : 0.7505 acc : 0.8151 | Val loss : 1.0623 acc : 0.7086\n",
            "step : 1100 train loss : 0.7474 acc : 0.8184 | Val loss : 1.0597 acc : 0.7096\n",
            "step : 1200 train loss : 0.7405 acc : 0.8221 | Val loss : 1.0599 acc : 0.7094\n",
            "step : 1300 train loss : 0.7542 acc : 0.8180 | Val loss : 1.0616 acc : 0.7076\n",
            "step : 1400 train loss : 0.7428 acc : 0.8215 | Val loss : 1.0603 acc : 0.7101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0618 07:39:40.553194 139769143691136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step : 1500 train loss : 0.7463 acc : 0.8199 | Val loss : 1.0591 acc : 0.7104\n",
            "model save!\n",
            "step : 1600 train loss : 0.7509 acc : 0.8144 | Val loss : 1.0602 acc : 0.7097\n",
            "step : 1700 train loss : 0.7314 acc : 0.8244 | Val loss : 1.0623 acc : 0.7077\n",
            "step : 1800 train loss : 0.7399 acc : 0.8205 | Val loss : 1.0598 acc : 0.7097\n",
            "step : 1900 train loss : 0.7525 acc : 0.8168 | Val loss : 1.0608 acc : 0.7101\n",
            "step : 2000 train loss : 0.7349 acc : 0.8261 | Val loss : 1.0615 acc : 0.7095\n",
            "step : 2100 train loss : 0.7487 acc : 0.8184 | Val loss : 1.0631 acc : 0.7075\n",
            "step : 2200 train loss : 0.7489 acc : 0.8156 | Val loss : 1.0633 acc : 0.7085\n",
            "step : 2300 train loss : 0.7347 acc : 0.8226 | Val loss : 1.0598 acc : 0.7095\n",
            "step : 2400 train loss : 0.7395 acc : 0.8215 | Val loss : 1.0610 acc : 0.7095\n",
            "step : 2500 train loss : 0.7315 acc : 0.8282 | Val loss : 1.0609 acc : 0.7117\n",
            "step : 2600 train loss : 0.7427 acc : 0.8197 | Val loss : 1.0621 acc : 0.7098\n",
            "step : 2700 train loss : 0.7512 acc : 0.8157 | Val loss : 1.0642 acc : 0.7077\n",
            "step : 2800 train loss : 0.7328 acc : 0.8250 | Val loss : 1.0623 acc : 0.7117\n",
            "step : 2900 train loss : 0.7465 acc : 0.8213 | Val loss : 1.0605 acc : 0.7106\n",
            "step : 3000 train loss : 0.7474 acc : 0.8164 | Val loss : 1.0604 acc : 0.7108\n",
            "step : 3100 train loss : 0.7557 acc : 0.8194 | Val loss : 1.0609 acc : 0.7108\n",
            "step : 3200 train loss : 0.7370 acc : 0.8246 | Val loss : 1.0627 acc : 0.7088\n",
            "step : 3300 train loss : 0.7379 acc : 0.8213 | Val loss : 1.0634 acc : 0.7088\n",
            "step : 3400 train loss : 0.7475 acc : 0.8196 | Val loss : 1.0621 acc : 0.7109\n",
            "step : 3500 train loss : 0.7467 acc : 0.8223 | Val loss : 1.0639 acc : 0.7096\n",
            "step : 3600 train loss : 0.7478 acc : 0.8168 | Val loss : 1.0617 acc : 0.7105\n",
            "step : 3700 train loss : 0.7484 acc : 0.8192 | Val loss : 1.0622 acc : 0.7116\n",
            "step : 3800 train loss : 0.7457 acc : 0.8196 | Val loss : 1.0616 acc : 0.7103\n",
            "step : 3900 train loss : 0.7415 acc : 0.8207 | Val loss : 1.0607 acc : 0.7120\n",
            "step : 4000 train loss : 0.7342 acc : 0.8237 | Val loss : 1.0611 acc : 0.7110\n",
            "step : 4100 train loss : 0.7388 acc : 0.8202 | Val loss : 1.0635 acc : 0.7103\n",
            "step : 4200 train loss : 0.7223 acc : 0.8289 | Val loss : 1.0622 acc : 0.7114\n",
            "step : 4300 train loss : 0.7425 acc : 0.8231 | Val loss : 1.0613 acc : 0.7119\n",
            "step : 4400 train loss : 0.7415 acc : 0.8221 | Val loss : 1.0633 acc : 0.7089\n",
            "step : 4500 train loss : 0.7322 acc : 0.8202 | Val loss : 1.0630 acc : 0.7115\n",
            "step : 4600 train loss : 0.7420 acc : 0.8229 | Val loss : 1.0621 acc : 0.7115\n",
            "step : 4700 train loss : 0.7480 acc : 0.8199 | Val loss : 1.0626 acc : 0.7104\n",
            "step : 4800 train loss : 0.7326 acc : 0.8260 | Val loss : 1.0617 acc : 0.7122\n",
            "step : 4900 train loss : 0.7348 acc : 0.8216 | Val loss : 1.0672 acc : 0.7069\n",
            "step : 5000 train loss : 0.7378 acc : 0.8199 | Val loss : 1.0615 acc : 0.7123\n",
            "step : 5100 train loss : 0.7362 acc : 0.8201 | Val loss : 1.0624 acc : 0.7119\n",
            "step : 5200 train loss : 0.7370 acc : 0.8236 | Val loss : 1.0616 acc : 0.7116\n",
            "step : 5300 train loss : 0.7217 acc : 0.8299 | Val loss : 1.0631 acc : 0.7110\n",
            "step : 5400 train loss : 0.7444 acc : 0.8237 | Val loss : 1.0662 acc : 0.7104\n",
            "step : 5500 train loss : 0.7356 acc : 0.8248 | Val loss : 1.0633 acc : 0.7100\n",
            "step : 5600 train loss : 0.7413 acc : 0.8201 | Val loss : 1.0633 acc : 0.7117\n",
            "step : 5700 train loss : 0.7250 acc : 0.8280 | Val loss : 1.0645 acc : 0.7109\n",
            "step : 5800 train loss : 0.7502 acc : 0.8165 | Val loss : 1.0639 acc : 0.7121\n",
            "step : 5900 train loss : 0.7445 acc : 0.8163 | Val loss : 1.0674 acc : 0.7091\n",
            "step : 6000 train loss : 0.7310 acc : 0.8265 | Val loss : 1.0638 acc : 0.7099\n",
            "step : 6100 train loss : 0.7336 acc : 0.8212 | Val loss : 1.0629 acc : 0.7103\n",
            "step : 6200 train loss : 0.7331 acc : 0.8287 | Val loss : 1.0661 acc : 0.7086\n",
            "step : 6300 train loss : 0.7396 acc : 0.8230 | Val loss : 1.0662 acc : 0.7067\n",
            "step : 6400 train loss : 0.7375 acc : 0.8215 | Val loss : 1.0635 acc : 0.7111\n",
            "step : 6500 train loss : 0.7363 acc : 0.8231 | Val loss : 1.0650 acc : 0.7089\n",
            "step : 6600 train loss : 0.7375 acc : 0.8250 | Val loss : 1.0635 acc : 0.7102\n",
            "step : 6700 train loss : 0.7613 acc : 0.8124 | Val loss : 1.0634 acc : 0.7119\n",
            "step : 6800 train loss : 0.7321 acc : 0.8199 | Val loss : 1.0652 acc : 0.7084\n",
            "step : 6900 train loss : 0.7357 acc : 0.8213 | Val loss : 1.0647 acc : 0.7099\n",
            "step : 7000 train loss : 0.7340 acc : 0.8185 | Val loss : 1.0639 acc : 0.7117\n",
            "step : 7100 train loss : 0.7297 acc : 0.8266 | Val loss : 1.0637 acc : 0.7115\n",
            "step : 7200 train loss : 0.7265 acc : 0.8240 | Val loss : 1.0640 acc : 0.7115\n",
            "step : 7300 train loss : 0.7290 acc : 0.8288 | Val loss : 1.0651 acc : 0.7107\n",
            "step : 7400 train loss : 0.7368 acc : 0.8261 | Val loss : 1.0641 acc : 0.7100\n",
            "step : 7500 train loss : 0.7504 acc : 0.8153 | Val loss : 1.0668 acc : 0.7112\n",
            "step : 7600 train loss : 0.7535 acc : 0.8178 | Val loss : 1.0693 acc : 0.7108\n",
            "step : 7700 train loss : 0.7158 acc : 0.8282 | Val loss : 1.0668 acc : 0.7108\n",
            "step : 7800 train loss : 0.7275 acc : 0.8285 | Val loss : 1.0648 acc : 0.7112\n",
            "step : 7900 train loss : 0.7311 acc : 0.8212 | Val loss : 1.0642 acc : 0.7118\n",
            "step : 8000 train loss : 0.7360 acc : 0.8226 | Val loss : 1.0639 acc : 0.7118\n",
            "step : 8100 train loss : 0.7423 acc : 0.8195 | Val loss : 1.0648 acc : 0.7118\n",
            "step : 8200 train loss : 0.7212 acc : 0.8240 | Val loss : 1.0660 acc : 0.7121\n",
            "step : 8300 train loss : 0.7254 acc : 0.8254 | Val loss : 1.0656 acc : 0.7112\n",
            "step : 8400 train loss : 0.7312 acc : 0.8272 | Val loss : 1.0640 acc : 0.7114\n",
            "step : 8500 train loss : 0.7312 acc : 0.8257 | Val loss : 1.0655 acc : 0.7081\n",
            "step : 8600 train loss : 0.7199 acc : 0.8294 | Val loss : 1.0664 acc : 0.7080\n",
            "step : 8700 train loss : 0.7416 acc : 0.8211 | Val loss : 1.0650 acc : 0.7110\n",
            "step : 8800 train loss : 0.7252 acc : 0.8241 | Val loss : 1.0672 acc : 0.7110\n",
            "step : 8900 train loss : 0.7215 acc : 0.8296 | Val loss : 1.0679 acc : 0.7108\n",
            "step : 9000 train loss : 0.7354 acc : 0.8191 | Val loss : 1.0649 acc : 0.7123\n",
            "step : 9100 train loss : 0.7366 acc : 0.8248 | Val loss : 1.0676 acc : 0.7081\n",
            "step : 9200 train loss : 0.7264 acc : 0.8205 | Val loss : 1.0639 acc : 0.7132\n",
            "step : 9300 train loss : 0.7307 acc : 0.8245 | Val loss : 1.0654 acc : 0.7112\n",
            "step : 9400 train loss : 0.7307 acc : 0.8249 | Val loss : 1.0675 acc : 0.7126\n",
            "step : 9500 train loss : 0.7356 acc : 0.8217 | Val loss : 1.0657 acc : 0.7120\n",
            "step : 9600 train loss : 0.7243 acc : 0.8243 | Val loss : 1.0650 acc : 0.7107\n",
            "step : 9700 train loss : 0.7272 acc : 0.8212 | Val loss : 1.0663 acc : 0.7108\n",
            "step : 9800 train loss : 0.7291 acc : 0.8288 | Val loss : 1.0644 acc : 0.7120\n",
            "step : 9900 train loss : 0.7321 acc : 0.8214 | Val loss : 1.0661 acc : 0.7122\n",
            "step : 10000 train loss : 0.7349 acc : 0.8234 | Val loss : 1.0661 acc : 0.7132\n",
            "step : 10100 train loss : 0.7362 acc : 0.8254 | Val loss : 1.0661 acc : 0.7114\n",
            "step : 10200 train loss : 0.7291 acc : 0.8244 | Val loss : 1.0661 acc : 0.7104\n",
            "step : 10300 train loss : 0.7295 acc : 0.8290 | Val loss : 1.0648 acc : 0.7129\n",
            "step : 10400 train loss : 0.7228 acc : 0.8257 | Val loss : 1.0669 acc : 0.7103\n",
            "step : 10500 train loss : 0.7332 acc : 0.8226 | Val loss : 1.0655 acc : 0.7108\n",
            "step : 10600 train loss : 0.7378 acc : 0.8232 | Val loss : 1.0664 acc : 0.7111\n",
            "step : 10700 train loss : 0.7124 acc : 0.8292 | Val loss : 1.0654 acc : 0.7124\n",
            "step : 10800 train loss : 0.7231 acc : 0.8226 | Val loss : 1.0645 acc : 0.7132\n",
            "step : 10900 train loss : 0.7369 acc : 0.8236 | Val loss : 1.0669 acc : 0.7111\n",
            "step : 11000 train loss : 0.7176 acc : 0.8321 | Val loss : 1.0671 acc : 0.7127\n",
            "step : 11100 train loss : 0.7252 acc : 0.8252 | Val loss : 1.0670 acc : 0.7112\n",
            "step : 11200 train loss : 0.7349 acc : 0.8228 | Val loss : 1.0657 acc : 0.7116\n",
            "step : 11300 train loss : 0.7343 acc : 0.8256 | Val loss : 1.0667 acc : 0.7117\n",
            "step : 11400 train loss : 0.7348 acc : 0.8174 | Val loss : 1.0669 acc : 0.7131\n",
            "step : 11500 train loss : 0.7360 acc : 0.8199 | Val loss : 1.0701 acc : 0.7094\n",
            "step : 11600 train loss : 0.7302 acc : 0.8261 | Val loss : 1.0673 acc : 0.7085\n",
            "step : 11700 train loss : 0.7195 acc : 0.8298 | Val loss : 1.0686 acc : 0.7111\n",
            "step : 11800 train loss : 0.7217 acc : 0.8227 | Val loss : 1.0661 acc : 0.7114\n",
            "step : 11900 train loss : 0.7175 acc : 0.8298 | Val loss : 1.0657 acc : 0.7108\n",
            "step : 12000 train loss : 0.7390 acc : 0.8221 | Val loss : 1.0656 acc : 0.7115\n",
            "step : 12100 train loss : 0.7370 acc : 0.8174 | Val loss : 1.0661 acc : 0.7126\n",
            "step : 12200 train loss : 0.7343 acc : 0.8222 | Val loss : 1.0669 acc : 0.7108\n",
            "step : 12300 train loss : 0.7226 acc : 0.8261 | Val loss : 1.0678 acc : 0.7116\n",
            "step : 12400 train loss : 0.7277 acc : 0.8276 | Val loss : 1.0665 acc : 0.7119\n",
            "step : 12500 train loss : 0.7228 acc : 0.8296 | Val loss : 1.0663 acc : 0.7111\n",
            "step : 12600 train loss : 0.7244 acc : 0.8225 | Val loss : 1.0667 acc : 0.7123\n",
            "step : 12700 train loss : 0.7246 acc : 0.8248 | Val loss : 1.0665 acc : 0.7136\n",
            "step : 12800 train loss : 0.7252 acc : 0.8250 | Val loss : 1.0674 acc : 0.7122\n",
            "step : 12900 train loss : 0.7323 acc : 0.8269 | Val loss : 1.0674 acc : 0.7117\n",
            "step : 13000 train loss : 0.7429 acc : 0.8182 | Val loss : 1.0674 acc : 0.7131\n",
            "step : 13100 train loss : 0.7304 acc : 0.8248 | Val loss : 1.0699 acc : 0.7102\n",
            "step : 13200 train loss : 0.7180 acc : 0.8317 | Val loss : 1.0655 acc : 0.7122\n",
            "step : 13300 train loss : 0.7254 acc : 0.8259 | Val loss : 1.0694 acc : 0.7120\n",
            "step : 13400 train loss : 0.7299 acc : 0.8241 | Val loss : 1.0675 acc : 0.7118\n",
            "step : 13500 train loss : 0.7234 acc : 0.8248 | Val loss : 1.0681 acc : 0.7104\n",
            "step : 13600 train loss : 0.7300 acc : 0.8274 | Val loss : 1.0692 acc : 0.7098\n",
            "step : 13700 train loss : 0.7459 acc : 0.8180 | Val loss : 1.0717 acc : 0.7080\n",
            "step : 13800 train loss : 0.7006 acc : 0.8358 | Val loss : 1.0674 acc : 0.7104\n",
            "step : 13900 train loss : 0.7227 acc : 0.8295 | Val loss : 1.0686 acc : 0.7105\n",
            "step : 14000 train loss : 0.7390 acc : 0.8206 | Val loss : 1.0679 acc : 0.7116\n",
            "step : 14100 train loss : 0.7397 acc : 0.8235 | Val loss : 1.0686 acc : 0.7117\n",
            "step : 14200 train loss : 0.7377 acc : 0.8210 | Val loss : 1.0670 acc : 0.7129\n",
            "step : 14300 train loss : 0.7215 acc : 0.8261 | Val loss : 1.0682 acc : 0.7124\n",
            "step : 14400 train loss : 0.7251 acc : 0.8265 | Val loss : 1.0699 acc : 0.7087\n",
            "step : 14500 train loss : 0.7301 acc : 0.8236 | Val loss : 1.0680 acc : 0.7121\n",
            "step : 14600 train loss : 0.7270 acc : 0.8256 | Val loss : 1.0709 acc : 0.7113\n",
            "step : 14700 train loss : 0.7114 acc : 0.8254 | Val loss : 1.0675 acc : 0.7125\n",
            "step : 14800 train loss : 0.7124 acc : 0.8295 | Val loss : 1.0675 acc : 0.7110\n",
            "step : 14900 train loss : 0.7180 acc : 0.8285 | Val loss : 1.0670 acc : 0.7126\n",
            "step : 15000 train loss : 0.7388 acc : 0.8254 | Val loss : 1.0682 acc : 0.7104\n",
            "step : 15100 train loss : 0.7186 acc : 0.8302 | Val loss : 1.0663 acc : 0.7128\n",
            "step : 15200 train loss : 0.7053 acc : 0.8332 | Val loss : 1.0702 acc : 0.7091\n",
            "step : 15300 train loss : 0.7286 acc : 0.8221 | Val loss : 1.0683 acc : 0.7121\n",
            "step : 15400 train loss : 0.7213 acc : 0.8268 | Val loss : 1.0678 acc : 0.7122\n",
            "step : 15500 train loss : 0.7128 acc : 0.8311 | Val loss : 1.0666 acc : 0.7126\n",
            "step : 15600 train loss : 0.7288 acc : 0.8223 | Val loss : 1.0713 acc : 0.7096\n",
            "step : 15700 train loss : 0.7166 acc : 0.8287 | Val loss : 1.0679 acc : 0.7115\n",
            "step : 15800 train loss : 0.7224 acc : 0.8263 | Val loss : 1.0685 acc : 0.7102\n",
            "step : 15900 train loss : 0.7179 acc : 0.8309 | Val loss : 1.0693 acc : 0.7111\n",
            "step : 16000 train loss : 0.7323 acc : 0.8225 | Val loss : 1.0681 acc : 0.7119\n",
            "step : 16100 train loss : 0.7343 acc : 0.8229 | Val loss : 1.0725 acc : 0.7091\n",
            "step : 16200 train loss : 0.7125 acc : 0.8339 | Val loss : 1.0670 acc : 0.7135\n",
            "step : 16300 train loss : 0.7220 acc : 0.8290 | Val loss : 1.0682 acc : 0.7127\n",
            "step : 16400 train loss : 0.7336 acc : 0.8229 | Val loss : 1.0688 acc : 0.7093\n",
            "step : 16500 train loss : 0.7322 acc : 0.8229 | Val loss : 1.0715 acc : 0.7090\n",
            "step : 16600 train loss : 0.7108 acc : 0.8299 | Val loss : 1.0724 acc : 0.7102\n",
            "step : 16700 train loss : 0.7228 acc : 0.8313 | Val loss : 1.0684 acc : 0.7112\n",
            "step : 16800 train loss : 0.7228 acc : 0.8286 | Val loss : 1.0695 acc : 0.7107\n",
            "step : 16900 train loss : 0.7137 acc : 0.8318 | Val loss : 1.0692 acc : 0.7122\n",
            "step : 17000 train loss : 0.7387 acc : 0.8210 | Val loss : 1.0714 acc : 0.7116\n",
            "step : 17100 train loss : 0.7285 acc : 0.8218 | Val loss : 1.0683 acc : 0.7103\n",
            "step : 17200 train loss : 0.7272 acc : 0.8237 | Val loss : 1.0690 acc : 0.7131\n",
            "step : 17300 train loss : 0.7321 acc : 0.8247 | Val loss : 1.0679 acc : 0.7108\n",
            "step : 17400 train loss : 0.7285 acc : 0.8236 | Val loss : 1.0681 acc : 0.7132\n",
            "step : 17500 train loss : 0.7217 acc : 0.8295 | Val loss : 1.0718 acc : 0.7126\n",
            "step : 17600 train loss : 0.7200 acc : 0.8273 | Val loss : 1.0701 acc : 0.7102\n",
            "step : 17700 train loss : 0.7186 acc : 0.8308 | Val loss : 1.0705 acc : 0.7102\n",
            "step : 17800 train loss : 0.7198 acc : 0.8234 | Val loss : 1.0697 acc : 0.7109\n",
            "step : 17900 train loss : 0.7354 acc : 0.8228 | Val loss : 1.0725 acc : 0.7124\n",
            "step : 18000 train loss : 0.7372 acc : 0.8218 | Val loss : 1.0705 acc : 0.7116\n",
            "step : 18100 train loss : 0.7138 acc : 0.8294 | Val loss : 1.0701 acc : 0.7083\n",
            "step : 18200 train loss : 0.7203 acc : 0.8291 | Val loss : 1.0697 acc : 0.7115\n",
            "step : 18300 train loss : 0.7200 acc : 0.8295 | Val loss : 1.0681 acc : 0.7131\n",
            "step : 18400 train loss : 0.7087 acc : 0.8311 | Val loss : 1.0697 acc : 0.7132\n",
            "step : 18500 train loss : 0.7150 acc : 0.8290 | Val loss : 1.0690 acc : 0.7124\n",
            "step : 18600 train loss : 0.7214 acc : 0.8250 | Val loss : 1.0757 acc : 0.7107\n",
            "step : 18700 train loss : 0.7100 acc : 0.8310 | Val loss : 1.0697 acc : 0.7119\n",
            "step : 18800 train loss : 0.7365 acc : 0.8234 | Val loss : 1.0694 acc : 0.7125\n",
            "step : 18900 train loss : 0.7164 acc : 0.8274 | Val loss : 1.0699 acc : 0.7106\n",
            "step : 19000 train loss : 0.7322 acc : 0.8235 | Val loss : 1.0712 acc : 0.7088\n",
            "step : 19100 train loss : 0.7168 acc : 0.8276 | Val loss : 1.0712 acc : 0.7106\n",
            "step : 19200 train loss : 0.7178 acc : 0.8269 | Val loss : 1.0700 acc : 0.7120\n",
            "step : 19300 train loss : 0.7175 acc : 0.8283 | Val loss : 1.0729 acc : 0.7109\n",
            "step : 19400 train loss : 0.7193 acc : 0.8257 | Val loss : 1.0700 acc : 0.7129\n",
            "step : 19500 train loss : 0.7204 acc : 0.8261 | Val loss : 1.0692 acc : 0.7113\n",
            "step : 19600 train loss : 0.7249 acc : 0.8250 | Val loss : 1.0698 acc : 0.7123\n",
            "step : 19700 train loss : 0.7329 acc : 0.8225 | Val loss : 1.0691 acc : 0.7135\n",
            "step : 19800 train loss : 0.7316 acc : 0.8243 | Val loss : 1.0707 acc : 0.7100\n",
            "step : 19900 train loss : 0.7222 acc : 0.8252 | Val loss : 1.0693 acc : 0.7119\n",
            "step : 20000 train loss : 0.7314 acc : 0.8260 | Val loss : 1.0717 acc : 0.7104\n",
            "step : 20100 train loss : 0.7211 acc : 0.8323 | Val loss : 1.0706 acc : 0.7105\n",
            "step : 20200 train loss : 0.7122 acc : 0.8308 | Val loss : 1.0721 acc : 0.7113\n",
            "step : 20300 train loss : 0.7260 acc : 0.8266 | Val loss : 1.0691 acc : 0.7120\n",
            "step : 20400 train loss : 0.7277 acc : 0.8224 | Val loss : 1.0700 acc : 0.7122\n",
            "step : 20500 train loss : 0.7201 acc : 0.8286 | Val loss : 1.0706 acc : 0.7123\n",
            "step : 20600 train loss : 0.7225 acc : 0.8301 | Val loss : 1.0717 acc : 0.7112\n",
            "step : 20700 train loss : 0.7049 acc : 0.8317 | Val loss : 1.0712 acc : 0.7118\n",
            "step : 20800 train loss : 0.7230 acc : 0.8219 | Val loss : 1.0705 acc : 0.7118\n",
            "step : 20900 train loss : 0.7245 acc : 0.8286 | Val loss : 1.0716 acc : 0.7104\n",
            "step : 21000 train loss : 0.7267 acc : 0.8262 | Val loss : 1.0699 acc : 0.7095\n",
            "step : 21100 train loss : 0.7081 acc : 0.8326 | Val loss : 1.0694 acc : 0.7133\n",
            "step : 21200 train loss : 0.7111 acc : 0.8304 | Val loss : 1.0735 acc : 0.7099\n",
            "step : 21300 train loss : 0.7099 acc : 0.8338 | Val loss : 1.0742 acc : 0.7103\n",
            "step : 21400 train loss : 0.7077 acc : 0.8306 | Val loss : 1.0748 acc : 0.7081\n",
            "step : 21500 train loss : 0.7242 acc : 0.8249 | Val loss : 1.0693 acc : 0.7120\n",
            "step : 21600 train loss : 0.7164 acc : 0.8308 | Val loss : 1.0708 acc : 0.7125\n",
            "step : 21700 train loss : 0.7263 acc : 0.8217 | Val loss : 1.0720 acc : 0.7115\n",
            "step : 21800 train loss : 0.7060 acc : 0.8331 | Val loss : 1.0711 acc : 0.7124\n",
            "step : 21900 train loss : 0.7163 acc : 0.8275 | Val loss : 1.0716 acc : 0.7112\n",
            "step : 22000 train loss : 0.7118 acc : 0.8319 | Val loss : 1.0728 acc : 0.7093\n",
            "step : 22100 train loss : 0.7270 acc : 0.8235 | Val loss : 1.0732 acc : 0.7098\n",
            "step : 22200 train loss : 0.7223 acc : 0.8300 | Val loss : 1.0738 acc : 0.7104\n",
            "step : 22300 train loss : 0.7244 acc : 0.8248 | Val loss : 1.0722 acc : 0.7106\n",
            "step : 22400 train loss : 0.7131 acc : 0.8318 | Val loss : 1.0718 acc : 0.7137\n",
            "step : 22500 train loss : 0.7222 acc : 0.8236 | Val loss : 1.0729 acc : 0.7107\n",
            "step : 22600 train loss : 0.7327 acc : 0.8224 | Val loss : 1.0723 acc : 0.7125\n",
            "step : 22700 train loss : 0.7174 acc : 0.8311 | Val loss : 1.0725 acc : 0.7114\n",
            "step : 22800 train loss : 0.7135 acc : 0.8303 | Val loss : 1.0706 acc : 0.7125\n",
            "step : 22900 train loss : 0.7186 acc : 0.8271 | Val loss : 1.0766 acc : 0.7107\n",
            "step : 23000 train loss : 0.7138 acc : 0.8277 | Val loss : 1.0733 acc : 0.7122\n",
            "step : 23100 train loss : 0.7183 acc : 0.8322 | Val loss : 1.0731 acc : 0.7113\n",
            "step : 23200 train loss : 0.7182 acc : 0.8298 | Val loss : 1.0711 acc : 0.7113\n",
            "step : 23300 train loss : 0.7150 acc : 0.8298 | Val loss : 1.0741 acc : 0.7100\n",
            "step : 23400 train loss : 0.7248 acc : 0.8268 | Val loss : 1.0732 acc : 0.7121\n",
            "step : 23500 train loss : 0.7086 acc : 0.8341 | Val loss : 1.0738 acc : 0.7124\n",
            "step : 23600 train loss : 0.7221 acc : 0.8281 | Val loss : 1.0722 acc : 0.7118\n",
            "step : 23700 train loss : 0.7148 acc : 0.8293 | Val loss : 1.0717 acc : 0.7098\n",
            "step : 23800 train loss : 0.7143 acc : 0.8287 | Val loss : 1.0723 acc : 0.7109\n",
            "step : 23900 train loss : 0.7176 acc : 0.8279 | Val loss : 1.0738 acc : 0.7117\n",
            "step : 24000 train loss : 0.7214 acc : 0.8248 | Val loss : 1.0721 acc : 0.7130\n",
            "step : 24100 train loss : 0.7086 acc : 0.8338 | Val loss : 1.0715 acc : 0.7120\n",
            "step : 24200 train loss : 0.7262 acc : 0.8230 | Val loss : 1.0727 acc : 0.7133\n",
            "step : 24300 train loss : 0.7049 acc : 0.8279 | Val loss : 1.0749 acc : 0.7095\n",
            "step : 24400 train loss : 0.7131 acc : 0.8305 | Val loss : 1.0723 acc : 0.7119\n",
            "step : 24500 train loss : 0.7164 acc : 0.8303 | Val loss : 1.0720 acc : 0.7110\n",
            "step : 24600 train loss : 0.7370 acc : 0.8210 | Val loss : 1.0737 acc : 0.7093\n",
            "step : 24700 train loss : 0.7294 acc : 0.8244 | Val loss : 1.0745 acc : 0.7095\n",
            "step : 24800 train loss : 0.7363 acc : 0.8192 | Val loss : 1.0721 acc : 0.7121\n",
            "step : 24900 train loss : 0.7161 acc : 0.8285 | Val loss : 1.0715 acc : 0.7126\n",
            "step : 25000 train loss : 0.7099 acc : 0.8319 | Val loss : 1.0711 acc : 0.7129\n",
            "step : 25100 train loss : 0.7074 acc : 0.8315 | Val loss : 1.0723 acc : 0.7117\n",
            "step : 25200 train loss : 0.7116 acc : 0.8286 | Val loss : 1.0734 acc : 0.7126\n",
            "step : 25300 train loss : 0.7116 acc : 0.8287 | Val loss : 1.0708 acc : 0.7146\n",
            "step : 25400 train loss : 0.7125 acc : 0.8303 | Val loss : 1.0708 acc : 0.7128\n",
            "step : 25500 train loss : 0.7248 acc : 0.8264 | Val loss : 1.0721 acc : 0.7113\n",
            "step : 25600 train loss : 0.7210 acc : 0.8273 | Val loss : 1.0724 acc : 0.7104\n",
            "step : 25700 train loss : 0.7216 acc : 0.8254 | Val loss : 1.0718 acc : 0.7117\n",
            "step : 25800 train loss : 0.7208 acc : 0.8295 | Val loss : 1.0718 acc : 0.7130\n",
            "step : 25900 train loss : 0.7185 acc : 0.8291 | Val loss : 1.0722 acc : 0.7112\n",
            "step : 26000 train loss : 0.7178 acc : 0.8277 | Val loss : 1.0722 acc : 0.7137\n",
            "step : 26100 train loss : 0.7230 acc : 0.8269 | Val loss : 1.0739 acc : 0.7108\n",
            "step : 26200 train loss : 0.7202 acc : 0.8314 | Val loss : 1.0764 acc : 0.7127\n",
            "step : 26300 train loss : 0.7085 acc : 0.8347 | Val loss : 1.0733 acc : 0.7117\n",
            "step : 26400 train loss : 0.7194 acc : 0.8247 | Val loss : 1.0714 acc : 0.7123\n",
            "step : 26500 train loss : 0.7140 acc : 0.8296 | Val loss : 1.0734 acc : 0.7124\n",
            "step : 26600 train loss : 0.7071 acc : 0.8312 | Val loss : 1.0739 acc : 0.7115\n",
            "step : 26700 train loss : 0.7111 acc : 0.8290 | Val loss : 1.0729 acc : 0.7128\n",
            "step : 26800 train loss : 0.7119 acc : 0.8314 | Val loss : 1.0729 acc : 0.7120\n",
            "step : 26900 train loss : 0.7252 acc : 0.8275 | Val loss : 1.0729 acc : 0.7107\n",
            "step : 27000 train loss : 0.7254 acc : 0.8275 | Val loss : 1.0731 acc : 0.7110\n",
            "step : 27100 train loss : 0.7119 acc : 0.8350 | Val loss : 1.0722 acc : 0.7131\n",
            "step : 27200 train loss : 0.7133 acc : 0.8303 | Val loss : 1.0742 acc : 0.7101\n",
            "step : 27300 train loss : 0.7149 acc : 0.8298 | Val loss : 1.0739 acc : 0.7126\n",
            "step : 27400 train loss : 0.7211 acc : 0.8311 | Val loss : 1.0726 acc : 0.7125\n",
            "step : 27500 train loss : 0.7148 acc : 0.8314 | Val loss : 1.0726 acc : 0.7125\n",
            "step : 27600 train loss : 0.7076 acc : 0.8330 | Val loss : 1.0748 acc : 0.7104\n",
            "step : 27700 train loss : 0.7052 acc : 0.8365 | Val loss : 1.0720 acc : 0.7107\n",
            "step : 27800 train loss : 0.7219 acc : 0.8289 | Val loss : 1.0755 acc : 0.7117\n",
            "step : 27900 train loss : 0.7100 acc : 0.8348 | Val loss : 1.0724 acc : 0.7129\n",
            "step : 28000 train loss : 0.7198 acc : 0.8301 | Val loss : 1.0727 acc : 0.7115\n",
            "step : 28100 train loss : 0.7276 acc : 0.8219 | Val loss : 1.0750 acc : 0.7109\n",
            "step : 28200 train loss : 0.7154 acc : 0.8304 | Val loss : 1.0743 acc : 0.7117\n",
            "step : 28300 train loss : 0.7123 acc : 0.8288 | Val loss : 1.0725 acc : 0.7129\n",
            "step : 28400 train loss : 0.7072 acc : 0.8302 | Val loss : 1.0746 acc : 0.7111\n",
            "step : 28500 train loss : 0.7163 acc : 0.8287 | Val loss : 1.0722 acc : 0.7113\n",
            "step : 28600 train loss : 0.7144 acc : 0.8270 | Val loss : 1.0726 acc : 0.7123\n",
            "step : 28700 train loss : 0.7164 acc : 0.8266 | Val loss : 1.0727 acc : 0.7121\n",
            "step : 28800 train loss : 0.7263 acc : 0.8235 | Val loss : 1.0725 acc : 0.7114\n",
            "step : 28900 train loss : 0.7212 acc : 0.8319 | Val loss : 1.0724 acc : 0.7111\n",
            "step : 29000 train loss : 0.7115 acc : 0.8295 | Val loss : 1.0726 acc : 0.7122\n",
            "step : 29100 train loss : 0.7114 acc : 0.8351 | Val loss : 1.0710 acc : 0.7124\n",
            "step : 29200 train loss : 0.7263 acc : 0.8233 | Val loss : 1.0724 acc : 0.7128\n",
            "step : 29300 train loss : 0.7201 acc : 0.8278 | Val loss : 1.0731 acc : 0.7108\n",
            "step : 29400 train loss : 0.7108 acc : 0.8322 | Val loss : 1.0731 acc : 0.7107\n",
            "step : 29500 train loss : 0.7116 acc : 0.8308 | Val loss : 1.0748 acc : 0.7137\n",
            "step : 29600 train loss : 0.7038 acc : 0.8325 | Val loss : 1.0738 acc : 0.7127\n",
            "step : 29700 train loss : 0.6946 acc : 0.8382 | Val loss : 1.0728 acc : 0.7121\n",
            "step : 29800 train loss : 0.7203 acc : 0.8302 | Val loss : 1.0735 acc : 0.7123\n",
            "step : 29900 train loss : 0.7061 acc : 0.8306 | Val loss : 1.0730 acc : 0.7124\n",
            "step : 30000 train loss : 0.7174 acc : 0.8264 | Val loss : 1.0744 acc : 0.7136\n",
            "step : 30100 train loss : 0.7151 acc : 0.8281 | Val loss : 1.0733 acc : 0.7121\n",
            "step : 30200 train loss : 0.7106 acc : 0.8303 | Val loss : 1.0739 acc : 0.7134\n",
            "step : 30300 train loss : 0.6962 acc : 0.8375 | Val loss : 1.0744 acc : 0.7115\n",
            "step : 30400 train loss : 0.7178 acc : 0.8282 | Val loss : 1.0747 acc : 0.7118\n",
            "step : 30500 train loss : 0.7148 acc : 0.8314 | Val loss : 1.0749 acc : 0.7118\n",
            "step : 30600 train loss : 0.7154 acc : 0.8300 | Val loss : 1.0729 acc : 0.7128\n",
            "step : 30700 train loss : 0.6993 acc : 0.8323 | Val loss : 1.0741 acc : 0.7140\n",
            "step : 30800 train loss : 0.7094 acc : 0.8320 | Val loss : 1.0731 acc : 0.7128\n",
            "step : 30900 train loss : 0.7200 acc : 0.8267 | Val loss : 1.0760 acc : 0.7114\n",
            "step : 31000 train loss : 0.7126 acc : 0.8289 | Val loss : 1.0749 acc : 0.7126\n",
            "step : 31100 train loss : 0.7201 acc : 0.8293 | Val loss : 1.0788 acc : 0.7111\n",
            "step : 31200 train loss : 0.7036 acc : 0.8325 | Val loss : 1.0750 acc : 0.7117\n",
            "step : 31300 train loss : 0.6982 acc : 0.8338 | Val loss : 1.0738 acc : 0.7119\n",
            "step : 31400 train loss : 0.7110 acc : 0.8296 | Val loss : 1.0726 acc : 0.7120\n",
            "step : 31500 train loss : 0.7266 acc : 0.8247 | Val loss : 1.0766 acc : 0.7112\n",
            "step : 31600 train loss : 0.7169 acc : 0.8318 | Val loss : 1.0739 acc : 0.7134\n",
            "step : 31700 train loss : 0.7074 acc : 0.8308 | Val loss : 1.0726 acc : 0.7140\n",
            "step : 31800 train loss : 0.7130 acc : 0.8301 | Val loss : 1.0768 acc : 0.7109\n",
            "step : 31900 train loss : 0.7157 acc : 0.8288 | Val loss : 1.0733 acc : 0.7124\n",
            "step : 32000 train loss : 0.7114 acc : 0.8280 | Val loss : 1.0739 acc : 0.7133\n",
            "step : 32100 train loss : 0.7163 acc : 0.8287 | Val loss : 1.0746 acc : 0.7145\n",
            "step : 32200 train loss : 0.7241 acc : 0.8243 | Val loss : 1.0752 acc : 0.7127\n",
            "step : 32300 train loss : 0.7117 acc : 0.8297 | Val loss : 1.0732 acc : 0.7128\n",
            "step : 32400 train loss : 0.7087 acc : 0.8336 | Val loss : 1.0737 acc : 0.7137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0dd8fd30f473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         train_loss, train_acc, train_merged = sess.run([loss, acc, merged], feed_dict = { xs : train_xtest,\n\u001b[1;32m     31\u001b[0m                                                                                           \u001b[0mys\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain_ytest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                                                                         phase_train : False})\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m        \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}