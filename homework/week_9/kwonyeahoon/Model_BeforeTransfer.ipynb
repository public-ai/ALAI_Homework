{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "배포_Homework_Pretraining.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5DbTV6Y8_fm"
      },
      "source": [
        "## CIFAR-10, 100 학습시키기\n",
        "\n",
        "## Objective\n",
        "\n",
        "1.[CIFAR -10 Data](https://www.cs.toronto.edu/~kriz/cifar.html) 을 Convolution Neural Network 을 이용해 학습해봅니다.\n",
        "----\n",
        "![Imgur](https://i.imgur.com/yy09iLz.png)\n",
        "\n",
        "\n",
        "- loss 가 가장 작은 model 을 저장합니다.\n",
        "- 목표 accuracy 는 75% 입니다. \n",
        "​\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-syRx_WBWebm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okPBT4DkqPmu"
      },
      "source": [
        "# Load Cifar-10 dataset \n",
        " - cifar 10 dataset 을 다운로드 합니다. \n",
        " - normalize 을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8KwcN7baxvN",
        "colab_type": "code",
        "outputId": "4821486f-a353-47b6-cb0f-cf13ea22cc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "import tensorboardcolab\n",
        "#content/tensorboard\n",
        "tbc=tensorboardcolab.TensorBoardColab(graph_path='./tensorboard')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://6f6e0c65.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ApD4EzRiqOGj",
        "outputId": "c70fe560-7fbb-42e8-b045-a3a024828d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# load cifar10 dataset \n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# reshape (None, 1) -> (None)\n",
        "y_train = np.reshape(y_train, (-1))\n",
        "y_test = np.reshape(y_test, (-1))\n",
        "\n",
        "# normalization \n",
        "x_train, x_test = x_train/255. , x_test/255.\n",
        "\n",
        "# N class\n",
        "n_classes = 10\n",
        "print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n",
        "print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n",
        "print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n",
        "print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image shape : (50000, 32, 32, 3), label shape : (50000,) \n",
            "image shape : (10000, 32, 32, 3), label shape : (10000,) \n",
            "train minimun : 0.0, train_maximum : 1.0 \n",
            "tests minimun : 0.0, test_maximum : 1.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb-_UZEUQ_N",
        "colab_type": "text"
      },
      "source": [
        "# DataProvider "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmE4_GikBI6I",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "np.random.seed(0)\n",
        "class DataProvider(object):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_fix = images # fix data저장\n",
        "        self.labels_fix = labels # fix data저장\n",
        "        self.len_ = self.images.shape[0] # 총 이미지 갯수저장\n",
        "        self.len_fix = copy.deepcopy(self.len_)  # fix길이 저장\n",
        "        self.ind_range = self.images.shape[0] # index\n",
        "        self.ind = [ x for x in range(self.ind_range)]\n",
        "        np.random.shuffle(self.ind) # index shuffle\n",
        "        self.images = self.images[self.ind, :] # shuffle 수행\n",
        "        self.labels = self.labels[self.ind]\n",
        "        self.images = list(self.images) #list화 시킴(del() 등 list연산 사용필요)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        #fix me#\n",
        "        if self.len_ <= batch_size :\n",
        "            ### 해당 epoch의 마지막 batch case ###\n",
        "            # 1.나머지 모두 내보냄\n",
        "            out_batch_image = self.images[:][:]\n",
        "            out_batch_labels = self.labels[:]\n",
        "            del(self.images[:])\n",
        "            del(self.labels[:])\n",
        "            \n",
        "            # 2.다음 epoch의 shuffle 수행\n",
        "            self.len_ = self.len_fix\n",
        "            self.images = self.images_fix\n",
        "            self.labels = self.labels_fix\n",
        "            self.ind = [x for x in range(self.ind_range)]\n",
        "            np.random.shuffle(self.ind)\n",
        "            self.images = self.images[self.ind,:] # shuffle 수행\n",
        "            self.labels = self.labels[self.ind]\n",
        "            self.images = list(self.images)\n",
        "            self.labels = list(self.labels)\n",
        "        else : \n",
        "            # 일반 batch수행\n",
        "            out_batch_image, out_batch_labels = self.images[:batch_size][:], self.labels[:batch_size] # slice함\n",
        "            del(self.images[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            del(self.labels[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            self.len_ = self.len_ - batch_size # 길이줄임\n",
        "            out_batch_labels = np.array(out_batch_labels)\n",
        "            out_batch_image = np.array(out_batch_image)\n",
        "        return out_batch_image, out_batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b0Hku8a98_fo"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "설계한 모델을 표로 작성합니다. \n",
        "\n",
        "- 목표 Receptive Field : ? <br>\n",
        "- Convolution Phase 후  출력 크기  :  ? <br>\n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| ? |?| ?|? |?  | ? |?| ?|\n",
        "\n",
        "\n",
        "- 모델 설계가 끝나면 간단한 그림을 작성해 아래에 붙여주세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDhKG-rSUQ_P",
        "colab_type": "text"
      },
      "source": [
        "예시1) \n",
        "\n",
        "\n",
        "- 목표 Receptive Field : 28 <br>\n",
        "- Convolution Phase 후  출력 크기  :  4 <br>\n",
        "- Regularization  : L2 \n",
        "- Batch size : 120\n",
        "- Learning rate : 0.0001 \n",
        "- Data normalization : min max normalization \n",
        "- Standardization : None \n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| c1 |conv| 64| 3x3| 1  | SAME | None| 32x32 |\n",
        "| s2 |max-pooling| None| 3x3| 2  | SAME | None|16x16 | \n",
        "| c3 |conv| 128| 3x3| 2  | SAME |NOne |16x16 | \n",
        "| s4 |max-pooling| None| 3x3| 2  | SAME | None|8 x8 | \n",
        "| c5 |conv| 128| 3x3| 2  | SAME | None |8 x8 | \n",
        "| s6 |conv| 256| 3x3| 2  | SAME | None |4 x 4 | \n",
        "| c7 |conv| 256| 1x1| 2  | SAME | None |4 x 4 | \n",
        "| f8 ||| | FC 256  | |  || \n",
        "| f8 ||| | Dropout 0.7 | |  || \n",
        "| f9 ||| | FC 256  | |  || \n",
        "| f9 ||| | Dropout 0.6 | |  || \n",
        "| f10||| | FC 10   | |  || \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Imgur](https://i.imgur.com/yqrIm5u.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2qtnd7UQ_Q",
        "colab_type": "text"
      },
      "source": [
        "# Convolution layer\n",
        "- convolution layer helper function 을 정의합니다.\n",
        "- 위 설계한 convolution layer 을 구현합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rzr0by6Kyv6J",
        "colab": {}
      },
      "source": [
        "# convolution helper function\n",
        "def conv(input_xs ,units, k, s, padding, activation, name):\n",
        "    layer = tf.layers.Conv2D(filters = units, kernel_size = k, strides = s,\n",
        "                             padding = padding, activation = activation, name = name )(input_xs)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmKroxF-CDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define input placeholder \n",
        "xs = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 3])\n",
        "ys = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "#ys_one_hot = tf.one_hot()\n",
        "lr = tf.placeholder(dtype = tf.float32, shape = ())\n",
        "phase_train = tf.placeholder(tf.bool, shape = (), name = 'phase_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AJcmfqHJ9lLm",
        "outputId": "909b2133-9cfa-4c66-f1c0-364be2d95645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Model implementation \n",
        "# convolution Neural Network \n",
        "# 자신이 설계한 모형을 구현해주세요.\n",
        "with tf.variable_scope('VGG_block-1') :\n",
        "    layer = conv(xs, 10, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv' )\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-2') :\n",
        "    layer = conv(pooling, 50, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-3') :\n",
        "    layer = conv(pooling, 100, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 100, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-4') :\n",
        "    layer = conv(pooling, 200, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 200, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-5') :\n",
        "    layer = conv(pooling, 400, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer,400, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')    \n",
        "    #pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "top_conv = tf.identity(pooling, 'top_conv') # 마지막 layer 을 top conv 에 넣습니다.\n",
        "tf.shape(top_conv)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0616 16:35:00.101978 140658729490304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Shape:0' shape=(4,) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRukJjRZUQ_V",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layer\n",
        "- 설계한 fully connected layer 을 구현합니다.\n",
        "- dropout 을 적용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNx29qxUQ_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(flat_layer, units, initializer_, layer_name):\n",
        "    dense = tf.layers.Dense(units = units, activation = tf.nn.relu, kernel_initializer = initializer_, name = layer_name)(flat_layer)\n",
        "    return dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL1M4gvUQ_X",
        "colab_type": "code",
        "outputId": "b8327af9-429f-4ae4-bb85-eb885bcf4546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# flat layer \n",
        "flatten_layer = tf.layers.flatten(top_conv)\n",
        "#print(\"tf.shape(flat_layer) :\", tf.shape(flatten_layer))\n",
        "\n",
        "# fully connected layer 1\n",
        "fc_initializer = tf.initializers.he_normal()\n",
        "fc_layer_1 = fc(flat_layer = flatten_layer, units = 2000, initializer_ = fc_initializer, layer_name = \"FC1\" )\n",
        "fc_layer_1 = tf.layers.dropout(fc_layer_1,rate=0.5, training=phase_train)\n",
        "\n",
        "# fix me # 자신이 설계한 fully connected layer 을 구현합니다.  \n",
        "\n",
        "fc_layer_2 = fc(flat_layer = fc_layer_1, units = 500, initializer_ = fc_initializer, layer_name = \"FC2\" )\n",
        "fc_layer_2 = tf.layers.dropout(fc_layer_2,rate=0.5, training=phase_train)\n",
        "\n",
        "fc_layer_3 = fc(flat_layer = fc_layer_2, units = 100, initializer_ = fc_initializer, layer_name = \"FC3\" )\n",
        "\n",
        "fc_layer_4 = fc(flat_layer = fc_layer_3, units = 10, initializer_ = fc_initializer, layer_name = \"FC4\" )\n",
        "\n",
        "logits= tf.identity(fc_layer_4, 'logits')\n",
        "\n",
        "ys = tf.cast(ys, tf.int32)\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(ys, logits)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0616 16:35:00.561223 140658729490304 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:1: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0616 16:35:01.156575 140658729490304 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:7: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0616 16:35:01.288250 140658729490304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-625ULtUQ_Z",
        "colab_type": "text"
      },
      "source": [
        "#  Loss function \n",
        "- loss function 을 정의합니다. L2 regularization 을 사용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2DkXG-UQ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_reg = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n",
        "l2_beta = 5e-4\n",
        "\n",
        "#loss \n",
        "# L2 reularization \n",
        "loss = loss + (l2_beta * l2_reg)\n",
        "loss = tf.identity(loss, name = 'loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Oke8OgUQ_e",
        "colab_type": "text"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_TMXVtX1pXK",
        "colab": {}
      },
      "source": [
        "# metric\n",
        "pred = tf.nn.softmax(logits)\n",
        "one_hot_label = tf.one_hot(ys, 10)\n",
        "pred_arg = tf.argmax(pred, axis = 1)\n",
        "label_arg = tf.argmax(one_hot_label, axis = 1)\n",
        "eq = tf.cast(tf.equal(pred_arg, label_arg), dtype = tf.float32)\n",
        "acc = tf.reduce_mean(eq, axis =0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfoWSrLUQ_h",
        "colab_type": "text"
      },
      "source": [
        "# Add tensor to Tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqUXVsUTUQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add accuracy to tensorboard nodes \n",
        "#fix me #\n",
        "acc_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "# add loss to tensorboard nodes \n",
        "#fix me #\n",
        "loss_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "\n",
        "#merge all tensorboard nodes \n",
        "#fix me #\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGeaVRCOUQ_j",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83C5s7mfUQ_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_op : adamoptimizer \n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6N1fBH2BBfX6"
      },
      "source": [
        "# Session open "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wiz-2rkLBdTw",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "#초기학습\n",
        "init_g = tf.global_variables_initializer() # : globalal initializer\n",
        "init_l = tf.local_variables_initializer() # : globalal initializer\n",
        "sess.run([init_l,init_g])\n",
        "\n",
        "# saver \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "#Weight Transfer \n",
        "#saver.restore(sess, './ttee/model-48700')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFmQDdJDUQ_o",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard Filewriter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPH7CLomUQ_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorboard \n",
        "train_writer=tf.summary.FileWriter(logdir='./tensorboard/train')\n",
        "\n",
        "test_writer=tf.summary.FileWriter(logdir='./tensorboard/test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P88ef2rRUQ_q",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yv99AR4A_Y7P",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6223
        },
        "outputId": "e07cb81a-9190-413e-ab14-d41f100b2b20"
      },
      "source": [
        "dataprovider = DataProvider(images=x_train, labels=y_train)\n",
        "#save_root_folder = #fix me # : models saved folder \n",
        "\n",
        "# hparam \n",
        "batch_size = 100\n",
        "min_loss = 1000000.0\n",
        "learning_rate = 0.0001\n",
        "\n",
        "np.random.seed(0)\n",
        "#local variable initialize\n",
        "for i in range(50000):\n",
        "    batch_xs, batch_ys = dataprovider.next_batch(batch_size)\n",
        "    # training \n",
        "    _= sess.run(train_op, feed_dict = {xs : batch_xs,\n",
        "                                        ys : batch_ys,\n",
        "                                        lr : learning_rate,\n",
        "                                        phase_train : True})\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        # Validate validation dataset \n",
        "        fetches=[loss, acc, merged]\n",
        "        val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n",
        "                                                                      ys : y_test,\n",
        "                                                                      phase_train : False})\n",
        "\n",
        "        # Validate train dataset : extract randomly 10000 samples from train dataset \n",
        "        ran = [ x for x in range(0, 50000)]\n",
        "        nansu = np.random.choice(ran, size = 10000, replace=False)\n",
        "        train_xtest, train_ytest = x_train[nansu], y_train[nansu]\n",
        "        train_loss, train_acc, train_merged = sess.run([loss, acc, merged], feed_dict = { xs : train_xtest,\n",
        "                                                                                          ys : train_ytest,\n",
        "                                                                                        phase_train : False})\n",
        "       \n",
        "        print('step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'.\\\n",
        "        format(i, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "        # Save Model \n",
        "        if val_loss < min_loss : #fix me # : when val_loss < min_loss \n",
        "            min_loss = val_loss\n",
        "            save_path = './ttee/model'\n",
        "            saver.save(sess, save_path, global_step=i)\n",
        "            print('model save!')\n",
        "            \n",
        "        # Add values to tensorboard \n",
        "        train_writer.add_summary(train_merged, i)\n",
        "        test_writer.add_summary(val_merged, i)\n",
        "        train_writer.flush()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 0 train loss : 3.8984 acc : 0.1002 | Val loss : 3.8985 acc : 0.1014\n",
            "model save!\n",
            "step : 100 train loss : 3.4357 acc : 0.1109 | Val loss : 3.4340 acc : 0.1124\n",
            "model save!\n",
            "step : 200 train loss : 2.9413 acc : 0.2701 | Val loss : 2.9357 acc : 0.2748\n",
            "model save!\n",
            "step : 300 train loss : 2.7497 acc : 0.3043 | Val loss : 2.7341 acc : 0.3227\n",
            "model save!\n",
            "step : 400 train loss : 2.5973 acc : 0.3511 | Val loss : 2.5885 acc : 0.3612\n",
            "model save!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0616 16:35:20.798560 140658729490304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step : 500 train loss : 2.4855 acc : 0.3788 | Val loss : 2.4601 acc : 0.3917\n",
            "model save!\n",
            "step : 600 train loss : 2.3867 acc : 0.4043 | Val loss : 2.3712 acc : 0.4100\n",
            "model save!\n",
            "step : 700 train loss : 2.3214 acc : 0.4188 | Val loss : 2.3035 acc : 0.4252\n",
            "model save!\n",
            "step : 800 train loss : 2.1921 acc : 0.4278 | Val loss : 2.2014 acc : 0.4215\n",
            "model save!\n",
            "step : 900 train loss : 2.0953 acc : 0.4532 | Val loss : 2.0952 acc : 0.4571\n",
            "model save!\n",
            "step : 1000 train loss : 2.0629 acc : 0.4628 | Val loss : 2.0672 acc : 0.4589\n",
            "model save!\n",
            "step : 1100 train loss : 2.0618 acc : 0.4570 | Val loss : 2.0485 acc : 0.4590\n",
            "model save!\n",
            "step : 1200 train loss : 1.9671 acc : 0.4845 | Val loss : 1.9722 acc : 0.4760\n",
            "model save!\n",
            "step : 1300 train loss : 1.9400 acc : 0.4836 | Val loss : 1.9311 acc : 0.4873\n",
            "model save!\n",
            "step : 1400 train loss : 1.9075 acc : 0.4897 | Val loss : 1.9004 acc : 0.4938\n",
            "model save!\n",
            "step : 1500 train loss : 1.8994 acc : 0.4944 | Val loss : 1.9028 acc : 0.4932\n",
            "step : 1600 train loss : 1.8209 acc : 0.5285 | Val loss : 1.8258 acc : 0.5196\n",
            "model save!\n",
            "step : 1700 train loss : 1.7884 acc : 0.5335 | Val loss : 1.8028 acc : 0.5244\n",
            "model save!\n",
            "step : 1800 train loss : 1.7529 acc : 0.5443 | Val loss : 1.7711 acc : 0.5325\n",
            "model save!\n",
            "step : 1900 train loss : 1.8054 acc : 0.5198 | Val loss : 1.7935 acc : 0.5215\n",
            "step : 2000 train loss : 1.7031 acc : 0.5505 | Val loss : 1.7283 acc : 0.5433\n",
            "model save!\n",
            "step : 2100 train loss : 1.7123 acc : 0.5410 | Val loss : 1.7196 acc : 0.5327\n",
            "model save!\n",
            "step : 2200 train loss : 1.6721 acc : 0.5580 | Val loss : 1.7018 acc : 0.5463\n",
            "model save!\n",
            "step : 2300 train loss : 1.6265 acc : 0.5779 | Val loss : 1.6536 acc : 0.5632\n",
            "model save!\n",
            "step : 2400 train loss : 1.6104 acc : 0.5754 | Val loss : 1.6384 acc : 0.5652\n",
            "model save!\n",
            "step : 2500 train loss : 1.5698 acc : 0.5886 | Val loss : 1.6250 acc : 0.5691\n",
            "model save!\n",
            "step : 2600 train loss : 1.5988 acc : 0.5752 | Val loss : 1.6132 acc : 0.5678\n",
            "model save!\n",
            "step : 2700 train loss : 1.6304 acc : 0.5621 | Val loss : 1.6505 acc : 0.5527\n",
            "step : 2800 train loss : 1.5406 acc : 0.5946 | Val loss : 1.5846 acc : 0.5763\n",
            "model save!\n",
            "step : 2900 train loss : 1.5386 acc : 0.5924 | Val loss : 1.5689 acc : 0.5816\n",
            "model save!\n",
            "step : 3000 train loss : 1.5258 acc : 0.5968 | Val loss : 1.5605 acc : 0.5794\n",
            "model save!\n",
            "step : 3100 train loss : 1.5300 acc : 0.5898 | Val loss : 1.5670 acc : 0.5721\n",
            "step : 3200 train loss : 1.5033 acc : 0.6004 | Val loss : 1.5438 acc : 0.5754\n",
            "model save!\n",
            "step : 3300 train loss : 1.4658 acc : 0.6151 | Val loss : 1.5073 acc : 0.5962\n",
            "model save!\n",
            "step : 3400 train loss : 1.4489 acc : 0.6201 | Val loss : 1.5042 acc : 0.5927\n",
            "model save!\n",
            "step : 3500 train loss : 1.4409 acc : 0.6126 | Val loss : 1.4841 acc : 0.6057\n",
            "model save!\n",
            "step : 3600 train loss : 1.4006 acc : 0.6296 | Val loss : 1.4590 acc : 0.6099\n",
            "model save!\n",
            "step : 3700 train loss : 1.4059 acc : 0.6260 | Val loss : 1.4541 acc : 0.6075\n",
            "model save!\n",
            "step : 3800 train loss : 1.3744 acc : 0.6393 | Val loss : 1.4240 acc : 0.6197\n",
            "model save!\n",
            "step : 3900 train loss : 1.3658 acc : 0.6382 | Val loss : 1.4329 acc : 0.6150\n",
            "step : 4000 train loss : 1.3354 acc : 0.6498 | Val loss : 1.4128 acc : 0.6213\n",
            "model save!\n",
            "step : 4100 train loss : 1.3540 acc : 0.6404 | Val loss : 1.4146 acc : 0.6181\n",
            "step : 4200 train loss : 1.3040 acc : 0.6610 | Val loss : 1.3879 acc : 0.6261\n",
            "model save!\n",
            "step : 4300 train loss : 1.3145 acc : 0.6544 | Val loss : 1.3753 acc : 0.6301\n",
            "model save!\n",
            "step : 4400 train loss : 1.3103 acc : 0.6560 | Val loss : 1.3946 acc : 0.6252\n",
            "step : 4500 train loss : 1.2486 acc : 0.6711 | Val loss : 1.3421 acc : 0.6435\n",
            "model save!\n",
            "step : 4600 train loss : 1.2656 acc : 0.6720 | Val loss : 1.3570 acc : 0.6345\n",
            "step : 4700 train loss : 1.3120 acc : 0.6476 | Val loss : 1.3682 acc : 0.6333\n",
            "step : 4800 train loss : 1.2710 acc : 0.6635 | Val loss : 1.3564 acc : 0.6297\n",
            "step : 4900 train loss : 1.2492 acc : 0.6746 | Val loss : 1.3267 acc : 0.6452\n",
            "model save!\n",
            "step : 5000 train loss : 1.2370 acc : 0.6667 | Val loss : 1.3275 acc : 0.6434\n",
            "step : 5100 train loss : 1.1837 acc : 0.6934 | Val loss : 1.2993 acc : 0.6463\n",
            "model save!\n",
            "step : 5200 train loss : 1.1874 acc : 0.6883 | Val loss : 1.2858 acc : 0.6560\n",
            "model save!\n",
            "step : 5300 train loss : 1.2079 acc : 0.6843 | Val loss : 1.3303 acc : 0.6375\n",
            "step : 5400 train loss : 1.2030 acc : 0.6866 | Val loss : 1.3097 acc : 0.6409\n",
            "step : 5500 train loss : 1.1514 acc : 0.7040 | Val loss : 1.2661 acc : 0.6575\n",
            "model save!\n",
            "step : 5600 train loss : 1.1569 acc : 0.6981 | Val loss : 1.2721 acc : 0.6564\n",
            "step : 5700 train loss : 1.1368 acc : 0.7027 | Val loss : 1.2643 acc : 0.6567\n",
            "model save!\n",
            "step : 5800 train loss : 1.1873 acc : 0.6874 | Val loss : 1.2972 acc : 0.6444\n",
            "step : 5900 train loss : 1.1028 acc : 0.7159 | Val loss : 1.2366 acc : 0.6693\n",
            "model save!\n",
            "step : 6000 train loss : 1.1036 acc : 0.7151 | Val loss : 1.2317 acc : 0.6663\n",
            "model save!\n",
            "step : 6100 train loss : 1.1302 acc : 0.7002 | Val loss : 1.2724 acc : 0.6547\n",
            "step : 6200 train loss : 1.0944 acc : 0.7124 | Val loss : 1.2550 acc : 0.6638\n",
            "step : 6300 train loss : 1.1190 acc : 0.6990 | Val loss : 1.2422 acc : 0.6591\n",
            "step : 6400 train loss : 1.0942 acc : 0.7125 | Val loss : 1.2424 acc : 0.6656\n",
            "step : 6500 train loss : 1.0622 acc : 0.7217 | Val loss : 1.2028 acc : 0.6776\n",
            "model save!\n",
            "step : 6600 train loss : 1.0475 acc : 0.7241 | Val loss : 1.2167 acc : 0.6687\n",
            "step : 6700 train loss : 1.0524 acc : 0.7320 | Val loss : 1.1834 acc : 0.6805\n",
            "model save!\n",
            "step : 6800 train loss : 1.0475 acc : 0.7282 | Val loss : 1.2232 acc : 0.6708\n",
            "step : 6900 train loss : 1.0470 acc : 0.7274 | Val loss : 1.2146 acc : 0.6706\n",
            "step : 7000 train loss : 1.0078 acc : 0.7455 | Val loss : 1.1882 acc : 0.6793\n",
            "step : 7100 train loss : 1.0057 acc : 0.7436 | Val loss : 1.1957 acc : 0.6774\n",
            "step : 7200 train loss : 1.0354 acc : 0.7382 | Val loss : 1.2168 acc : 0.6733\n",
            "step : 7300 train loss : 1.0323 acc : 0.7316 | Val loss : 1.2236 acc : 0.6700\n",
            "step : 7400 train loss : 1.0126 acc : 0.7403 | Val loss : 1.1868 acc : 0.6837\n",
            "step : 7500 train loss : 0.9907 acc : 0.7438 | Val loss : 1.1671 acc : 0.6817\n",
            "model save!\n",
            "step : 7600 train loss : 1.0170 acc : 0.7402 | Val loss : 1.1954 acc : 0.6767\n",
            "step : 7700 train loss : 0.9454 acc : 0.7641 | Val loss : 1.1576 acc : 0.6836\n",
            "model save!\n",
            "step : 7800 train loss : 0.9388 acc : 0.7638 | Val loss : 1.1474 acc : 0.6897\n",
            "model save!\n",
            "step : 7900 train loss : 0.9276 acc : 0.7723 | Val loss : 1.1334 acc : 0.6936\n",
            "model save!\n",
            "step : 8000 train loss : 0.9177 acc : 0.7709 | Val loss : 1.1276 acc : 0.7001\n",
            "model save!\n",
            "step : 8100 train loss : 0.9589 acc : 0.7506 | Val loss : 1.1864 acc : 0.6812\n",
            "step : 8200 train loss : 0.8946 acc : 0.7755 | Val loss : 1.1360 acc : 0.6986\n",
            "step : 8300 train loss : 0.9294 acc : 0.7615 | Val loss : 1.1577 acc : 0.6842\n",
            "step : 8400 train loss : 0.9129 acc : 0.7732 | Val loss : 1.1402 acc : 0.6942\n",
            "step : 8500 train loss : 0.8921 acc : 0.7785 | Val loss : 1.1184 acc : 0.6988\n",
            "model save!\n",
            "step : 8600 train loss : 0.8935 acc : 0.7755 | Val loss : 1.1535 acc : 0.6934\n",
            "step : 8700 train loss : 0.9085 acc : 0.7683 | Val loss : 1.1603 acc : 0.6954\n",
            "step : 8800 train loss : 0.8680 acc : 0.7790 | Val loss : 1.1424 acc : 0.6928\n",
            "step : 8900 train loss : 0.9030 acc : 0.7726 | Val loss : 1.1636 acc : 0.6845\n",
            "step : 9000 train loss : 0.8507 acc : 0.7973 | Val loss : 1.1059 acc : 0.7076\n",
            "model save!\n",
            "step : 9100 train loss : 0.8500 acc : 0.7936 | Val loss : 1.1301 acc : 0.7005\n",
            "step : 9200 train loss : 0.8417 acc : 0.7974 | Val loss : 1.1180 acc : 0.7024\n",
            "step : 9300 train loss : 0.8400 acc : 0.8003 | Val loss : 1.1233 acc : 0.6984\n",
            "step : 9400 train loss : 0.8214 acc : 0.8041 | Val loss : 1.1112 acc : 0.7047\n",
            "step : 9500 train loss : 0.8278 acc : 0.7997 | Val loss : 1.1157 acc : 0.6997\n",
            "step : 9600 train loss : 0.7963 acc : 0.8118 | Val loss : 1.1164 acc : 0.7051\n",
            "step : 9700 train loss : 0.8388 acc : 0.7959 | Val loss : 1.1357 acc : 0.6922\n",
            "step : 9800 train loss : 0.7922 acc : 0.8141 | Val loss : 1.1081 acc : 0.7096\n",
            "step : 9900 train loss : 0.7765 acc : 0.8195 | Val loss : 1.0944 acc : 0.7100\n",
            "model save!\n",
            "step : 10000 train loss : 0.7762 acc : 0.8212 | Val loss : 1.0915 acc : 0.7094\n",
            "model save!\n",
            "step : 10100 train loss : 0.7804 acc : 0.8158 | Val loss : 1.1228 acc : 0.7024\n",
            "step : 10200 train loss : 0.7795 acc : 0.8163 | Val loss : 1.1344 acc : 0.7005\n",
            "step : 10300 train loss : 0.7492 acc : 0.8284 | Val loss : 1.1014 acc : 0.7106\n",
            "step : 10400 train loss : 0.7368 acc : 0.8322 | Val loss : 1.1059 acc : 0.7095\n",
            "step : 10500 train loss : 0.7988 acc : 0.8049 | Val loss : 1.1574 acc : 0.6912\n",
            "step : 10600 train loss : 0.7255 acc : 0.8330 | Val loss : 1.1083 acc : 0.7130\n",
            "step : 10700 train loss : 0.7259 acc : 0.8353 | Val loss : 1.1346 acc : 0.7094\n",
            "step : 10800 train loss : 0.7311 acc : 0.8308 | Val loss : 1.1152 acc : 0.7100\n",
            "step : 10900 train loss : 0.7139 acc : 0.8402 | Val loss : 1.1250 acc : 0.7054\n",
            "step : 11000 train loss : 0.7111 acc : 0.8365 | Val loss : 1.1156 acc : 0.7107\n",
            "step : 11100 train loss : 0.6921 acc : 0.8420 | Val loss : 1.1150 acc : 0.7048\n",
            "step : 11200 train loss : 0.7026 acc : 0.8412 | Val loss : 1.1329 acc : 0.7148\n",
            "step : 11300 train loss : 0.7030 acc : 0.8425 | Val loss : 1.1330 acc : 0.7049\n",
            "step : 11400 train loss : 0.7215 acc : 0.8395 | Val loss : 1.1427 acc : 0.7046\n",
            "step : 11500 train loss : 0.6870 acc : 0.8460 | Val loss : 1.1323 acc : 0.7032\n",
            "step : 11600 train loss : 0.6787 acc : 0.8513 | Val loss : 1.1329 acc : 0.7067\n",
            "step : 11700 train loss : 0.6646 acc : 0.8565 | Val loss : 1.1216 acc : 0.7102\n",
            "step : 11800 train loss : 0.6402 acc : 0.8683 | Val loss : 1.1044 acc : 0.7158\n",
            "step : 11900 train loss : 0.6528 acc : 0.8619 | Val loss : 1.1333 acc : 0.7077\n",
            "step : 12000 train loss : 0.6628 acc : 0.8535 | Val loss : 1.1379 acc : 0.7034\n",
            "step : 12100 train loss : 0.6127 acc : 0.8747 | Val loss : 1.1245 acc : 0.7141\n",
            "step : 12200 train loss : 0.6079 acc : 0.8815 | Val loss : 1.1018 acc : 0.7188\n",
            "step : 12300 train loss : 0.5977 acc : 0.8842 | Val loss : 1.1026 acc : 0.7213\n",
            "step : 12400 train loss : 0.6057 acc : 0.8798 | Val loss : 1.1064 acc : 0.7166\n",
            "step : 12500 train loss : 0.5817 acc : 0.8916 | Val loss : 1.0910 acc : 0.7236\n",
            "model save!\n",
            "step : 12600 train loss : 0.5927 acc : 0.8818 | Val loss : 1.1477 acc : 0.7078\n",
            "step : 12700 train loss : 0.5626 acc : 0.9011 | Val loss : 1.1236 acc : 0.7209\n",
            "step : 12800 train loss : 0.5737 acc : 0.8937 | Val loss : 1.1342 acc : 0.7194\n",
            "step : 12900 train loss : 0.6046 acc : 0.8768 | Val loss : 1.1587 acc : 0.7079\n",
            "step : 13000 train loss : 0.5923 acc : 0.8830 | Val loss : 1.1518 acc : 0.7106\n",
            "step : 13100 train loss : 0.5374 acc : 0.9032 | Val loss : 1.1504 acc : 0.7213\n",
            "step : 13200 train loss : 0.5359 acc : 0.9057 | Val loss : 1.1473 acc : 0.7182\n",
            "step : 13300 train loss : 0.5341 acc : 0.9065 | Val loss : 1.1240 acc : 0.7190\n",
            "step : 13400 train loss : 0.5579 acc : 0.8951 | Val loss : 1.2022 acc : 0.7091\n",
            "step : 13500 train loss : 0.5436 acc : 0.9029 | Val loss : 1.1607 acc : 0.7187\n",
            "step : 13600 train loss : 0.5141 acc : 0.9123 | Val loss : 1.1410 acc : 0.7211\n",
            "step : 13700 train loss : 0.5181 acc : 0.9114 | Val loss : 1.1670 acc : 0.7237\n",
            "step : 13800 train loss : 0.5133 acc : 0.9123 | Val loss : 1.1644 acc : 0.7196\n",
            "step : 13900 train loss : 0.5061 acc : 0.9166 | Val loss : 1.1733 acc : 0.7173\n",
            "step : 14000 train loss : 0.5175 acc : 0.9133 | Val loss : 1.1701 acc : 0.7178\n",
            "step : 14100 train loss : 0.5502 acc : 0.8933 | Val loss : 1.2537 acc : 0.7064\n",
            "step : 14200 train loss : 0.4735 acc : 0.9298 | Val loss : 1.1803 acc : 0.7245\n",
            "step : 14300 train loss : 0.5172 acc : 0.9092 | Val loss : 1.2300 acc : 0.7129\n",
            "step : 14400 train loss : 0.4708 acc : 0.9320 | Val loss : 1.1801 acc : 0.7221\n",
            "step : 14500 train loss : 0.4850 acc : 0.9221 | Val loss : 1.1809 acc : 0.7176\n",
            "step : 14600 train loss : 0.4583 acc : 0.9307 | Val loss : 1.2242 acc : 0.7170\n",
            "step : 14700 train loss : 0.4655 acc : 0.9294 | Val loss : 1.2084 acc : 0.7160\n",
            "step : 14800 train loss : 0.4429 acc : 0.9368 | Val loss : 1.2025 acc : 0.7198\n",
            "step : 14900 train loss : 0.4454 acc : 0.9404 | Val loss : 1.2241 acc : 0.7170\n",
            "step : 15000 train loss : 0.4487 acc : 0.9419 | Val loss : 1.1970 acc : 0.7184\n",
            "step : 15100 train loss : 0.4267 acc : 0.9456 | Val loss : 1.2540 acc : 0.7202\n",
            "step : 15200 train loss : 0.4233 acc : 0.9473 | Val loss : 1.2442 acc : 0.7128\n",
            "step : 15300 train loss : 0.4192 acc : 0.9499 | Val loss : 1.2321 acc : 0.7200\n",
            "step : 15400 train loss : 0.4201 acc : 0.9494 | Val loss : 1.2266 acc : 0.7200\n",
            "step : 15500 train loss : 0.4629 acc : 0.9321 | Val loss : 1.2815 acc : 0.7096\n",
            "step : 15600 train loss : 0.4097 acc : 0.9533 | Val loss : 1.2560 acc : 0.7207\n",
            "step : 15700 train loss : 0.4010 acc : 0.9538 | Val loss : 1.2576 acc : 0.7196\n",
            "step : 15800 train loss : 0.4007 acc : 0.9550 | Val loss : 1.2648 acc : 0.7167\n",
            "step : 15900 train loss : 0.4689 acc : 0.9251 | Val loss : 1.3690 acc : 0.6949\n",
            "step : 16000 train loss : 0.4181 acc : 0.9433 | Val loss : 1.2812 acc : 0.7138\n",
            "step : 16100 train loss : 0.4050 acc : 0.9517 | Val loss : 1.2933 acc : 0.7168\n",
            "step : 16200 train loss : 0.3944 acc : 0.9555 | Val loss : 1.3201 acc : 0.7166\n",
            "step : 16300 train loss : 0.3827 acc : 0.9618 | Val loss : 1.3036 acc : 0.7197\n",
            "step : 16400 train loss : 0.3774 acc : 0.9646 | Val loss : 1.2987 acc : 0.7218\n",
            "step : 16500 train loss : 0.3917 acc : 0.9576 | Val loss : 1.2952 acc : 0.7236\n",
            "step : 16600 train loss : 0.3567 acc : 0.9720 | Val loss : 1.3014 acc : 0.7206\n",
            "step : 16700 train loss : 0.3765 acc : 0.9616 | Val loss : 1.3078 acc : 0.7121\n",
            "step : 16800 train loss : 0.3682 acc : 0.9660 | Val loss : 1.3463 acc : 0.7179\n",
            "step : 16900 train loss : 0.3830 acc : 0.9592 | Val loss : 1.3661 acc : 0.7167\n",
            "step : 17000 train loss : 0.3623 acc : 0.9685 | Val loss : 1.3091 acc : 0.7215\n",
            "step : 17100 train loss : 0.3863 acc : 0.9547 | Val loss : 1.4384 acc : 0.7142\n",
            "step : 17200 train loss : 0.3671 acc : 0.9626 | Val loss : 1.3592 acc : 0.7143\n",
            "step : 17300 train loss : 0.3715 acc : 0.9635 | Val loss : 1.3686 acc : 0.7146\n",
            "step : 17400 train loss : 0.3726 acc : 0.9614 | Val loss : 1.3866 acc : 0.7108\n",
            "step : 17500 train loss : 0.3517 acc : 0.9706 | Val loss : 1.3327 acc : 0.7184\n",
            "step : 17600 train loss : 0.3375 acc : 0.9767 | Val loss : 1.3983 acc : 0.7145\n",
            "step : 17700 train loss : 0.3515 acc : 0.9685 | Val loss : 1.3928 acc : 0.7137\n",
            "step : 17800 train loss : 0.3235 acc : 0.9813 | Val loss : 1.3936 acc : 0.7205\n",
            "step : 17900 train loss : 0.3322 acc : 0.9764 | Val loss : 1.3796 acc : 0.7202\n",
            "step : 18000 train loss : 0.3522 acc : 0.9693 | Val loss : 1.3809 acc : 0.7196\n",
            "step : 18100 train loss : 0.3467 acc : 0.9698 | Val loss : 1.4732 acc : 0.7081\n",
            "step : 18200 train loss : 0.3226 acc : 0.9792 | Val loss : 1.4016 acc : 0.7244\n",
            "step : 18300 train loss : 0.3364 acc : 0.9756 | Val loss : 1.4265 acc : 0.7120\n",
            "step : 18400 train loss : 0.3411 acc : 0.9727 | Val loss : 1.4233 acc : 0.7111\n",
            "step : 18500 train loss : 0.3247 acc : 0.9791 | Val loss : 1.4215 acc : 0.7206\n",
            "step : 18600 train loss : 0.3078 acc : 0.9847 | Val loss : 1.4172 acc : 0.7292\n",
            "step : 18700 train loss : 0.3143 acc : 0.9820 | Val loss : 1.4550 acc : 0.7165\n",
            "step : 18800 train loss : 0.3171 acc : 0.9816 | Val loss : 1.4469 acc : 0.7178\n",
            "step : 18900 train loss : 0.3236 acc : 0.9825 | Val loss : 1.4212 acc : 0.7187\n",
            "step : 19000 train loss : 0.3433 acc : 0.9707 | Val loss : 1.4277 acc : 0.7163\n",
            "step : 19100 train loss : 0.3048 acc : 0.9847 | Val loss : 1.4451 acc : 0.7249\n",
            "step : 19200 train loss : 0.3266 acc : 0.9765 | Val loss : 1.4831 acc : 0.7152\n",
            "step : 19300 train loss : 0.3041 acc : 0.9874 | Val loss : 1.4231 acc : 0.7237\n",
            "step : 19400 train loss : 0.3406 acc : 0.9684 | Val loss : 1.4772 acc : 0.7151\n",
            "step : 19500 train loss : 0.3020 acc : 0.9875 | Val loss : 1.4262 acc : 0.7237\n",
            "step : 19600 train loss : 0.3065 acc : 0.9853 | Val loss : 1.4433 acc : 0.7219\n",
            "step : 19700 train loss : 0.3016 acc : 0.9857 | Val loss : 1.5239 acc : 0.7215\n",
            "step : 19800 train loss : 0.3069 acc : 0.9861 | Val loss : 1.4415 acc : 0.7188\n",
            "step : 19900 train loss : 0.3025 acc : 0.9874 | Val loss : 1.4513 acc : 0.7135\n",
            "step : 20000 train loss : 0.3235 acc : 0.9779 | Val loss : 1.4893 acc : 0.7194\n",
            "step : 20100 train loss : 0.3176 acc : 0.9766 | Val loss : 1.5249 acc : 0.7110\n",
            "step : 20200 train loss : 0.2977 acc : 0.9862 | Val loss : 1.4461 acc : 0.7213\n",
            "step : 20300 train loss : 0.3072 acc : 0.9841 | Val loss : 1.4229 acc : 0.7178\n",
            "step : 20400 train loss : 0.2991 acc : 0.9863 | Val loss : 1.4929 acc : 0.7152\n",
            "step : 20500 train loss : 0.3116 acc : 0.9814 | Val loss : 1.5049 acc : 0.7256\n",
            "step : 20600 train loss : 0.2924 acc : 0.9883 | Val loss : 1.5295 acc : 0.7238\n",
            "step : 20700 train loss : 0.2856 acc : 0.9896 | Val loss : 1.5425 acc : 0.7172\n",
            "step : 20800 train loss : 0.2898 acc : 0.9889 | Val loss : 1.4846 acc : 0.7242\n",
            "step : 20900 train loss : 0.3064 acc : 0.9825 | Val loss : 1.5200 acc : 0.7126\n",
            "step : 21000 train loss : 0.3034 acc : 0.9825 | Val loss : 1.4199 acc : 0.7223\n",
            "step : 21100 train loss : 0.2928 acc : 0.9874 | Val loss : 1.4659 acc : 0.7219\n",
            "step : 21200 train loss : 0.2873 acc : 0.9884 | Val loss : 1.5057 acc : 0.7187\n",
            "step : 21300 train loss : 0.2909 acc : 0.9879 | Val loss : 1.4855 acc : 0.7187\n",
            "step : 21400 train loss : 0.3524 acc : 0.9658 | Val loss : 1.5673 acc : 0.7072\n",
            "step : 21500 train loss : 0.2837 acc : 0.9897 | Val loss : 1.5144 acc : 0.7255\n",
            "step : 21600 train loss : 0.2920 acc : 0.9861 | Val loss : 1.5329 acc : 0.7144\n",
            "step : 21700 train loss : 0.2813 acc : 0.9906 | Val loss : 1.4906 acc : 0.7240\n",
            "step : 21800 train loss : 0.3052 acc : 0.9806 | Val loss : 1.5335 acc : 0.7130\n",
            "step : 21900 train loss : 0.2891 acc : 0.9858 | Val loss : 1.5378 acc : 0.7232\n",
            "step : 22000 train loss : 0.2905 acc : 0.9855 | Val loss : 1.5098 acc : 0.7220\n",
            "step : 22100 train loss : 0.2756 acc : 0.9920 | Val loss : 1.5350 acc : 0.7259\n",
            "step : 22200 train loss : 0.2721 acc : 0.9923 | Val loss : 1.5553 acc : 0.7261\n",
            "step : 22300 train loss : 0.2936 acc : 0.9847 | Val loss : 1.4875 acc : 0.7201\n",
            "step : 22400 train loss : 0.2754 acc : 0.9927 | Val loss : 1.4956 acc : 0.7174\n",
            "step : 22500 train loss : 0.2773 acc : 0.9915 | Val loss : 1.5300 acc : 0.7204\n",
            "step : 22600 train loss : 0.2729 acc : 0.9925 | Val loss : 1.5537 acc : 0.7235\n",
            "step : 22700 train loss : 0.2658 acc : 0.9941 | Val loss : 1.5528 acc : 0.7215\n",
            "step : 22800 train loss : 0.2714 acc : 0.9928 | Val loss : 1.5233 acc : 0.7177\n",
            "step : 22900 train loss : 0.3475 acc : 0.9622 | Val loss : 1.5349 acc : 0.7023\n",
            "step : 23000 train loss : 0.2843 acc : 0.9882 | Val loss : 1.5223 acc : 0.7140\n",
            "step : 23100 train loss : 0.2636 acc : 0.9956 | Val loss : 1.5152 acc : 0.7227\n",
            "step : 23200 train loss : 0.2685 acc : 0.9931 | Val loss : 1.5108 acc : 0.7250\n",
            "step : 23300 train loss : 0.2661 acc : 0.9937 | Val loss : 1.5038 acc : 0.7226\n",
            "step : 23400 train loss : 0.2637 acc : 0.9937 | Val loss : 1.5332 acc : 0.7280\n",
            "step : 23500 train loss : 0.2932 acc : 0.9856 | Val loss : 1.5182 acc : 0.7104\n",
            "step : 23600 train loss : 0.2788 acc : 0.9892 | Val loss : 1.5425 acc : 0.7136\n",
            "step : 23700 train loss : 0.2746 acc : 0.9909 | Val loss : 1.5650 acc : 0.7210\n",
            "step : 23800 train loss : 0.2761 acc : 0.9894 | Val loss : 1.5853 acc : 0.7138\n",
            "step : 23900 train loss : 0.2670 acc : 0.9927 | Val loss : 1.5410 acc : 0.7204\n",
            "step : 24000 train loss : 0.2893 acc : 0.9844 | Val loss : 1.5693 acc : 0.7123\n",
            "step : 24100 train loss : 0.2697 acc : 0.9916 | Val loss : 1.5160 acc : 0.7250\n",
            "step : 24200 train loss : 0.2716 acc : 0.9907 | Val loss : 1.5932 acc : 0.7163\n",
            "step : 24300 train loss : 0.2619 acc : 0.9937 | Val loss : 1.5754 acc : 0.7173\n",
            "step : 24400 train loss : 0.3303 acc : 0.9657 | Val loss : 1.6984 acc : 0.6920\n",
            "step : 24500 train loss : 0.2569 acc : 0.9962 | Val loss : 1.5361 acc : 0.7207\n",
            "step : 24600 train loss : 0.2635 acc : 0.9927 | Val loss : 1.5952 acc : 0.7161\n",
            "step : 24700 train loss : 0.2669 acc : 0.9925 | Val loss : 1.5695 acc : 0.7181\n",
            "step : 24800 train loss : 0.2690 acc : 0.9906 | Val loss : 1.5607 acc : 0.7139\n",
            "step : 24900 train loss : 0.2991 acc : 0.9797 | Val loss : 1.5238 acc : 0.7121\n",
            "step : 25000 train loss : 0.2692 acc : 0.9908 | Val loss : 1.5083 acc : 0.7245\n",
            "step : 25100 train loss : 0.2513 acc : 0.9973 | Val loss : 1.5220 acc : 0.7245\n",
            "step : 25200 train loss : 0.2588 acc : 0.9947 | Val loss : 1.5603 acc : 0.7191\n",
            "step : 25300 train loss : 0.2707 acc : 0.9894 | Val loss : 1.6364 acc : 0.7127\n",
            "step : 25400 train loss : 0.2609 acc : 0.9936 | Val loss : 1.5288 acc : 0.7210\n",
            "step : 25500 train loss : 0.2568 acc : 0.9948 | Val loss : 1.5822 acc : 0.7276\n",
            "step : 25600 train loss : 0.2586 acc : 0.9936 | Val loss : 1.5843 acc : 0.7093\n",
            "step : 25700 train loss : 0.2466 acc : 0.9973 | Val loss : 1.5559 acc : 0.7225\n",
            "step : 25800 train loss : 0.2538 acc : 0.9950 | Val loss : 1.5451 acc : 0.7194\n",
            "step : 25900 train loss : 0.2615 acc : 0.9923 | Val loss : 1.5743 acc : 0.7181\n",
            "step : 26000 train loss : 0.2676 acc : 0.9904 | Val loss : 1.5249 acc : 0.7152\n",
            "step : 26100 train loss : 0.2661 acc : 0.9901 | Val loss : 1.5925 acc : 0.7176\n",
            "step : 26200 train loss : 0.2619 acc : 0.9914 | Val loss : 1.6512 acc : 0.7106\n",
            "step : 26300 train loss : 0.2521 acc : 0.9952 | Val loss : 1.5530 acc : 0.7221\n",
            "step : 26400 train loss : 0.2621 acc : 0.9920 | Val loss : 1.5658 acc : 0.7217\n",
            "step : 26500 train loss : 0.2737 acc : 0.9874 | Val loss : 1.6037 acc : 0.7108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ab0ec1e7af4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n\u001b[1;32m     23\u001b[0m                                                                       \u001b[0mys\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                       phase_train : False})\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Validate train dataset : extract randomly 10000 samples from train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3IzIVKHoYgiV"
      },
      "source": [
        "<hr>\n",
        "<div style = \"background-image: url('https://algorithmai.io/static/media/logo.665798c4.png');background-repeat: no-repeat; background-position: right; background-size: 220px 40px; padding : 5px 10px 5px 5px;\">\n",
        "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
        "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/06/17\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-lE48e_vHpa",
        "colab_type": "text"
      },
      "source": [
        "acc  <br>\n",
        "\n",
        "| trade-off step | LR | conv1 | fc  | test acc | test loss | vali acc | vali loss |\n",
        "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
        "| 1700 | 0.0005 | conv | fc  | 0.09 | 2.3026 | 0.1 | 2.3026 |\n",
        "|1600 | 0.0001 | conv | fc  | 0.1 | 2.3026 | 0.1 | 2.3026 |\n",
        "| 6900 | 0.0001 | conv | fc  | 0.8038 | 0.7374 | 0.7214  | 1.0139  |\n",
        "| 9700 | 0.0001 | conv | fc  | 0.8210 | 0.7939 | 0.7157  |1.1145  |\n",
        "\n"
      ]
    }
  ]
}