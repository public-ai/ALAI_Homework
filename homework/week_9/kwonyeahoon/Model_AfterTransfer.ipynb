{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_SameModel.ipynb의 사본의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5DbTV6Y8_fm"
      },
      "source": [
        "## CIFAR-10, 100 학습시키기\n",
        "\n",
        "## Objective\n",
        "\n",
        "1.[CIFAR -10 Data](https://www.cs.toronto.edu/~kriz/cifar.html) 을 Convolution Neural Network 을 이용해 학습해봅니다.\n",
        "----\n",
        "![Imgur](https://i.imgur.com/yy09iLz.png)\n",
        "\n",
        "\n",
        "- loss 가 가장 작은 model 을 저장합니다.\n",
        "- 목표 accuracy 는 75% 입니다. \n",
        "​\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-syRx_WBWebm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okPBT4DkqPmu"
      },
      "source": [
        "# Load Cifar-10 dataset \n",
        " - cifar 10 dataset 을 다운로드 합니다. \n",
        " - normalize 을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8KwcN7baxvN",
        "colab_type": "code",
        "outputId": "7053f6cc-b57d-4470-9baa-944da76d4118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "import tensorboardcolab\n",
        "#content/tensorboard\n",
        "tbc=tensorboardcolab.TensorBoardColab(graph_path='./tensorboard')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://b2c4dd98.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ApD4EzRiqOGj",
        "outputId": "e4492ffa-3595-47fc-bdbd-610656b72382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# load cifar10 dataset \n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# reshape (None, 1) -> (None)\n",
        "y_train = np.reshape(y_train, (-1))\n",
        "y_test = np.reshape(y_test, (-1))\n",
        "\n",
        "# normalization \n",
        "x_train, x_test = x_train/255. , x_test/255.\n",
        "\n",
        "# N class\n",
        "n_classes = 10\n",
        "print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n",
        "print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n",
        "print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n",
        "print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image shape : (50000, 32, 32, 3), label shape : (50000,) \n",
            "image shape : (10000, 32, 32, 3), label shape : (10000,) \n",
            "train minimun : 0.0, train_maximum : 1.0 \n",
            "tests minimun : 0.0, test_maximum : 1.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb-_UZEUQ_N",
        "colab_type": "text"
      },
      "source": [
        "# DataProvider "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmE4_GikBI6I",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "np.random.seed(0)\n",
        "class DataProvider(object):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_fix = images # fix data저장\n",
        "        self.labels_fix = labels # fix data저장\n",
        "        self.len_ = self.images.shape[0] # 총 이미지 갯수저장\n",
        "        self.len_fix = copy.deepcopy(self.len_)  # fix길이 저장\n",
        "        self.ind_range = self.images.shape[0] # index\n",
        "        self.ind = [ x for x in range(self.ind_range)]\n",
        "        np.random.shuffle(self.ind) # index shuffle\n",
        "        self.images = self.images[self.ind, :] # shuffle 수행\n",
        "        self.labels = self.labels[self.ind]\n",
        "        self.images = list(self.images) #list화 시킴(del() 등 list연산 사용필요)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        #fix me#\n",
        "        if self.len_ <= batch_size :\n",
        "            ### 해당 epoch의 마지막 batch case ###\n",
        "            # 1.나머지 모두 내보냄\n",
        "            out_batch_image = self.images[:][:]\n",
        "            out_batch_labels = self.labels[:]\n",
        "            del(self.images[:])\n",
        "            del(self.labels[:])\n",
        "            \n",
        "            # 2.다음 epoch의 shuffle 수행\n",
        "            self.len_ = self.len_fix\n",
        "            self.images = self.images_fix\n",
        "            self.labels = self.labels_fix\n",
        "            self.ind = [x for x in range(self.ind_range)]\n",
        "            np.random.shuffle(self.ind)\n",
        "            self.images = self.images[self.ind,:] # shuffle 수행\n",
        "            self.labels = self.labels[self.ind]\n",
        "            self.images = list(self.images)\n",
        "            self.labels = list(self.labels)\n",
        "        else : \n",
        "            # 일반 batch수행\n",
        "            out_batch_image, out_batch_labels = self.images[:batch_size][:], self.labels[:batch_size] # slice함\n",
        "            del(self.images[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            del(self.labels[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            self.len_ = self.len_ - batch_size # 길이줄임\n",
        "            out_batch_labels = np.array(out_batch_labels)\n",
        "            out_batch_image = np.array(out_batch_image)\n",
        "        return out_batch_image, out_batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b0Hku8a98_fo"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "설계한 모델을 표로 작성합니다. \n",
        "\n",
        "- 목표 Receptive Field : ? <br>\n",
        "- Convolution Phase 후  출력 크기  :  ? <br>\n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| ? |?| ?|? |?  | ? |?| ?|\n",
        "\n",
        "\n",
        "- 모델 설계가 끝나면 간단한 그림을 작성해 아래에 붙여주세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDhKG-rSUQ_P",
        "colab_type": "text"
      },
      "source": [
        "예시1) \n",
        "\n",
        "\n",
        "- 목표 Receptive Field : 28 <br>\n",
        "- Convolution Phase 후  출력 크기  :  4 <br>\n",
        "- Regularization  : L2 \n",
        "- Batch size : 120\n",
        "- Learning rate : 0.0001 \n",
        "- Data normalization : min max normalization \n",
        "- Standardization : None \n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| c1 |conv| 64| 3x3| 1  | SAME | None| 32x32 |\n",
        "| s2 |max-pooling| None| 3x3| 2  | SAME | None|16x16 | \n",
        "| c3 |conv| 128| 3x3| 2  | SAME |NOne |16x16 | \n",
        "| s4 |max-pooling| None| 3x3| 2  | SAME | None|8 x8 | \n",
        "| c5 |conv| 128| 3x3| 2  | SAME | None |8 x8 | \n",
        "| s6 |conv| 256| 3x3| 2  | SAME | None |4 x 4 | \n",
        "| c7 |conv| 256| 1x1| 2  | SAME | None |4 x 4 | \n",
        "| f8 ||| | FC 256  | |  || \n",
        "| f8 ||| | Dropout 0.7 | |  || \n",
        "| f9 ||| | FC 256  | |  || \n",
        "| f9 ||| | Dropout 0.6 | |  || \n",
        "| f10||| | FC 10   | |  || \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Imgur](https://i.imgur.com/yqrIm5u.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2qtnd7UQ_Q",
        "colab_type": "text"
      },
      "source": [
        "# Convolution layer\n",
        "- convolution layer helper function 을 정의합니다.\n",
        "- 위 설계한 convolution layer 을 구현합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rzr0by6Kyv6J",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# convolution helper function\n",
        "def conv(input_xs ,units, k, s, padding, activation, name):\n",
        "    layer = tf.layers.Conv2D(filters = units, kernel_size = k, strides = s,\n",
        "                             padding = padding, activation = activation, name = name )(input_xs)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmKroxF-CDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define input placeholder \n",
        "xs = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 3])\n",
        "ys = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "#ys_one_hot = tf.one_hot()\n",
        "lr = tf.placeholder(dtype = tf.float32, shape = ())\n",
        "phase_train = tf.placeholder(tf.bool, shape = (), name = 'phase_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AJcmfqHJ9lLm",
        "outputId": "ac4a64b5-be8d-400a-d00b-e5b06a1f4ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Model implementation \n",
        "# convolution Neural Network \n",
        "# 자신이 설계한 모형을 구현해주세요.\n",
        "with tf.variable_scope('VGG_block-1') :\n",
        "    layer = conv(xs, 10, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv' )\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-2') :\n",
        "    layer = conv(pooling, 50, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-3') :\n",
        "    layer = conv(pooling, 100, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 100, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-4') :\n",
        "    layer = conv(pooling, 200, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 200, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-5') :\n",
        "    layer = conv(pooling, 400, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer,400, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')    \n",
        "    #pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "top_conv = tf.identity(pooling, 'top_conv') # 마지막 layer 을 top conv 에 넣습니다.\n",
        "tf.shape(top_conv)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0616 16:45:40.514133 139648462849920 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Shape:0' shape=(4,) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRukJjRZUQ_V",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layer\n",
        "- 설계한 fully connected layer 을 구현합니다.\n",
        "- dropout 을 적용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNx29qxUQ_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(flat_layer, units, initializer_, layer_name):\n",
        "    dense = tf.layers.Dense(units = units, activation = tf.nn.relu, kernel_initializer = initializer_, name = layer_name)(flat_layer)\n",
        "    return dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL1M4gvUQ_X",
        "colab_type": "code",
        "outputId": "23c537ae-7f61-4011-dbca-b69b36f378c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# flat layer \n",
        "flatten_layer = tf.layers.flatten(top_conv)\n",
        "#print(\"tf.shape(flat_layer) :\", tf.shape(flatten_layer))\n",
        "\n",
        "# fully connected layer 1\n",
        "fc_initializer = tf.initializers.he_normal()\n",
        "fc_layer_1 = fc(flat_layer = flatten_layer, units = 2000, initializer_ = fc_initializer, layer_name = \"FC1\" )\n",
        "fc_layer_1 = tf.layers.dropout(fc_layer_1,rate=0.5, training=phase_train)\n",
        "\n",
        "# fix me # 자신이 설계한 fully connected layer 을 구현합니다.  \n",
        "\n",
        "fc_layer_2 = fc(flat_layer = fc_layer_1, units = 500, initializer_ = fc_initializer, layer_name = \"FC2\" )\n",
        "fc_layer_2 = tf.layers.dropout(fc_layer_2,rate=0.5, training=phase_train)\n",
        "\n",
        "fc_layer_3 = fc(flat_layer = fc_layer_2, units = 100, initializer_ = fc_initializer, layer_name = \"FC3\" )\n",
        "\n",
        "fc_layer_4 = fc(flat_layer = fc_layer_3, units = 10, initializer_ = fc_initializer, layer_name = \"FC4\" )\n",
        "\n",
        "logits= tf.identity(fc_layer_4, 'logits')\n",
        "\n",
        "ys = tf.cast(ys, tf.int32)\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(ys, logits)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0616 16:45:41.003637 139648462849920 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:1: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0616 16:45:41.626222 139648462849920 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:7: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0616 16:45:41.769300 139648462849920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-625ULtUQ_Z",
        "colab_type": "text"
      },
      "source": [
        "#  Loss function \n",
        "- loss function 을 정의합니다. L2 regularization 을 사용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2DkXG-UQ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_reg = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n",
        "l2_beta = 5e-4\n",
        "\n",
        "#loss \n",
        "# L2 reularization \n",
        "loss = loss + (l2_beta * l2_reg)\n",
        "loss = tf.identity(loss, name = 'loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Oke8OgUQ_e",
        "colab_type": "text"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_TMXVtX1pXK",
        "colab": {}
      },
      "source": [
        "# metric\n",
        "pred = tf.nn.softmax(logits)\n",
        "one_hot_label = tf.one_hot(ys, 10)\n",
        "pred_arg = tf.argmax(pred, axis = 1)\n",
        "label_arg = tf.argmax(one_hot_label, axis = 1)\n",
        "eq = tf.cast(tf.equal(pred_arg, label_arg), dtype = tf.float32)\n",
        "acc = tf.reduce_mean(eq, axis =0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfoWSrLUQ_h",
        "colab_type": "text"
      },
      "source": [
        "# Add tensor to Tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqUXVsUTUQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add accuracy to tensorboard nodes \n",
        "#fix me #\n",
        "acc_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "# add loss to tensorboard nodes \n",
        "#fix me #\n",
        "loss_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "\n",
        "#merge all tensorboard nodes \n",
        "#fix me #\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGeaVRCOUQ_j",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83C5s7mfUQ_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_op : adamoptimizer \n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6N1fBH2BBfX6"
      },
      "source": [
        "# Session open "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wiz-2rkLBdTw",
        "outputId": "e92c8ef9-3532-4ce7-c5f9-e76b5d46ef89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "#초기학습\n",
        "#init_g = tf.global_variables_initializer() # : globalal initializer\n",
        "#init_l = tf.local_variables_initializer() # : local initializer\n",
        "#sess.run(init_l)\n",
        "#sess.run([init_l,init_g])\n",
        "\n",
        "# saver \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "#Weight Transfer \n",
        "saver.restore(sess, './ttee/model-12500')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0616 16:45:42.602114 139648462849920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFmQDdJDUQ_o",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard Filewriter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPH7CLomUQ_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorboard \n",
        "train_writer=tf.summary.FileWriter(logdir='./tensorboard/train')\n",
        "\n",
        "test_writer=tf.summary.FileWriter(logdir='./tensorboard/test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P88ef2rRUQ_q",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yv99AR4A_Y7P",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1387
        },
        "outputId": "47ba64e8-e980-4f02-d161-94c25af87a3b"
      },
      "source": [
        "dataprovider = DataProvider(images=x_train, labels=y_train)\n",
        "#save_root_folder = #fix me # : models saved folder \n",
        "\n",
        "# hparam \n",
        "batch_size = 100\n",
        "min_loss = 1000000.0\n",
        "learning_rate = 0.000001\n",
        "\n",
        "np.random.seed(0)\n",
        "#local variable initialize\n",
        "for i in range(50000):\n",
        "    batch_xs, batch_ys = dataprovider.next_batch(batch_size)\n",
        "    # training \n",
        "    _= sess.run(train_op, feed_dict = {xs : batch_xs,\n",
        "                                        ys : batch_ys,\n",
        "                                        lr : learning_rate,\n",
        "                                        phase_train : True})\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        # Validate validation dataset \n",
        "        fetches=[loss, acc, merged]\n",
        "        val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n",
        "                                                                      ys : y_test,\n",
        "                                                                      phase_train : False})\n",
        "\n",
        "        # Validate train dataset : extract randomly 10000 samples from train dataset \n",
        "        ran = [ x for x in range(0, 50000)]\n",
        "        nansu = np.random.choice(ran, size = 10000, replace=False)\n",
        "        train_xtest, train_ytest = x_train[nansu], y_train[nansu]\n",
        "        train_loss, train_acc, train_merged = sess.run([loss, acc, merged], feed_dict = { xs : train_xtest,\n",
        "                                                                                          ys : train_ytest,\n",
        "                                                                                        phase_train : False})\n",
        "       \n",
        "        print('step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'.\\\n",
        "        format(i, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "        # Save Model \n",
        "        if val_loss < min_loss : #fix me # : when val_loss < min_loss \n",
        "            min_loss = val_loss\n",
        "            save_path = './ttee/model'\n",
        "            saver.save(sess, save_path, global_step=i)\n",
        "            print('model save!')\n",
        "            \n",
        "        # Add values to tensorboard \n",
        "        train_writer.add_summary(train_merged, i)\n",
        "        test_writer.add_summary(val_merged, i)\n",
        "        train_writer.flush()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 0 train loss : 0.5761 acc : 0.8926 | Val loss : 1.0910 acc : 0.7236\n",
            "model save!\n",
            "step : 100 train loss : 0.5532 acc : 0.9046 | Val loss : 1.0804 acc : 0.7263\n",
            "model save!\n",
            "step : 200 train loss : 0.5605 acc : 0.8998 | Val loss : 1.0773 acc : 0.7278\n",
            "model save!\n",
            "step : 300 train loss : 0.5474 acc : 0.9019 | Val loss : 1.0761 acc : 0.7287\n",
            "model save!\n",
            "step : 400 train loss : 0.5462 acc : 0.9025 | Val loss : 1.0758 acc : 0.7280\n",
            "model save!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0616 16:46:02.262970 139648462849920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step : 500 train loss : 0.5524 acc : 0.9008 | Val loss : 1.0756 acc : 0.7282\n",
            "model save!\n",
            "step : 600 train loss : 0.5437 acc : 0.9070 | Val loss : 1.0754 acc : 0.7297\n",
            "model save!\n",
            "step : 700 train loss : 0.5444 acc : 0.9066 | Val loss : 1.0761 acc : 0.7302\n",
            "step : 800 train loss : 0.5303 acc : 0.9093 | Val loss : 1.0770 acc : 0.7300\n",
            "step : 900 train loss : 0.5319 acc : 0.9107 | Val loss : 1.0777 acc : 0.7298\n",
            "step : 1000 train loss : 0.5376 acc : 0.9084 | Val loss : 1.0784 acc : 0.7307\n",
            "step : 1100 train loss : 0.5275 acc : 0.9125 | Val loss : 1.0784 acc : 0.7311\n",
            "step : 1200 train loss : 0.5271 acc : 0.9103 | Val loss : 1.0791 acc : 0.7283\n",
            "step : 1300 train loss : 0.5260 acc : 0.9077 | Val loss : 1.0798 acc : 0.7295\n",
            "step : 1400 train loss : 0.5239 acc : 0.9130 | Val loss : 1.0808 acc : 0.7299\n",
            "step : 1500 train loss : 0.5242 acc : 0.9122 | Val loss : 1.0809 acc : 0.7301\n",
            "step : 1600 train loss : 0.5309 acc : 0.9087 | Val loss : 1.0816 acc : 0.7311\n",
            "step : 1700 train loss : 0.5219 acc : 0.9123 | Val loss : 1.0831 acc : 0.7306\n",
            "step : 1800 train loss : 0.5204 acc : 0.9151 | Val loss : 1.0834 acc : 0.7295\n",
            "step : 1900 train loss : 0.5321 acc : 0.9114 | Val loss : 1.0839 acc : 0.7289\n",
            "step : 2000 train loss : 0.5177 acc : 0.9161 | Val loss : 1.0845 acc : 0.7308\n",
            "step : 2100 train loss : 0.5308 acc : 0.9108 | Val loss : 1.0859 acc : 0.7293\n",
            "step : 2200 train loss : 0.5204 acc : 0.9114 | Val loss : 1.0869 acc : 0.7301\n",
            "step : 2300 train loss : 0.5174 acc : 0.9159 | Val loss : 1.0867 acc : 0.7321\n",
            "step : 2400 train loss : 0.5164 acc : 0.9145 | Val loss : 1.0867 acc : 0.7307\n",
            "step : 2500 train loss : 0.5182 acc : 0.9152 | Val loss : 1.0879 acc : 0.7302\n",
            "step : 2600 train loss : 0.5241 acc : 0.9142 | Val loss : 1.0888 acc : 0.7296\n",
            "step : 2700 train loss : 0.5208 acc : 0.9137 | Val loss : 1.0890 acc : 0.7322\n",
            "step : 2800 train loss : 0.5097 acc : 0.9180 | Val loss : 1.0898 acc : 0.7309\n",
            "step : 2900 train loss : 0.5232 acc : 0.9138 | Val loss : 1.0895 acc : 0.7297\n",
            "step : 3000 train loss : 0.5222 acc : 0.9148 | Val loss : 1.0899 acc : 0.7307\n",
            "step : 3100 train loss : 0.5188 acc : 0.9165 | Val loss : 1.0907 acc : 0.7295\n",
            "step : 3200 train loss : 0.5191 acc : 0.9143 | Val loss : 1.0910 acc : 0.7309\n",
            "step : 3300 train loss : 0.5161 acc : 0.9163 | Val loss : 1.0922 acc : 0.7313\n",
            "step : 3400 train loss : 0.5199 acc : 0.9143 | Val loss : 1.0929 acc : 0.7307\n",
            "step : 3500 train loss : 0.5168 acc : 0.9174 | Val loss : 1.0933 acc : 0.7313\n",
            "step : 3600 train loss : 0.5082 acc : 0.9183 | Val loss : 1.0934 acc : 0.7307\n",
            "step : 3700 train loss : 0.5084 acc : 0.9167 | Val loss : 1.0942 acc : 0.7303\n",
            "step : 3800 train loss : 0.5147 acc : 0.9171 | Val loss : 1.0940 acc : 0.7319\n",
            "step : 3900 train loss : 0.5075 acc : 0.9195 | Val loss : 1.0943 acc : 0.7310\n",
            "step : 4000 train loss : 0.4993 acc : 0.9207 | Val loss : 1.0952 acc : 0.7308\n",
            "step : 4100 train loss : 0.5193 acc : 0.9149 | Val loss : 1.0968 acc : 0.7318\n",
            "step : 4200 train loss : 0.4945 acc : 0.9214 | Val loss : 1.0982 acc : 0.7307\n",
            "step : 4300 train loss : 0.5184 acc : 0.9136 | Val loss : 1.0974 acc : 0.7296\n",
            "step : 4400 train loss : 0.5072 acc : 0.9192 | Val loss : 1.0984 acc : 0.7305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a1cf59b8fe5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n\u001b[1;32m     23\u001b[0m                                                                       \u001b[0mys\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                       phase_train : False})\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Validate train dataset : extract randomly 10000 samples from train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3IzIVKHoYgiV"
      },
      "source": [
        "<hr>\n",
        "<div style = \"background-image: url('https://algorithmai.io/static/media/logo.665798c4.png');background-repeat: no-repeat; background-position: right; background-size: 220px 40px; padding : 5px 10px 5px 5px;\">\n",
        "    Copyright(c) 2019 by Public AI. All rights reserved.<br>\n",
        "    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/06/17\n",
        "</div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-lE48e_vHpa",
        "colab_type": "text"
      },
      "source": [
        "acc  <br> step : 6900 train loss : 0.7374 acc : 0.8038 | Val loss : 1.0139 acc : 0.7214\n",
        "\n",
        "| trade-off step | LR | conv1 | fc  | test acc | test loss | vali acc | vali loss |\n",
        "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
        "| 1700 | 0.0005 | conv | fc  | 0.09 | 2.3026 | 0.1 | 2.3026 |\n",
        "|1600 | 0.0001 | conv | fc  | 0.1 | 2.3026 | 0.1 | 2.3026 |\n",
        "| 6900 | 0.0001 | conv | fc  | 0.8038 | 0.7374 | 0.7214  | 1.0139  |\n",
        "| 9700 | 0.0001 | conv | fc  | 0.8210 | 0.7939 | 0.7157  |1.1145  |\n",
        "| 49900 | 0.0001 | conv | fc  | 0.8366 | 0.9351 | 1.3564  |0.7031  |\n",
        "| ㄴ> transfer_learning step 0  | 0.00001 -> 0.000001 |  |  |  0.8464  |  0.9174 |1.3278| 0.7111|\n",
        "\n"
      ]
    }
  ]
}