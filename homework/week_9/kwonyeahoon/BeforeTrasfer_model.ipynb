{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BeforeTrasfer_model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5DbTV6Y8_fm"
      },
      "source": [
        "## CIFAR-10, 100 학습시키기\n",
        "\n",
        "## Objective\n",
        "\n",
        "1.[CIFAR -10 Data](https://www.cs.toronto.edu/~kriz/cifar.html) 을 Convolution Neural Network 을 이용해 학습해봅니다.\n",
        "----\n",
        "![Imgur](https://i.imgur.com/yy09iLz.png)\n",
        "\n",
        "\n",
        "- loss 가 가장 작은 model 을 저장합니다.\n",
        "- 목표 accuracy 는 75% 입니다. \n",
        "​\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-syRx_WBWebm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okPBT4DkqPmu"
      },
      "source": [
        "# Load Cifar-10 dataset \n",
        " - cifar 10 dataset 을 다운로드 합니다. \n",
        " - normalize 을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8KwcN7baxvN",
        "colab_type": "code",
        "outputId": "e4166cad-3ae5-4bba-f635-fec9b1b15ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "import tensorboardcolab\n",
        "#content/tensorboard\n",
        "tbc=tensorboardcolab.TensorBoardColab(graph_path='./tensorboard')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://39adac04.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ApD4EzRiqOGj",
        "outputId": "4ffa2fd5-dd3f-4d45-a2ec-f5877c1ce892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# load cifar10 dataset \n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# reshape (None, 1) -> (None)\n",
        "y_train = np.reshape(y_train, (-1))\n",
        "y_test = np.reshape(y_test, (-1))\n",
        "\n",
        "# normalization \n",
        "x_train, x_test = x_train/255. , x_test/255.\n",
        "\n",
        "# N class\n",
        "n_classes = 10\n",
        "print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n",
        "print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n",
        "print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n",
        "print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "image shape : (50000, 32, 32, 3), label shape : (50000,) \n",
            "image shape : (10000, 32, 32, 3), label shape : (10000,) \n",
            "train minimun : 0.0, train_maximum : 1.0 \n",
            "tests minimun : 0.0, test_maximum : 1.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb-_UZEUQ_N",
        "colab_type": "text"
      },
      "source": [
        "# DataProvider "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JmE4_GikBI6I",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "np.random.seed(0)\n",
        "class DataProvider(object):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_fix = images # fix data저장\n",
        "        self.labels_fix = labels # fix data저장\n",
        "        self.len_ = self.images.shape[0] # 총 이미지 갯수저장\n",
        "        self.len_fix = copy.deepcopy(self.len_)  # fix길이 저장\n",
        "        self.ind_range = self.images.shape[0] # index\n",
        "        self.ind = [ x for x in range(self.ind_range)]\n",
        "        np.random.shuffle(self.ind) # index shuffle\n",
        "        self.images = self.images[self.ind, :] # shuffle 수행\n",
        "        self.labels = self.labels[self.ind]\n",
        "        self.images = list(self.images) #list화 시킴(del() 등 list연산 사용필요)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        #fix me#\n",
        "        if self.len_ <= batch_size :\n",
        "            ### 해당 epoch의 마지막 batch case ###\n",
        "            # 1.나머지 모두 내보냄\n",
        "            out_batch_image = self.images[:][:]\n",
        "            out_batch_labels = self.labels[:]\n",
        "            del(self.images[:])\n",
        "            del(self.labels[:])\n",
        "            \n",
        "            # 2.다음 epoch의 shuffle 수행\n",
        "            self.len_ = self.len_fix\n",
        "            self.images = self.images_fix\n",
        "            self.labels = self.labels_fix\n",
        "            self.ind = [x for x in range(self.ind_range)]\n",
        "            np.random.shuffle(self.ind)\n",
        "            self.images = self.images[self.ind,:] # shuffle 수행\n",
        "            self.labels = self.labels[self.ind]\n",
        "            self.images = list(self.images)\n",
        "            self.labels = list(self.labels)\n",
        "        else : \n",
        "            # 일반 batch수행\n",
        "            out_batch_image, out_batch_labels = self.images[:batch_size][:], self.labels[:batch_size] # slice함\n",
        "            del(self.images[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            del(self.labels[:batch_size]) # 해당 배치만큼 data삭제\n",
        "            self.len_ = self.len_ - batch_size # 길이줄임\n",
        "            out_batch_labels = np.array(out_batch_labels)\n",
        "            out_batch_image = np.array(out_batch_image)\n",
        "        return out_batch_image, out_batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b0Hku8a98_fo"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "설계한 모델을 표로 작성합니다. \n",
        "\n",
        "- 목표 Receptive Field : ? <br>\n",
        "- Convolution Phase 후  출력 크기  :  ? <br>\n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| ? |?| ?|? |?  | ? |?| ?|\n",
        "\n",
        "\n",
        "- 모델 설계가 끝나면 간단한 그림을 작성해 아래에 붙여주세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDhKG-rSUQ_P",
        "colab_type": "text"
      },
      "source": [
        "예시1) \n",
        "\n",
        "\n",
        "- 목표 Receptive Field : 28 <br>\n",
        "- Convolution Phase 후  출력 크기  :  4 <br>\n",
        "- Regularization  : L2 \n",
        "- Batch size : 120\n",
        "- Learning rate : 0.0001 \n",
        "- Data normalization : min max normalization \n",
        "- Standardization : None \n",
        "\n",
        "\n",
        "| 층  | 종류|필터 갯수  | 필터 크기 | 스트라이드 | 패딩   | Dropout | output size |\n",
        "|--- |--- |----|----|----|----|----| ---| \n",
        "| c1 |conv| 64| 3x3| 1  | SAME | None| 32x32 |\n",
        "| s2 |max-pooling| None| 3x3| 2  | SAME | None|16x16 | \n",
        "| c3 |conv| 128| 3x3| 2  | SAME |NOne |16x16 | \n",
        "| s4 |max-pooling| None| 3x3| 2  | SAME | None|8 x8 | \n",
        "| c5 |conv| 128| 3x3| 2  | SAME | None |8 x8 | \n",
        "| s6 |conv| 256| 3x3| 2  | SAME | None |4 x 4 | \n",
        "| c7 |conv| 256| 1x1| 2  | SAME | None |4 x 4 | \n",
        "| f8 ||| | FC 256  | |  || \n",
        "| f8 ||| | Dropout 0.7 | |  || \n",
        "| f9 ||| | FC 256  | |  || \n",
        "| f9 ||| | Dropout 0.6 | |  || \n",
        "| f10||| | FC 10   | |  || \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Imgur](https://i.imgur.com/yqrIm5u.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2qtnd7UQ_Q",
        "colab_type": "text"
      },
      "source": [
        "# Convolution layer\n",
        "- convolution layer helper function 을 정의합니다.\n",
        "- 위 설계한 convolution layer 을 구현합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rzr0by6Kyv6J",
        "colab": {}
      },
      "source": [
        "# convolution helper function\n",
        "def conv(input_xs ,units, k, s, padding, activation, name):\n",
        "    layer = tf.layers.Conv2D(filters = units, kernel_size = k, strides = s,\n",
        "                             padding = padding, activation = activation, name = name )(input_xs)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmKroxF-CDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define input placeholder \n",
        "xs = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32, 3])\n",
        "ys = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "#ys_one_hot = tf.one_hot()\n",
        "lr = tf.placeholder(dtype = tf.float32, shape = ())\n",
        "phase_train = tf.placeholder(tf.bool, shape = (), name = 'phase_train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AJcmfqHJ9lLm",
        "outputId": "89ab2a69-dc36-4795-d1df-5a17d30903f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Model implementation \n",
        "# convolution Neural Network \n",
        "# 자신이 설계한 모형을 구현해주세요.\n",
        "with tf.variable_scope('VGG_block-1') :\n",
        "    layer = conv(xs, 10, (2,2), (1,1), 'SAME', tf.nn.relu, 'conv' )\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-2') :\n",
        "    layer = conv(pooling, 50, (2,2), (1,1), 'SAME', tf.nn.relu, 'conv')\n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "\n",
        "with tf.variable_scope('VGG_block-3') :\n",
        "    layer = conv(pooling, 100, (2,2), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 100, (2,2), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same')(layer)\n",
        "    \n",
        "with tf.variable_scope('VGG_block-4') :\n",
        "    layer = conv(pooling, 200, (2,2), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "    layer = conv(layer, 200, (2,2), (1,1), 'SAME', tf.nn.relu,'conv')    \n",
        "    pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "#with tf.variable_scope('VGG_block-5') :\n",
        "#    layer = conv(pooling, 400, (3,3), (1,1), 'SAME', tf.nn.relu,'conv')\n",
        "#    layer = conv(layer,400, (3,3), (1,1), 'SAME', tf.nn.relu, 'conv')    \n",
        "    #pooling = tf.layers.MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'same' )(layer)\n",
        "    \n",
        "top_conv = tf.identity(pooling, 'top_conv') # 마지막 layer 을 top conv 에 넣습니다.\n",
        "tf.shape(top_conv)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 14:57:29.827484 140415503660928 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Shape:0' shape=(4,) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRukJjRZUQ_V",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layer\n",
        "- 설계한 fully connected layer 을 구현합니다.\n",
        "- dropout 을 적용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woNx29qxUQ_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(flat_layer, units, initializer_, layer_name):\n",
        "    dense = tf.layers.Dense(units = units, activation = tf.nn.relu, kernel_initializer = initializer_, name = layer_name)(flat_layer)\n",
        "    return dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fL1M4gvUQ_X",
        "colab_type": "code",
        "outputId": "47b0c369-b861-4ff6-87f0-7b4aabf37f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# flat layer \n",
        "flatten_layer = tf.layers.flatten(top_conv)\n",
        "#print(\"tf.shape(flat_layer) :\", tf.shape(flatten_layer))\n",
        "\n",
        "# fully connected layer 1\n",
        "fc_initializer = tf.initializers.he_normal()\n",
        "fc_layer_1 = fc(flat_layer = flatten_layer, units = 2000, initializer_ = fc_initializer, layer_name = \"FC1\" )\n",
        "fc_layer_1 = tf.layers.dropout(fc_layer_1,rate=0.5, training=phase_train)\n",
        "\n",
        "# fix me # 자신이 설계한 fully connected layer 을 구현합니다.  \n",
        "\n",
        "fc_layer_2 = fc(flat_layer = fc_layer_1, units = 500, initializer_ = fc_initializer, layer_name = \"FC2\" )\n",
        "fc_layer_2 = tf.layers.dropout(fc_layer_2,rate=0.5, training=phase_train)\n",
        "\n",
        "fc_layer_3 = fc(flat_layer = fc_layer_2, units = 100, initializer_ = fc_initializer, layer_name = \"FC3\" )\n",
        "\n",
        "fc_layer_4 = fc(flat_layer = fc_layer_3, units = 10, initializer_ = fc_initializer, layer_name = \"FC4\" )\n",
        "\n",
        "logits= tf.identity(fc_layer_4, 'logits')\n",
        "\n",
        "ys = tf.cast(ys, tf.int32)\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(ys, logits)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 14:57:30.324651 140415503660928 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:1: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0617 14:57:30.968535 140415503660928 deprecation.py:323] From <ipython-input-9-cf7a4a918be0>:7: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0617 14:57:31.128563 140415503660928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-625ULtUQ_Z",
        "colab_type": "text"
      },
      "source": [
        "#  Loss function \n",
        "- loss function 을 정의합니다. L2 regularization 을 사용합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2DkXG-UQ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_reg = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n",
        "l2_beta = 5e-4\n",
        "\n",
        "#loss \n",
        "# L2 reularization \n",
        "loss = loss + (l2_beta * l2_reg)\n",
        "loss = tf.identity(loss, name = 'loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Oke8OgUQ_e",
        "colab_type": "text"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_TMXVtX1pXK",
        "colab": {}
      },
      "source": [
        "# metric\n",
        "pred = tf.nn.softmax(logits)\n",
        "one_hot_label = tf.one_hot(ys, 10)\n",
        "pred_arg = tf.argmax(pred, axis = 1)\n",
        "label_arg = tf.argmax(one_hot_label, axis = 1)\n",
        "eq = tf.cast(tf.equal(pred_arg, label_arg), dtype = tf.float32)\n",
        "acc = tf.reduce_mean(eq, axis =0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obfoWSrLUQ_h",
        "colab_type": "text"
      },
      "source": [
        "# Add tensor to Tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqUXVsUTUQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add accuracy to tensorboard nodes \n",
        "#fix me #\n",
        "acc_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "# add loss to tensorboard nodes \n",
        "#fix me #\n",
        "loss_summary = tf.summary.scalar(name='a', tensor=acc)\n",
        "\n",
        "#merge all tensorboard nodes \n",
        "#fix me #\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGeaVRCOUQ_j",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83C5s7mfUQ_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_op : adamoptimizer \n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6N1fBH2BBfX6"
      },
      "source": [
        "# Session open "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wiz-2rkLBdTw",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "#초기학습\n",
        "init_g = tf.global_variables_initializer() # : globalal initializer\n",
        "init_l = tf.local_variables_initializer() # : globalal initializer\n",
        "sess.run([init_l,init_g])\n",
        "\n",
        "# saver \n",
        "saver = tf.train.Saver()\n",
        "\n",
        "#Weight Transfer \n",
        "#saver.restore(sess, './ttee/model-48700')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFmQDdJDUQ_o",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard Filewriter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPH7CLomUQ_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorboard \n",
        "train_writer=tf.summary.FileWriter(logdir='./tensorboard/train')\n",
        "\n",
        "test_writer=tf.summary.FileWriter(logdir='./tensorboard/test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P88ef2rRUQ_q",
        "colab_type": "text"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yv99AR4A_Y7P",
        "scrolled": true,
        "outputId": "19876f09-22d7-458a-b3b0-99b5271e4d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6600
        }
      },
      "source": [
        "dataprovider = DataProvider(images=x_train, labels=y_train)\n",
        "#save_root_folder = #fix me # : models saved folder \n",
        "\n",
        "# hparam \n",
        "batch_size = 100\n",
        "min_loss = 1000000.0\n",
        "learning_rate = 0.0001\n",
        "\n",
        "np.random.seed(0)\n",
        "#local variable initialize\n",
        "for i in range(50000):\n",
        "    batch_xs, batch_ys = dataprovider.next_batch(batch_size)\n",
        "    # training \n",
        "    _= sess.run(train_op, feed_dict = {xs : batch_xs,\n",
        "                                        ys : batch_ys,\n",
        "                                        lr : learning_rate,\n",
        "                                        phase_train : True})\n",
        "    \n",
        "    if i % 100 == 0 :\n",
        "        # Validate validation dataset \n",
        "        fetches=[loss, acc, merged]\n",
        "        val_loss, val_acc, val_merged = sess.run(fetches, feed_dict = {xs : x_test,\n",
        "                                                                      ys : y_test,\n",
        "                                                                      phase_train : False})\n",
        "\n",
        "        # Validate train dataset : extract randomly 10000 samples from train dataset \n",
        "        ran = [ x for x in range(0, 50000)]\n",
        "        nansu = np.random.choice(ran, size = 10000, replace=False)\n",
        "        train_xtest, train_ytest = x_train[nansu], y_train[nansu]\n",
        "        train_loss, train_acc, train_merged = sess.run([loss, acc, merged], feed_dict = { xs : train_xtest,\n",
        "                                                                                          ys : train_ytest,\n",
        "                                                                                        phase_train : False})\n",
        "       \n",
        "        print('step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'.\\\n",
        "        format(i, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "        # Save Model \n",
        "        if val_loss < min_loss : #fix me # : when val_loss < min_loss \n",
        "            min_loss = val_loss\n",
        "            save_path = './ttee/model'\n",
        "            saver.save(sess, save_path, global_step=i)\n",
        "            print('model save!')\n",
        "            \n",
        "        # Add values to tensorboard \n",
        "        train_writer.add_summary(train_merged, i)\n",
        "        test_writer.add_summary(val_merged, i)\n",
        "        train_writer.flush()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step : 0 train loss : 3.7347 acc : 0.0954 | Val loss : 3.7347 acc : 0.1001\n",
            "model save!\n",
            "step : 100 train loss : 3.3711 acc : 0.1384 | Val loss : 3.3714 acc : 0.1363\n",
            "model save!\n",
            "step : 200 train loss : 3.0053 acc : 0.2302 | Val loss : 2.9966 acc : 0.2374\n",
            "model save!\n",
            "step : 300 train loss : 2.7355 acc : 0.2771 | Val loss : 2.7258 acc : 0.2792\n",
            "model save!\n",
            "step : 400 train loss : 2.5760 acc : 0.3197 | Val loss : 2.5699 acc : 0.3329\n",
            "model save!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0617 14:57:53.246374 140415503660928 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step : 500 train loss : 2.4714 acc : 0.3313 | Val loss : 2.4585 acc : 0.3454\n",
            "model save!\n",
            "step : 600 train loss : 2.3049 acc : 0.3635 | Val loss : 2.2934 acc : 0.3725\n",
            "model save!\n",
            "step : 700 train loss : 2.2380 acc : 0.3891 | Val loss : 2.2204 acc : 0.3936\n",
            "model save!\n",
            "step : 800 train loss : 2.1359 acc : 0.4119 | Val loss : 2.1420 acc : 0.4075\n",
            "model save!\n",
            "step : 900 train loss : 2.1424 acc : 0.3985 | Val loss : 2.1410 acc : 0.3959\n",
            "model save!\n",
            "step : 1000 train loss : 2.0474 acc : 0.4270 | Val loss : 2.0548 acc : 0.4269\n",
            "model save!\n",
            "step : 1100 train loss : 2.0178 acc : 0.4283 | Val loss : 2.0017 acc : 0.4438\n",
            "model save!\n",
            "step : 1200 train loss : 1.9504 acc : 0.4472 | Val loss : 1.9591 acc : 0.4527\n",
            "model save!\n",
            "step : 1300 train loss : 1.9196 acc : 0.4565 | Val loss : 1.9163 acc : 0.4573\n",
            "model save!\n",
            "step : 1400 train loss : 1.8920 acc : 0.4676 | Val loss : 1.8901 acc : 0.4691\n",
            "model save!\n",
            "step : 1500 train loss : 1.8534 acc : 0.4676 | Val loss : 1.8594 acc : 0.4746\n",
            "model save!\n",
            "step : 1600 train loss : 1.8182 acc : 0.4900 | Val loss : 1.8325 acc : 0.4831\n",
            "model save!\n",
            "step : 1700 train loss : 1.7935 acc : 0.4940 | Val loss : 1.8085 acc : 0.4864\n",
            "model save!\n",
            "step : 1800 train loss : 1.7736 acc : 0.4934 | Val loss : 1.7856 acc : 0.4947\n",
            "model save!\n",
            "step : 1900 train loss : 1.7868 acc : 0.4865 | Val loss : 1.7799 acc : 0.4879\n",
            "model save!\n",
            "step : 2000 train loss : 1.7320 acc : 0.5015 | Val loss : 1.7616 acc : 0.4967\n",
            "model save!\n",
            "step : 2100 train loss : 1.8099 acc : 0.4789 | Val loss : 1.8251 acc : 0.4741\n",
            "step : 2200 train loss : 1.7144 acc : 0.5133 | Val loss : 1.7326 acc : 0.5030\n",
            "model save!\n",
            "step : 2300 train loss : 1.6759 acc : 0.5228 | Val loss : 1.6991 acc : 0.5180\n",
            "model save!\n",
            "step : 2400 train loss : 1.6631 acc : 0.5182 | Val loss : 1.6868 acc : 0.5158\n",
            "model save!\n",
            "step : 2500 train loss : 1.6178 acc : 0.5453 | Val loss : 1.6552 acc : 0.5297\n",
            "model save!\n",
            "step : 2600 train loss : 1.6613 acc : 0.5222 | Val loss : 1.6652 acc : 0.5164\n",
            "step : 2700 train loss : 1.6339 acc : 0.5259 | Val loss : 1.6448 acc : 0.5301\n",
            "model save!\n",
            "step : 2800 train loss : 1.5817 acc : 0.5489 | Val loss : 1.6189 acc : 0.5374\n",
            "model save!\n",
            "step : 2900 train loss : 1.5892 acc : 0.5448 | Val loss : 1.6103 acc : 0.5365\n",
            "model save!\n",
            "step : 3000 train loss : 1.5999 acc : 0.5414 | Val loss : 1.6239 acc : 0.5260\n",
            "step : 3100 train loss : 1.5723 acc : 0.5456 | Val loss : 1.5863 acc : 0.5443\n",
            "model save!\n",
            "step : 3200 train loss : 1.5633 acc : 0.5486 | Val loss : 1.5840 acc : 0.5423\n",
            "model save!\n",
            "step : 3300 train loss : 1.5586 acc : 0.5550 | Val loss : 1.5908 acc : 0.5397\n",
            "step : 3400 train loss : 1.5315 acc : 0.5580 | Val loss : 1.5576 acc : 0.5529\n",
            "model save!\n",
            "step : 3500 train loss : 1.5489 acc : 0.5559 | Val loss : 1.5694 acc : 0.5456\n",
            "step : 3600 train loss : 1.5059 acc : 0.5638 | Val loss : 1.5326 acc : 0.5575\n",
            "model save!\n",
            "step : 3700 train loss : 1.5025 acc : 0.5701 | Val loss : 1.5343 acc : 0.5571\n",
            "step : 3800 train loss : 1.5120 acc : 0.5626 | Val loss : 1.5428 acc : 0.5529\n",
            "step : 3900 train loss : 1.4392 acc : 0.5888 | Val loss : 1.4822 acc : 0.5776\n",
            "model save!\n",
            "step : 4000 train loss : 1.4516 acc : 0.5774 | Val loss : 1.5085 acc : 0.5596\n",
            "step : 4100 train loss : 1.4603 acc : 0.5789 | Val loss : 1.5036 acc : 0.5640\n",
            "step : 4200 train loss : 1.4289 acc : 0.5954 | Val loss : 1.4748 acc : 0.5764\n",
            "model save!\n",
            "step : 4300 train loss : 1.4556 acc : 0.5834 | Val loss : 1.4799 acc : 0.5766\n",
            "step : 4400 train loss : 1.4313 acc : 0.5883 | Val loss : 1.4746 acc : 0.5704\n",
            "model save!\n",
            "step : 4500 train loss : 1.3967 acc : 0.5970 | Val loss : 1.4469 acc : 0.5834\n",
            "model save!\n",
            "step : 4600 train loss : 1.3842 acc : 0.6018 | Val loss : 1.4342 acc : 0.5891\n",
            "model save!\n",
            "step : 4700 train loss : 1.3860 acc : 0.6038 | Val loss : 1.4234 acc : 0.5935\n",
            "model save!\n",
            "step : 4800 train loss : 1.3650 acc : 0.6122 | Val loss : 1.4101 acc : 0.5954\n",
            "model save!\n",
            "step : 4900 train loss : 1.3676 acc : 0.6050 | Val loss : 1.4204 acc : 0.5949\n",
            "step : 5000 train loss : 1.3568 acc : 0.6116 | Val loss : 1.4076 acc : 0.5924\n",
            "model save!\n",
            "step : 5100 train loss : 1.3366 acc : 0.6200 | Val loss : 1.3984 acc : 0.5982\n",
            "model save!\n",
            "step : 5200 train loss : 1.3654 acc : 0.6082 | Val loss : 1.4010 acc : 0.5965\n",
            "step : 5300 train loss : 1.3147 acc : 0.6272 | Val loss : 1.3865 acc : 0.6001\n",
            "model save!\n",
            "step : 5400 train loss : 1.3253 acc : 0.6222 | Val loss : 1.3768 acc : 0.6031\n",
            "model save!\n",
            "step : 5500 train loss : 1.2982 acc : 0.6293 | Val loss : 1.3633 acc : 0.6113\n",
            "model save!\n",
            "step : 5600 train loss : 1.3138 acc : 0.6283 | Val loss : 1.3702 acc : 0.6070\n",
            "step : 5700 train loss : 1.2892 acc : 0.6289 | Val loss : 1.3574 acc : 0.6134\n",
            "model save!\n",
            "step : 5800 train loss : 1.3048 acc : 0.6229 | Val loss : 1.3552 acc : 0.6084\n",
            "model save!\n",
            "step : 5900 train loss : 1.2869 acc : 0.6324 | Val loss : 1.3479 acc : 0.6160\n",
            "model save!\n",
            "step : 6000 train loss : 1.2769 acc : 0.6352 | Val loss : 1.3397 acc : 0.6118\n",
            "model save!\n",
            "step : 6100 train loss : 1.2850 acc : 0.6254 | Val loss : 1.3486 acc : 0.6068\n",
            "step : 6200 train loss : 1.2502 acc : 0.6474 | Val loss : 1.3404 acc : 0.6145\n",
            "step : 6300 train loss : 1.2616 acc : 0.6352 | Val loss : 1.3148 acc : 0.6189\n",
            "model save!\n",
            "step : 6400 train loss : 1.2892 acc : 0.6278 | Val loss : 1.3479 acc : 0.6102\n",
            "step : 6500 train loss : 1.2361 acc : 0.6440 | Val loss : 1.3029 acc : 0.6277\n",
            "model save!\n",
            "step : 6600 train loss : 1.2314 acc : 0.6509 | Val loss : 1.3005 acc : 0.6285\n",
            "model save!\n",
            "step : 6700 train loss : 1.2443 acc : 0.6508 | Val loss : 1.2969 acc : 0.6274\n",
            "model save!\n",
            "step : 6800 train loss : 1.2192 acc : 0.6570 | Val loss : 1.2951 acc : 0.6277\n",
            "model save!\n",
            "step : 6900 train loss : 1.2118 acc : 0.6562 | Val loss : 1.2911 acc : 0.6308\n",
            "model save!\n",
            "step : 7000 train loss : 1.2088 acc : 0.6598 | Val loss : 1.2819 acc : 0.6352\n",
            "model save!\n",
            "step : 7100 train loss : 1.1981 acc : 0.6623 | Val loss : 1.2821 acc : 0.6280\n",
            "step : 7200 train loss : 1.1883 acc : 0.6663 | Val loss : 1.2869 acc : 0.6275\n",
            "step : 7300 train loss : 1.2505 acc : 0.6391 | Val loss : 1.3498 acc : 0.6095\n",
            "step : 7400 train loss : 1.2022 acc : 0.6553 | Val loss : 1.2806 acc : 0.6317\n",
            "model save!\n",
            "step : 7500 train loss : 1.1914 acc : 0.6556 | Val loss : 1.2578 acc : 0.6428\n",
            "model save!\n",
            "step : 7600 train loss : 1.2282 acc : 0.6453 | Val loss : 1.2995 acc : 0.6270\n",
            "step : 7700 train loss : 1.1487 acc : 0.6794 | Val loss : 1.2532 acc : 0.6361\n",
            "model save!\n",
            "step : 7800 train loss : 1.1560 acc : 0.6770 | Val loss : 1.2480 acc : 0.6432\n",
            "model save!\n",
            "step : 7900 train loss : 1.1450 acc : 0.6716 | Val loss : 1.2440 acc : 0.6440\n",
            "model save!\n",
            "step : 8000 train loss : 1.1518 acc : 0.6716 | Val loss : 1.2429 acc : 0.6397\n",
            "model save!\n",
            "step : 8100 train loss : 1.1588 acc : 0.6697 | Val loss : 1.2566 acc : 0.6439\n",
            "step : 8200 train loss : 1.1149 acc : 0.6855 | Val loss : 1.2284 acc : 0.6498\n",
            "model save!\n",
            "step : 8300 train loss : 1.1068 acc : 0.6950 | Val loss : 1.2129 acc : 0.6502\n",
            "model save!\n",
            "step : 8400 train loss : 1.1151 acc : 0.6871 | Val loss : 1.2195 acc : 0.6500\n",
            "step : 8500 train loss : 1.1040 acc : 0.6876 | Val loss : 1.2101 acc : 0.6553\n",
            "model save!\n",
            "step : 8600 train loss : 1.0939 acc : 0.6954 | Val loss : 1.2095 acc : 0.6543\n",
            "model save!\n",
            "step : 8700 train loss : 1.1277 acc : 0.6833 | Val loss : 1.2156 acc : 0.6513\n",
            "step : 8800 train loss : 1.0715 acc : 0.6997 | Val loss : 1.1988 acc : 0.6566\n",
            "model save!\n",
            "step : 8900 train loss : 1.0781 acc : 0.7012 | Val loss : 1.1985 acc : 0.6572\n",
            "model save!\n",
            "step : 9000 train loss : 1.0893 acc : 0.6930 | Val loss : 1.2019 acc : 0.6596\n",
            "step : 9100 train loss : 1.1162 acc : 0.6930 | Val loss : 1.2241 acc : 0.6475\n",
            "step : 9200 train loss : 1.0870 acc : 0.6986 | Val loss : 1.1997 acc : 0.6543\n",
            "step : 9300 train loss : 1.0898 acc : 0.6967 | Val loss : 1.2167 acc : 0.6477\n",
            "step : 9400 train loss : 1.0633 acc : 0.7023 | Val loss : 1.1900 acc : 0.6568\n",
            "model save!\n",
            "step : 9500 train loss : 1.0829 acc : 0.6949 | Val loss : 1.1960 acc : 0.6591\n",
            "step : 9600 train loss : 1.0557 acc : 0.7075 | Val loss : 1.1796 acc : 0.6611\n",
            "model save!\n",
            "step : 9700 train loss : 1.0697 acc : 0.6977 | Val loss : 1.1950 acc : 0.6563\n",
            "step : 9800 train loss : 1.0383 acc : 0.7136 | Val loss : 1.1713 acc : 0.6663\n",
            "model save!\n",
            "step : 9900 train loss : 1.0565 acc : 0.7003 | Val loss : 1.1849 acc : 0.6568\n",
            "step : 10000 train loss : 1.0635 acc : 0.7057 | Val loss : 1.1946 acc : 0.6564\n",
            "step : 10100 train loss : 1.0709 acc : 0.7024 | Val loss : 1.2060 acc : 0.6556\n",
            "step : 10200 train loss : 1.0416 acc : 0.7077 | Val loss : 1.1801 acc : 0.6626\n",
            "step : 10300 train loss : 1.0225 acc : 0.7141 | Val loss : 1.1699 acc : 0.6681\n",
            "model save!\n",
            "step : 10400 train loss : 1.0220 acc : 0.7200 | Val loss : 1.1769 acc : 0.6592\n",
            "step : 10500 train loss : 1.0669 acc : 0.6928 | Val loss : 1.2099 acc : 0.6520\n",
            "step : 10600 train loss : 1.0205 acc : 0.7189 | Val loss : 1.1629 acc : 0.6653\n",
            "model save!\n",
            "step : 10700 train loss : 0.9919 acc : 0.7238 | Val loss : 1.1568 acc : 0.6676\n",
            "model save!\n",
            "step : 10800 train loss : 0.9982 acc : 0.7268 | Val loss : 1.1538 acc : 0.6683\n",
            "model save!\n",
            "step : 10900 train loss : 1.0071 acc : 0.7209 | Val loss : 1.1625 acc : 0.6648\n",
            "step : 11000 train loss : 0.9897 acc : 0.7313 | Val loss : 1.1459 acc : 0.6740\n",
            "model save!\n",
            "step : 11100 train loss : 0.9805 acc : 0.7292 | Val loss : 1.1519 acc : 0.6734\n",
            "step : 11200 train loss : 0.9816 acc : 0.7310 | Val loss : 1.1370 acc : 0.6779\n",
            "model save!\n",
            "step : 11300 train loss : 1.0168 acc : 0.7182 | Val loss : 1.1720 acc : 0.6621\n",
            "step : 11400 train loss : 0.9921 acc : 0.7234 | Val loss : 1.1491 acc : 0.6732\n",
            "step : 11500 train loss : 0.9807 acc : 0.7330 | Val loss : 1.1362 acc : 0.6742\n",
            "model save!\n",
            "step : 11600 train loss : 0.9601 acc : 0.7389 | Val loss : 1.1196 acc : 0.6822\n",
            "model save!\n",
            "step : 11700 train loss : 0.9669 acc : 0.7358 | Val loss : 1.1378 acc : 0.6745\n",
            "step : 11800 train loss : 0.9990 acc : 0.7200 | Val loss : 1.1599 acc : 0.6692\n",
            "step : 11900 train loss : 0.9545 acc : 0.7422 | Val loss : 1.1395 acc : 0.6793\n",
            "step : 12000 train loss : 0.9530 acc : 0.7450 | Val loss : 1.1188 acc : 0.6811\n",
            "model save!\n",
            "step : 12100 train loss : 0.9697 acc : 0.7325 | Val loss : 1.1392 acc : 0.6830\n",
            "step : 12200 train loss : 0.9507 acc : 0.7456 | Val loss : 1.1186 acc : 0.6867\n",
            "model save!\n",
            "step : 12300 train loss : 0.9573 acc : 0.7434 | Val loss : 1.1528 acc : 0.6712\n",
            "step : 12400 train loss : 0.9459 acc : 0.7436 | Val loss : 1.1339 acc : 0.6754\n",
            "step : 12500 train loss : 0.9378 acc : 0.7470 | Val loss : 1.1279 acc : 0.6763\n",
            "step : 12600 train loss : 0.9508 acc : 0.7359 | Val loss : 1.1584 acc : 0.6676\n",
            "step : 12700 train loss : 0.9161 acc : 0.7558 | Val loss : 1.1087 acc : 0.6837\n",
            "model save!\n",
            "step : 12800 train loss : 0.9359 acc : 0.7477 | Val loss : 1.1414 acc : 0.6748\n",
            "step : 12900 train loss : 0.9318 acc : 0.7454 | Val loss : 1.1183 acc : 0.6849\n",
            "step : 13000 train loss : 0.9327 acc : 0.7451 | Val loss : 1.1349 acc : 0.6820\n",
            "step : 13100 train loss : 0.9021 acc : 0.7618 | Val loss : 1.1152 acc : 0.6837\n",
            "step : 13200 train loss : 0.9024 acc : 0.7628 | Val loss : 1.1274 acc : 0.6811\n",
            "step : 13300 train loss : 0.9238 acc : 0.7454 | Val loss : 1.1336 acc : 0.6780\n",
            "step : 13400 train loss : 0.9044 acc : 0.7585 | Val loss : 1.1174 acc : 0.6823\n",
            "step : 13500 train loss : 0.9308 acc : 0.7443 | Val loss : 1.1467 acc : 0.6754\n",
            "step : 13600 train loss : 0.8713 acc : 0.7714 | Val loss : 1.0936 acc : 0.6937\n",
            "model save!\n",
            "step : 13700 train loss : 0.9051 acc : 0.7572 | Val loss : 1.0960 acc : 0.6906\n",
            "step : 13800 train loss : 0.8526 acc : 0.7770 | Val loss : 1.1050 acc : 0.6850\n",
            "step : 13900 train loss : 0.8684 acc : 0.7788 | Val loss : 1.0994 acc : 0.6917\n",
            "step : 14000 train loss : 0.9055 acc : 0.7577 | Val loss : 1.1133 acc : 0.6852\n",
            "step : 14100 train loss : 0.8975 acc : 0.7582 | Val loss : 1.1223 acc : 0.6856\n",
            "step : 14200 train loss : 0.8745 acc : 0.7683 | Val loss : 1.0952 acc : 0.6948\n",
            "step : 14300 train loss : 0.8690 acc : 0.7663 | Val loss : 1.0989 acc : 0.6915\n",
            "step : 14400 train loss : 0.8584 acc : 0.7787 | Val loss : 1.0890 acc : 0.6947\n",
            "model save!\n",
            "step : 14500 train loss : 0.8840 acc : 0.7669 | Val loss : 1.1034 acc : 0.6870\n",
            "step : 14600 train loss : 0.8561 acc : 0.7763 | Val loss : 1.1018 acc : 0.6900\n",
            "step : 14700 train loss : 0.8267 acc : 0.7842 | Val loss : 1.0916 acc : 0.6970\n",
            "step : 14800 train loss : 0.8380 acc : 0.7853 | Val loss : 1.1026 acc : 0.6904\n",
            "step : 14900 train loss : 0.8327 acc : 0.7827 | Val loss : 1.0912 acc : 0.6958\n",
            "step : 15000 train loss : 0.8705 acc : 0.7713 | Val loss : 1.1006 acc : 0.6923\n",
            "step : 15100 train loss : 0.8337 acc : 0.7855 | Val loss : 1.1078 acc : 0.6931\n",
            "step : 15200 train loss : 0.8216 acc : 0.7871 | Val loss : 1.0999 acc : 0.6888\n",
            "step : 15300 train loss : 0.8318 acc : 0.7804 | Val loss : 1.0971 acc : 0.6920\n",
            "step : 15400 train loss : 0.8145 acc : 0.7899 | Val loss : 1.0860 acc : 0.6990\n",
            "model save!\n",
            "step : 15500 train loss : 0.8047 acc : 0.7967 | Val loss : 1.0800 acc : 0.6999\n",
            "model save!\n",
            "step : 15600 train loss : 0.8560 acc : 0.7680 | Val loss : 1.1242 acc : 0.6899\n",
            "step : 15700 train loss : 0.8003 acc : 0.8039 | Val loss : 1.0819 acc : 0.7004\n",
            "step : 15800 train loss : 0.7948 acc : 0.7962 | Val loss : 1.0741 acc : 0.7043\n",
            "model save!\n",
            "step : 15900 train loss : 0.8044 acc : 0.8003 | Val loss : 1.0780 acc : 0.6992\n",
            "step : 16000 train loss : 0.8271 acc : 0.7851 | Val loss : 1.0882 acc : 0.7025\n",
            "step : 16100 train loss : 0.8411 acc : 0.7779 | Val loss : 1.1271 acc : 0.6890\n",
            "step : 16200 train loss : 0.8066 acc : 0.7953 | Val loss : 1.0970 acc : 0.6990\n",
            "step : 16300 train loss : 0.7856 acc : 0.8050 | Val loss : 1.0830 acc : 0.7026\n",
            "step : 16400 train loss : 0.7985 acc : 0.8018 | Val loss : 1.0775 acc : 0.7005\n",
            "step : 16500 train loss : 0.7933 acc : 0.8012 | Val loss : 1.0901 acc : 0.6981\n",
            "step : 16600 train loss : 0.7606 acc : 0.8168 | Val loss : 1.0734 acc : 0.7027\n",
            "model save!\n",
            "step : 16700 train loss : 0.7906 acc : 0.8038 | Val loss : 1.0938 acc : 0.6977\n",
            "step : 16800 train loss : 0.7771 acc : 0.8101 | Val loss : 1.0758 acc : 0.7028\n",
            "step : 16900 train loss : 0.7567 acc : 0.8161 | Val loss : 1.0779 acc : 0.7033\n",
            "step : 17000 train loss : 0.7750 acc : 0.8068 | Val loss : 1.0765 acc : 0.7023\n",
            "step : 17100 train loss : 0.8426 acc : 0.7767 | Val loss : 1.1597 acc : 0.6804\n",
            "step : 17200 train loss : 0.7663 acc : 0.8099 | Val loss : 1.0810 acc : 0.7042\n",
            "step : 17300 train loss : 0.7579 acc : 0.8166 | Val loss : 1.0752 acc : 0.7060\n",
            "step : 17400 train loss : 0.7857 acc : 0.8047 | Val loss : 1.0918 acc : 0.7060\n",
            "step : 17500 train loss : 0.7525 acc : 0.8197 | Val loss : 1.0802 acc : 0.7039\n",
            "step : 17600 train loss : 0.7518 acc : 0.8188 | Val loss : 1.0807 acc : 0.7044\n",
            "step : 17700 train loss : 0.7407 acc : 0.8209 | Val loss : 1.0895 acc : 0.7044\n",
            "step : 17800 train loss : 0.7587 acc : 0.8088 | Val loss : 1.1152 acc : 0.7002\n",
            "step : 17900 train loss : 0.7499 acc : 0.8180 | Val loss : 1.0884 acc : 0.7063\n",
            "step : 18000 train loss : 0.7542 acc : 0.8117 | Val loss : 1.0926 acc : 0.7025\n",
            "step : 18100 train loss : 0.7261 acc : 0.8251 | Val loss : 1.0892 acc : 0.7039\n",
            "step : 18200 train loss : 0.7369 acc : 0.8192 | Val loss : 1.0918 acc : 0.7048\n",
            "step : 18300 train loss : 0.7669 acc : 0.8089 | Val loss : 1.1150 acc : 0.6929\n",
            "step : 18400 train loss : 0.7320 acc : 0.8246 | Val loss : 1.0967 acc : 0.7034\n",
            "step : 18500 train loss : 0.7176 acc : 0.8251 | Val loss : 1.0856 acc : 0.7051\n",
            "step : 18600 train loss : 0.7219 acc : 0.8242 | Val loss : 1.0943 acc : 0.7072\n",
            "step : 18700 train loss : 0.7021 acc : 0.8350 | Val loss : 1.0940 acc : 0.7055\n",
            "step : 18800 train loss : 0.7149 acc : 0.8330 | Val loss : 1.0766 acc : 0.7089\n",
            "step : 18900 train loss : 0.7266 acc : 0.8221 | Val loss : 1.1134 acc : 0.7029\n",
            "step : 19000 train loss : 0.7438 acc : 0.8149 | Val loss : 1.1089 acc : 0.7001\n",
            "step : 19100 train loss : 0.6861 acc : 0.8403 | Val loss : 1.0840 acc : 0.7116\n",
            "step : 19200 train loss : 0.6933 acc : 0.8338 | Val loss : 1.0857 acc : 0.7099\n",
            "step : 19300 train loss : 0.6930 acc : 0.8369 | Val loss : 1.0885 acc : 0.7058\n",
            "step : 19400 train loss : 0.7220 acc : 0.8210 | Val loss : 1.1129 acc : 0.7022\n",
            "step : 19500 train loss : 0.7145 acc : 0.8278 | Val loss : 1.1208 acc : 0.7016\n",
            "step : 19600 train loss : 0.7059 acc : 0.8316 | Val loss : 1.1082 acc : 0.7021\n",
            "step : 19700 train loss : 0.6903 acc : 0.8373 | Val loss : 1.1010 acc : 0.7104\n",
            "step : 19800 train loss : 0.7053 acc : 0.8312 | Val loss : 1.0988 acc : 0.7083\n",
            "step : 19900 train loss : 0.6957 acc : 0.8392 | Val loss : 1.1008 acc : 0.7089\n",
            "step : 20000 train loss : 0.7030 acc : 0.8335 | Val loss : 1.1109 acc : 0.7054\n",
            "step : 20100 train loss : 0.6690 acc : 0.8488 | Val loss : 1.0889 acc : 0.7091\n",
            "step : 20200 train loss : 0.6749 acc : 0.8448 | Val loss : 1.1146 acc : 0.7063\n",
            "step : 20300 train loss : 0.6847 acc : 0.8450 | Val loss : 1.1009 acc : 0.7060\n",
            "step : 20400 train loss : 0.6827 acc : 0.8448 | Val loss : 1.0969 acc : 0.7055\n",
            "step : 20500 train loss : 0.6556 acc : 0.8546 | Val loss : 1.0942 acc : 0.7127\n",
            "step : 20600 train loss : 0.6722 acc : 0.8442 | Val loss : 1.1184 acc : 0.7047\n",
            "step : 20700 train loss : 0.6351 acc : 0.8613 | Val loss : 1.0973 acc : 0.7115\n",
            "step : 20800 train loss : 0.6497 acc : 0.8542 | Val loss : 1.1034 acc : 0.7100\n",
            "step : 20900 train loss : 0.6445 acc : 0.8520 | Val loss : 1.0864 acc : 0.7138\n",
            "step : 21000 train loss : 0.6700 acc : 0.8448 | Val loss : 1.1122 acc : 0.7049\n",
            "step : 21100 train loss : 0.6479 acc : 0.8567 | Val loss : 1.1314 acc : 0.7055\n",
            "step : 21200 train loss : 0.6583 acc : 0.8480 | Val loss : 1.1356 acc : 0.7051\n",
            "step : 21300 train loss : 0.6188 acc : 0.8653 | Val loss : 1.1029 acc : 0.7143\n",
            "step : 21400 train loss : 0.6124 acc : 0.8728 | Val loss : 1.1062 acc : 0.7121\n",
            "step : 21500 train loss : 0.6383 acc : 0.8606 | Val loss : 1.1008 acc : 0.7103\n",
            "step : 21600 train loss : 0.6373 acc : 0.8578 | Val loss : 1.1237 acc : 0.7078\n",
            "step : 21700 train loss : 0.6241 acc : 0.8640 | Val loss : 1.1213 acc : 0.7141\n",
            "step : 21800 train loss : 0.6319 acc : 0.8612 | Val loss : 1.1462 acc : 0.7037\n",
            "step : 21900 train loss : 0.6400 acc : 0.8577 | Val loss : 1.1238 acc : 0.7047\n",
            "step : 22000 train loss : 0.6264 acc : 0.8650 | Val loss : 1.1369 acc : 0.7060\n",
            "step : 22100 train loss : 0.6154 acc : 0.8649 | Val loss : 1.1224 acc : 0.7122\n",
            "step : 22200 train loss : 0.6069 acc : 0.8746 | Val loss : 1.1248 acc : 0.7134\n",
            "step : 22300 train loss : 0.6116 acc : 0.8725 | Val loss : 1.1235 acc : 0.7125\n",
            "step : 22400 train loss : 0.6352 acc : 0.8552 | Val loss : 1.1589 acc : 0.6976\n",
            "step : 22500 train loss : 0.6329 acc : 0.8575 | Val loss : 1.1502 acc : 0.7041\n",
            "step : 22600 train loss : 0.6139 acc : 0.8685 | Val loss : 1.1446 acc : 0.7060\n",
            "step : 22700 train loss : 0.5896 acc : 0.8791 | Val loss : 1.1293 acc : 0.7104\n",
            "step : 22800 train loss : 0.6182 acc : 0.8693 | Val loss : 1.1611 acc : 0.7020\n",
            "step : 22900 train loss : 0.5726 acc : 0.8863 | Val loss : 1.1185 acc : 0.7178\n",
            "step : 23000 train loss : 0.6118 acc : 0.8681 | Val loss : 1.1669 acc : 0.7025\n",
            "step : 23100 train loss : 0.5964 acc : 0.8771 | Val loss : 1.1445 acc : 0.7098\n",
            "step : 23200 train loss : 0.5861 acc : 0.8819 | Val loss : 1.1465 acc : 0.7090\n",
            "step : 23300 train loss : 0.6038 acc : 0.8701 | Val loss : 1.1760 acc : 0.7029\n",
            "step : 23400 train loss : 0.6059 acc : 0.8704 | Val loss : 1.1751 acc : 0.7038\n",
            "step : 23500 train loss : 0.5802 acc : 0.8838 | Val loss : 1.1567 acc : 0.7152\n",
            "step : 23600 train loss : 0.6203 acc : 0.8647 | Val loss : 1.1872 acc : 0.6975\n",
            "step : 23700 train loss : 0.5716 acc : 0.8816 | Val loss : 1.1559 acc : 0.7116\n",
            "step : 23800 train loss : 0.5799 acc : 0.8816 | Val loss : 1.1754 acc : 0.7066\n",
            "step : 23900 train loss : 0.5506 acc : 0.8950 | Val loss : 1.1290 acc : 0.7197\n",
            "step : 24000 train loss : 0.5715 acc : 0.8806 | Val loss : 1.1608 acc : 0.7125\n",
            "step : 24100 train loss : 0.5469 acc : 0.8971 | Val loss : 1.1590 acc : 0.7125\n",
            "step : 24200 train loss : 0.5864 acc : 0.8776 | Val loss : 1.1929 acc : 0.7098\n",
            "step : 24300 train loss : 0.5543 acc : 0.8870 | Val loss : 1.1701 acc : 0.7102\n",
            "step : 24400 train loss : 0.5431 acc : 0.8986 | Val loss : 1.1366 acc : 0.7163\n",
            "step : 24500 train loss : 0.5427 acc : 0.8968 | Val loss : 1.1644 acc : 0.7090\n",
            "step : 24600 train loss : 0.5687 acc : 0.8847 | Val loss : 1.2050 acc : 0.7065\n",
            "step : 24700 train loss : 0.5858 acc : 0.8783 | Val loss : 1.1989 acc : 0.7065\n",
            "step : 24800 train loss : 0.5424 acc : 0.8975 | Val loss : 1.1498 acc : 0.7168\n",
            "step : 24900 train loss : 0.5862 acc : 0.8732 | Val loss : 1.1957 acc : 0.7005\n",
            "step : 25000 train loss : 0.5347 acc : 0.9007 | Val loss : 1.1737 acc : 0.7114\n",
            "step : 25100 train loss : 0.5110 acc : 0.9101 | Val loss : 1.1462 acc : 0.7178\n",
            "step : 25200 train loss : 0.5167 acc : 0.9067 | Val loss : 1.1977 acc : 0.7121\n",
            "step : 25300 train loss : 0.5176 acc : 0.9090 | Val loss : 1.1706 acc : 0.7156\n",
            "step : 25400 train loss : 0.5160 acc : 0.9086 | Val loss : 1.1722 acc : 0.7126\n",
            "step : 25500 train loss : 0.5402 acc : 0.8917 | Val loss : 1.2136 acc : 0.7071\n",
            "step : 25600 train loss : 0.5303 acc : 0.8997 | Val loss : 1.1997 acc : 0.7093\n",
            "step : 25700 train loss : 0.5224 acc : 0.9008 | Val loss : 1.2006 acc : 0.7167\n",
            "step : 25800 train loss : 0.5216 acc : 0.9041 | Val loss : 1.2093 acc : 0.7096\n",
            "step : 25900 train loss : 0.5174 acc : 0.9039 | Val loss : 1.2114 acc : 0.7127\n",
            "step : 26000 train loss : 0.4975 acc : 0.9187 | Val loss : 1.1863 acc : 0.7144\n",
            "step : 26100 train loss : 0.5300 acc : 0.8993 | Val loss : 1.2139 acc : 0.7099\n",
            "step : 26200 train loss : 0.5047 acc : 0.9127 | Val loss : 1.1905 acc : 0.7124\n",
            "step : 26300 train loss : 0.4894 acc : 0.9178 | Val loss : 1.1778 acc : 0.7187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ab0ec1e7af4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                         \u001b[0mys\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                         \u001b[0mlr\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                         phase_train : True})\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-lE48e_vHpa",
        "colab_type": "text"
      },
      "source": [
        "conv filter : 3x3, CP CP CCP CCP CC , acc  <br>\n",
        "\n",
        "| trade-off step | LR | conv1 | fc  | test acc | test loss | vali acc | vali loss |\n",
        "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
        "| 1700 | 0.0005 | conv | fc  | 0.09 | 2.3026 | 0.1 | 2.3026 |\n",
        "|1600 | 0.0001 | conv | fc  | 0.1 | 2.3026 | 0.1 | 2.3026 |\n",
        "| 6900 | 0.0001 | conv | fc  | 0.8038 | 0.7374 | 0.7214  | 1.0139  |\n",
        "| 9700 | 0.0001 | conv | fc  | 0.8210 | 0.7939 | 0.7157  |1.1145  |\n",
        "\n",
        "<br> conv filter : 2x2, CP CP CCP CCP , acc, receptive field : 28  <br>\n",
        "\n",
        "step : 15000 train loss : 0.7886 acc : 0.8169 | Val loss : 1.1091 acc : 0.7035\n"
      ]
    }
  ]
}