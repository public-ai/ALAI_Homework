{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week09_vgg_net_modelB.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Dm71BZOKS2dR","colab_type":"code","outputId":"66cc0d9c-7d88-402e-fe1b-e1b8b2925bfb","executionInfo":{"status":"ok","timestamp":1560934386565,"user_tz":-540,"elapsed":6925,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%matplotlib inline\n","!pip install tensorboardcolab\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import tensorboardcolab\n","from tensorflow.python.training import checkpoint_utils as cp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fMckkTBoTDxV","colab_type":"code","outputId":"39cb3557-7282-4885-ab59-a343754e075c","executionInfo":{"status":"ok","timestamp":1560934394740,"user_tz":-540,"elapsed":14993,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# load cifar10 dataset \n","from keras.datasets import cifar10\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-_00tFXVTIkn","colab_type":"code","outputId":"c8b2de55-19f3-4676-8d46-88e9e134a70d","executionInfo":{"status":"ok","timestamp":1560934396530,"user_tz":-540,"elapsed":16643,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["# reshape (None, 1) -> (None)\n","y_train, y_test = [np.reshape(y_train, [-1]), np.reshape(y_test, [-1])]\n","\n","# normalization \n","x_train, x_test = [(x_train - x_train.max()) / (x_train.max() - x_train.min()),\n","                   (x_test - x_test.max()) / (x_test.max() - x_test.min())]\n","\n","# N class\n","n_classes = 10\n","print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n","print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n","print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n","print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["image shape : (50000, 32, 32, 3), label shape : (50000,) \n","image shape : (10000, 32, 32, 3), label shape : (10000,) \n","train minimun : 0.0, train_maximum : 1.0 \n","tests minimun : 0.0, test_maximum : 1.0 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fDdQkH56bJMO","colab_type":"code","outputId":"2e2efbb6-f8fa-4533-e9af-c8c0bbeac72c","executionInfo":{"status":"ok","timestamp":1560934532006,"user_tz":-540,"elapsed":974,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["temp = x_train\n","ratio = int(len(x_train) * 0.7)\n","ratio_end = int(len(x_train) * 0.85)  ### TODO\n","\n","x_train = temp[0:ratio, :, :, :]\n","x_validation = temp[ratio:ratio_end , :, :, :]\n","\n","y_train_label = y_train[0:ratio]\n","y_validation_label = y_train[ratio:ratio_end ]\n","\n","print(y_train_label[:10]) # label이 one_hot encoding상태가 아니다.\n","\n","print(x_train.shape, y_train_label.shape)\n","print(x_validation.shape, y_validation_label.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[6 9 9 4 1 1 2 7 8 3]\n","(24500, 32, 32, 3) (24500,)\n","(5250, 32, 32, 3) (5250,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mnZl2rmnLdKw","colab_type":"text"},"source":["## Data Provider"]},{"cell_type":"code","metadata":{"id":"Tpg4Xv_5Lckl","colab_type":"code","colab":{}},"source":["class DataProvider(object):\n","    def __init__(self, x, y):\n","        self.epoch_count = 0\n","        \n","        self.data = x\n","        self.label = y\n","        \n","        npr.seed(42)\n","        \n","        self.indices = self.generate_indices()\n","        \n","    def generate_indices(self):\n","        indices = list(range(len(self.data)))\n","        npr.shuffle(indices)\n","        \n","        return indices\n","    \n","    def next_batch(self, batch_size):\n","        idx = batch_size\n","        if len(self.indices) < batch_size:\n","            print(\"all data consumed, epoch + 1\")\n","            self.epoch_count += 1\n","            self.indices = self.generate_indices()\n","    \n","        target_indices = self.indices[:batch_size]\n","        del self.indices[:batch_size]\n","        \n","        return self.data[target_indices] , self.label[target_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2bPbJdyLlOS","colab_type":"code","colab":{}},"source":["def cifar_generator(data, labels, batch_size=32):\n","    start_idx = 0\n","    num_step = len(data) // batch_size\n","    indexes = np.arange(0, len(data))\n","    while True:\n","        if start_idx >= num_step-1:\n","            np.random.shuffle(indexes)\n","            start_idx = 0\n","        else:\n","            start_idx += 1            \n","        batch_index = indexes[start_idx*batch_size:\n","                              (start_idx+1)*batch_size]\n","\n","        batch_data = data[batch_index]\n","        batch_label = labels[batch_index]\n","\n","        yield batch_data, batch_label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tppzG1VKq6kj","colab_type":"text"},"source":["## Model B : Build  --> Model A와 동일한 Structure이고, Optimizer만 Momentum으로 변경함"]},{"cell_type":"code","metadata":{"id":"dIDOTTlnUBBu","colab_type":"code","outputId":"7ec79ec1-d85d-4e97-cca8-0c5602d875cc","executionInfo":{"status":"ok","timestamp":1560934538093,"user_tz":-540,"elapsed":5617,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!mkdir ./model\n","!cp gdrive/My\\ Drive/vgg/* model/ # from, to 임"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","mkdir: cannot create directory ‘./model’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVMczcqNq6Np","colab_type":"code","colab":{}},"source":["graph = tf.Graph()\n","with graph.as_default() :\n","    xs = tf.placeholder(tf.float32, (None, 32, 32, 3), name='xs') \n","    ys = tf.placeholder(tf.int32, (None), name='ys')\n","    lr = tf.placeholder_with_default(0.001, (), name='lr')\n","    wd = tf.placeholder_with_default(0.9, (), name='wd')\n","    is_train = tf.placeholder_with_default(False, (), name='is_train')\n","    m = tf.placeholder_with_default(0.9, (), name='momentum')\n","    \n","    # pretrained weight and bias\n","    conv1_w1  = cp.load_variable('./model/vgg_net_model_a', 'conv1/kernel1')   \n","    conv1_b1  = cp.load_variable('./model/vgg_net_model_a', 'conv1/bias1')\n","    \n","    conv2_w1  = cp.load_variable('./model/vgg_net_model_a', 'conv2/kernel1')   \n","    conv2_b1  = cp.load_variable('./model/vgg_net_model_a', 'conv2/bias1')\n","\n","    conv3_w1  = cp.load_variable('./model/vgg_net_model_a', 'conv3/kernel1')    \n","    conv3_b1  = cp.load_variable('./model/vgg_net_model_a', 'conv3/bias1')\n","    conv3_w2  = cp.load_variable('./model/vgg_net_model_a', 'conv3/kernel2')   \n","    conv3_b2  = cp.load_variable('./model/vgg_net_model_a', 'conv3/bias2')\n","\n","    conv4_w1  = cp.load_variable('./model/vgg_net_model_a', 'conv4/kernel1')  \n","    conv4_b1  = cp.load_variable('./model/vgg_net_model_a', 'conv4/bias1')\n","    conv4_w2  = cp.load_variable('./model/vgg_net_model_a', 'conv4/kernel2')   \n","    conv4_b2  = cp.load_variable('./model/vgg_net_model_a', 'conv4/bias2')\n","\n","    f1_w1    = cp.load_variable('./model/vgg_net_model_a', 'fc1/kernel1')   \n","    f1_b1    = cp.load_variable('./model/vgg_net_model_a', 'fc1/bias1')\n","    f2_w1    = cp.load_variable('./model/vgg_net_model_a', 'fc2/kernel1')   \n","    f2_b1    = cp.load_variable('./model/vgg_net_model_a', 'fc2/bias1')\n","    f3_w1    = cp.load_variable('./model/vgg_net_model_a', 'fc3/kernel1')   \n","    f3_b1    = cp.load_variable('./model/vgg_net_model_a', 'fc3/bias1')\n","\n","    with tf.name_scope('conv1') :  \n","        kernel = tf.Variable(conv1_w1, name='kernel1')               \n","        bias   = tf.Variable(conv1_b1, name='bias1')        \n","        layer  = tf.nn.conv2d(xs, kernel, strides=[1,1,1,1], padding='SAME')\n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","        pool   = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)        \n","\n","    \n","    with tf.name_scope('conv2') :  \n","        kernel = tf.Variable(conv2_w1, name='kernel1')    \n","        bias   = tf.Variable(conv2_b1, name='bias1')            \n","        layer  = tf.nn.conv2d(pool, kernel, strides=[1,1,1,1], padding='SAME') \n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","        pool   = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)        \n","\n","    with tf.name_scope('conv3') :       \n","        kernel = tf.Variable(conv3_w1, name='kernel1')    \n","        bias   = tf.Variable(conv3_b1, name='bias1')            \n","        layer  = tf.nn.conv2d(pool, kernel, strides=[1,1,1,1], padding='SAME') \n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","                                 \n","        kernel = tf.Variable(conv3_w2, name='kernel2')    \n","        bias   = tf.Variable(conv3_b2, name='bias2')            \n","        layer  = tf.nn.conv2d(layer, kernel, strides=[1,1,1,1], padding='SAME') \n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","        pool   = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)      \n","\n","    with tf.name_scope('conv4') :       \n","        kernel = tf.Variable(conv4_w1, name='kernel1')    \n","        bias   = tf.Variable(conv4_b1, name='bias1')            \n","        layer  = tf.nn.conv2d(pool, kernel, strides=[1,1,1,1], padding='SAME') \n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","                                 \n","        kernel = tf.Variable(conv4_w2, name='kernel2')    \n","        bias   = tf.Variable(conv4_b2, name='bias2')            \n","        layer  = tf.nn.conv2d(layer, kernel, strides=[1,1,1,1], padding='SAME') \n","        layer  = layer + bias\n","        layer  = tf.nn.relu(layer)\n","        pool   = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)      \n","\n","                                 \n","    with tf.name_scope('fc1') : \n","        flatten = tf.layers.flatten(pool)\n","                                 \n","        kernel  = tf.Variable(f1_w1, name = \"kernel1\")\n","        bias    = tf.Variable(f1_b1, name = \"bias1\")\n","        z       = tf.matmul(flatten, kernel) + bias  # [?,512], [32768,1024].\n","        logits  = tf.nn.relu(z)\n","        dropout = tf.layers.Dropout(0.5)(logits, training = is_train)\n","\n","    with tf.name_scope('fc2') :                                  \n","        kernel  = tf.Variable(f2_w1, name = \"kernel1\")\n","        bias    = tf.Variable(f2_b1, name = \"bias1\")\n","        z       = tf.matmul(dropout, kernel) + bias\n","        logits  = tf.nn.relu(z)\n","        dropout = tf.layers.Dropout(0.5)(logits, training = is_train)\n","\n","    with tf.name_scope('fc3') :                                  \n","        kernel  = tf.Variable(f3_w1, name = \"kernel1\")\n","        bias    = tf.Variable(f3_b1, name = \"bias1\")\n","        z       = tf.matmul(dropout, kernel) + bias\n","        logits  = tf.nn.relu(z)\n","        dropout = tf.layers.Dropout(0.5)(logits, training = is_train)    \n","                                 \n","        y_pred  = tf.layers.Dense(10, activation=None, name='y_pred')(dropout)     \n","    \n","    with tf.name_scope('Loss') :\n","        sce_loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=ys, logits=y_pred))\n","        l2_loss  = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n","        loss     = sce_loss + wd * l2_loss\n","    loss = tf.identity(loss, name='loss')\n","        \n","    with tf.name_scope('metric') :\n","        rmse = tf.sqrt(loss)\n","    \n","    with tf.name_scope('accuracy') :\n","        pred     = tf.cast(tf.arg_max(y_pred, 1), tf.int32)\n","        correct  = tf.equal(pred, ys)\n","        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","        \n","        # add tensor to tensorboard\n","        acc_tb   = tf.summary.scalar(name='accuracy', tensor=accuracy)\n","\n","    with tf.name_scope('train') :\n","        global_step = tf.train.get_or_create_global_step()\n","        #train_op   = tf.train.AdamOptimizer(lr).minimize(loss, global_step = global_step)\n","        \n","        optimizer   = tf.train.MomentumOptimizer(lr, momentum = m, use_nesterov = False)\n","        train_op    = optimizer.minimize(loss, global_step = global_step)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTazVoc9zcd3","colab_type":"text"},"source":["## Model B : Train"]},{"cell_type":"code","metadata":{"id":"ZEShi1e3zefJ","colab_type":"code","outputId":"d3a27dec-9cf9-4172-8589-542dd3ed8ea9","executionInfo":{"status":"ok","timestamp":1560934540645,"user_tz":-540,"elapsed":7405,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["with graph.as_default() :\n","    \n","    log_dir = \"./log/vgg_net_model_b\"   \n","    lode_dir = \"./model/vgg_net_model_a\"\n","    save_dir = \"./model/vgg_net_model_b\"    # dir + file name.\n","    \n","    train_writer = tf.summary.FileWriter(logdir = log_dir)\n","    train_writer.add_graph(tf.get_default_graph())\n","    merged_all = tf.summary.merge_all()    \n","    \n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer()) \n","    \n","\n","    # Training\n","    batch_size = 1000\n","    n_epoch = 0 # 30\n","    n_step = int(len(x_train) // batch_size)  # //은 몫이다.\n","    learing_rate = 0.001\n","    weight_decay = 0.0005\n","    \n","    # instance 생성\n","    train_generator = cifar_generator(x_train, y_train_label, batch_size)\n","    \n","    train_loss = []\n","    valid_loss = []\n","    valid_acc = []\n","    cnt = 0\n","    minimum_loss = 1.1\n","    momentum = 0.9\n","\n","\n","    loss_, acc_= sess.run([rmse, accuracy], feed_dict = { xs : x_validation, \n","                                                          ys : y_validation_label, \n","                                                          wd : 0, \n","                                                          m : 0, \n","                                                          is_train : False })\n","    print(\"Loading결과 확인 -> loss = {:.4f}, acc = {:.2f}%\".format(loss_, acc_*100))\n","\n","        \n","    # for i in tqdm(range(n_epoch)) :\n","    #     for step in range(n_step) :\n","    #         batch_xs, batch_ys = next(train_generator)\n","    #         _, train_loss_, tbs_train_ = sess.run([train_op, rmse, merged_all], feed_dict = { xs: batch_xs, \n","    #                                                                                           ys: batch_ys, \n","    #                                                                                           lr: learing_rate,\n","    #                                                                                           wd : weight_decay,\n","    #                                                                                           m : momentum,\n","    #                                                                                           is_train : True})\n","    #         train_writer.add_summary(tbs_train_, global_step=cnt) # 흠 되야 하는데 안된다.\n","    #         cnt += 1\n","    #         train_loss.append(train_loss_)\n","    #         \n","    #         # check validation set\n","    #         if step % 100 == 0 :\n","    #             loss_, acc_ = sess.run([rmse, accuracy], feed_dict = { xs: x_validation, \n","    #                                                                    ys: y_validation_label,\n","    #                                                                    wd : weight_decay,\n","    #                                                                    m : momentum,\n","    #                                                                    is_train : False})\n","    #             valid_loss.append(loss_)\n","    #             valid_acc.append(acc_)\n","    #             \n","    #             # Save the model\n","    #             if loss_ < minimum_loss :\n","    #                 print(\"log current model!\")\n","    #                 minimum_loss = loss_\n","    #                 saver.save(sess, save_path = save_dir)\n","    #     print(\"loss = {:.4f}, acc = {:.2f}%\".format(loss_, acc_*100))\n","    # print(\"loss = {:.4f}, acc = {:.2f}%\".format(loss_, acc_*100))\n","    \n","    train_writer.flush() # file을 disk에 쓴다"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Loading결과 확인 -> loss = 1.5514, acc = 6.78%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"orlNn-vYc6-Z","colab_type":"code","colab":{}},"source":["plt.plot(np.arange(0, len(train_loss), 1), train_loss)\n","plt.show()\n","plt.plot(np.arange(0, len(valid_loss), 1), valid_loss)\n","plt.show() # train과 validation 모두 봐야 한다."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y81GK9E5vo_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!mkdir gdrive/My\\ Drive/vgg\n","!mv ./model/vgg* gdrive/My\\ Drive/vgg\n","!mv ./model/checkpoint gdrive/My\\ Drive/vgg"],"execution_count":0,"outputs":[]}]}