{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week09_vgg_net_reconstruct_ver1.1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f6lmGly3toQS","colab_type":"text"},"source":["### Version History\n","+ Ver 1.0 : pretrained된 model의 network와 동일하게 구성하고, Optimizer를 Momentum으로 구성함\n","+ Ver 1.1 : train된 weights를 loading하는 부분을 cp mothold를 이용하지 않고 get collection으로 refactoring함\n","+ Ver 1.2 : (예정) augmentation code 테스트예정 , class화, batch normalization test"]},{"cell_type":"code","metadata":{"id":"Dm71BZOKS2dR","colab_type":"code","outputId":"6ffa2be1-bda8-43bf-e0a2-fa7f02091ac7","executionInfo":{"status":"ok","timestamp":1561035420690,"user_tz":-540,"elapsed":6944,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%matplotlib inline\n","!pip install tensorboardcolab\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import tensorboardcolab\n","from tensorflow.python.training import checkpoint_utils as cp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p1Kfmsih8PXF","colab_type":"text"},"source":["## Load Image Data set"]},{"cell_type":"code","metadata":{"id":"y9f2oWW-usPc","colab_type":"code","colab":{}},"source":["def load_cifar10() :\n","    # load cifar10 dataset \n","    from keras.datasets import cifar10\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    \n","    # reshape (None, 1) -> (None)\n","    y_train, y_test = [np.reshape(y_train, [-1]), np.reshape(y_test, [-1])]\n","\n","    # normalization \n","    x_train, x_test = [(x_train-x_train.max()) / (x_train.max()-x_train.min()),\n","                         (x_test-x_test.max()) / (x_test.max()-x_test.min())]\n","\n","    temp = x_train\n","    ratio = int(len(x_train) * 0.7)\n","    ratio_end = int(len(x_train) * 1.0)  \n","    \n","    train_image = temp[0:ratio, :, :, :]\n","    valid_image = temp[ratio:ratio_end , :, :, :]\n","    \n","    train_label = y_train[0:ratio]\n","    valid_label = y_train[ratio:ratio_end ]\n","    \n","    return train_image, train_label, valid_image, valid_label, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnZl2rmnLdKw","colab_type":"text"},"source":["## Data Provider"]},{"cell_type":"code","metadata":{"id":"Tpg4Xv_5Lckl","colab_type":"code","colab":{}},"source":["class DataProvider(object):\n","    def __init__(self, x, y):\n","        self.epoch_count = 0\n","        \n","        self.data = x\n","        self.label = y\n","        \n","        npr.seed(42)\n","        \n","        self.indices = self.generate_indices()\n","        \n","    def generate_indices(self):\n","        indices = list(range(len(self.data)))\n","        npr.shuffle(indices)\n","        \n","        return indices\n","    \n","    def next_batch(self, batch_size):\n","        idx = batch_size\n","        if len(self.indices) < batch_size:\n","            print(\"all data consumed, epoch + 1\")\n","            self.epoch_count += 1\n","            self.indices = self.generate_indices()\n","    \n","        target_indices = self.indices[:batch_size]\n","        del self.indices[:batch_size]\n","        \n","        return self.data[target_indices] , self.label[target_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2bPbJdyLlOS","colab_type":"code","colab":{}},"source":["def cifar_generator(data, labels, batch_size=32):\n","    start_idx = 0\n","    num_step = len(data) // batch_size\n","    indexes = np.arange(0, len(data))\n","    while True:\n","        if start_idx >= num_step-1:\n","            np.random.shuffle(indexes)\n","            start_idx = 0\n","        else:\n","            start_idx += 1            \n","        batch_index = indexes[start_idx*batch_size: (start_idx+1)*batch_size]\n","\n","        batch_data = data[batch_index]\n","        batch_label = labels[batch_index]\n","\n","        yield batch_data, batch_label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aK1rRzBM8Vmb","colab_type":"text"},"source":["## Load pretrained variables"]},{"cell_type":"code","metadata":{"id":"2WbQW6B2vHl8","colab_type":"code","colab":{}},"source":["def get_trained_weights(graph, sess) :\n","    \n","    # loading pretrained files\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    \n","    !mkdir ./model\n","    !cp gdrive/My\\ Drive/vgg/* model/ # from, to 임\n","    \n","    with graph.as_default() : \n","        lode_dir = \"./model/vgg_net_model_a\"\n","        saver = tf.train.import_meta_graph(lode_dir + '.meta')\n","        saver.restore(sess, save_path = lode_dir)    \n","        # Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist.\n","        # The operation, 'save/Const', does not exist in the graph.\n","        \n","        reuse_vars = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n","        \n","        # trained variable name : values\n","        reuse_vars_dict = dict([(var.name.replace(':0',''), sess.run(var.name)) \n","                                for var in reuse_vars])\n","        return reuse_vars_dict       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRcbockb8a9X","colab_type":"text"},"source":["## model functions"]},{"cell_type":"code","metadata":{"id":"C-XRcminN3uJ","colab_type":"code","colab":{}},"source":["def conv2d(input, trained_dict, trainable, floor, model, name) :\n","    for i in range(floor) :              \n","        kernel_init = trained_dict[str(name+'/kernel'+str(i+1))] # conv1/kernel1\n","        bias_init   = trained_dict[str(name+'/bias'+str(i+1))]\n","        kernel      = tf.Variable(kernel_init, name='kernel'+str(i+1))               \n","        bias        = tf.Variable(bias_init, name='bias'+str(i+1))        \n","        layer       = tf.nn.conv2d(input, kernel, strides=[1,1,1,1], \n","                                   padding='SAME')\n","        layer       = layer + bias\n","        layer       = tf.nn.relu(layer)\n","        \n","        if model == \"BN\" :\n","            layer = tf.layers.BatchNormalization()(layer, training=is_train)\n","            \n","    return layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jFNUz-1TF_2","colab_type":"code","colab":{}},"source":["def max_pooling2d(input) :\n","    pool  = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(input) \n","    \n","    return pool"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-xjFVuPTm8v","colab_type":"code","colab":{}},"source":["def fc(input, trained_dict, is_train, model, name) :\n","    kernel_init = trained_dict[str(name+'/kernel1')]\n","    bias_init   = trained_dict[str(name+'/bias1')]\n","    kernel      = tf.Variable(kernel_init, name = \"kernel1\")\n","    bias        = tf.Variable(bias_init, name = \"bias1\")\n","    z           = tf.matmul(input, kernel) + bias \n","    logits      = tf.nn.relu(z)\n","    \n","    if model == \"BN\" :\n","        logits = tf.layers.BatchNormalization()(logits, training=is_train)\n","        \n","    dropout     = tf.layers.Dropout(0.5)(logits, training = is_train)\n","    \n","    return dropout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2eesMFQUeHv","colab_type":"code","colab":{}},"source":["def softmax_l2_with_loss(ys_true, ys_pred, weight_decay) :  \n","    sce_loss = tf.reduce_mean(\n","        tf.losses.sparse_softmax_cross_entropy(labels=ys_true, logits=ys_pred))\n","    l2_loss  = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n","    loss     = sce_loss + weight_decay * l2_loss\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7-g17VsVBU6","colab_type":"code","colab":{}},"source":["def accuracy(y_true, y_pred) :\n","    pred     = tf.cast(tf.arg_max(y_pred, 1), tf.int32)\n","    correct  = tf.equal(pred, y_true)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","    # add tensor to tensorboard\n","    acc_tb   = tf.summary.scalar(name='accuracy', tensor=accuracy)\n","    \n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOLMQsFi8dS7","colab_type":"text"},"source":["## main model"]},{"cell_type":"code","metadata":{"id":"5OKwIlxabiGO","colab_type":"code","colab":{}},"source":["graph = tf.Graph()\n","sess  = tf.Session()   \n","sess.run(tf.global_variables_initializer())\n","temp = tf.Variable(0)\n","saver = tf.train.Saver() "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbIszm3083OJ","colab_type":"text"},"source":["## reconstruct model"]},{"cell_type":"code","metadata":{"id":"SVMczcqNq6Np","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":593},"outputId":"30044681-dbd7-433c-d8f6-905cc6b83fa8","executionInfo":{"status":"error","timestamp":1561035425806,"user_tz":-540,"elapsed":11927,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}}},"source":["# class Conv_net() :\n","#     def __init__(self, graph, sess) :\n","#         # super().__init__()\n","#         self.graph = graph\n","#         self.sess = sess\n","#         self.sess.run(tf.global_variables_initializer())\n","#         print(\"initial is complete!\")\n","#         \n","#     def train(self, image, label, lr) :\n","with graph.as_default() :\n","    xs       = tf.placeholder(tf.float32, (None, 32, 32, 3), name='xs') \n","    ys       = tf.placeholder(tf.int32, (None), name='ys')\n","    lr       = tf.placeholder_with_default(0.001, (), name='lr')\n","    wd       = tf.placeholder_with_default(0.9, (), name='wd')\n","    is_train = tf.placeholder_with_default(False, (), name='is_train')\n","    m        = tf.placeholder_with_default(0.9, (), name='momentum')\n","    dr       = tf.placeholder_with_default(0.9, (), name='dropout_ratio')   \n","    model    = tf.placeholder_with_default(\"VGG\", (), name='model')  \n","    1\n","    trained_dict = get_trained_weights(graph, sess)\n","        \n","    with tf.name_scope('VGGBlock-1') :\n","        layer = conv2d(xs, trained_dict, is_train, 1, 'conv1')\n","        pool  = max_pooling2d(layer)   \n","        \n","    with tf.name_scope('VGGBlock-2') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, 'conv2')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-3') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, 'conv3')\n","        layer = conv2d(layer, trained_dict, is_train, 2, 'conv3')\n","        pool = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-4') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, 'conv4')\n","        layer = conv2d(layer, trained_dict, is_train, 2, 'conv4')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('fc1') : \n","        flatten = tf.layers.flatten(pool)\n","        layer = fc(flatten, trained_dict, is_train, 'fc1')\n","        \n","    with tf.name_scope('fc2') :     \n","        layer = fc(layer, trained_dict, is_train, 'fc2')\n","\n","    with tf.name_scope('fc3') :                                  \n","        layer = fc(layer, trained_dict, is_train, 'fc3')\n","    \n","    with tf.name_scope('output') : \n","        y_pred  = tf.layers.Dense(10, \n","                                  activation=None, \n","                                  name='y_pred')(layer)     \n","    \n","    with tf.name_scope('Loss') :\n","        loss = softmax_l2_with_loss(ys, y_pred, wd)\n","    loss = tf.identity(loss, name='loss')\n","        \n","    with tf.name_scope('metric') :\n","        rmse = tf.sqrt(loss)\n","    rmse = tf.identity(rmse, name='rmse')\n","    \n","    with tf.name_scope('accuracy') :\n","        acc = accuracy(ys, y_pred)\n","    acc = tf.identity(acc, name='acc')\n","\n","    with tf.name_scope('train') :\n","        # global_step = tf.train.get_or_create_global_step()\n","        optimizer   = tf.train.MomentumOptimizer(lr, \n","                                                 momentum = m, \n","                                                 use_nesterov = False)\n","        train_op    = optimizer.minimize(loss)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","mkdir: cannot create directory ‘./model’: File exists\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0620 12:57:05.188652 140064012367744 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1113\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3795\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3837\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3838\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3839\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"The name 'save_1/Const:0' refers to a Tensor which does not exist. The operation, 'save_1/Const', does not exist in the graph.\"","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-991ddc65bb31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder_with_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VGG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VGGBlock-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-339ade36ddaa>\u001b[0m in \u001b[0;36mget_trained_weights\u001b[0;34m(graph, sess)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlode_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./model/vgg_net_model_a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlode_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# The operation, 'save/Const', does not exist in the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1284\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1286\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1116\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: The name 'save_1/Const:0' refers to a Tensor which does not exist. The operation, 'save_1/Const', does not exist in the graph."]}]},{"cell_type":"code","metadata":{"id":"43KGZAr7ttRO","colab_type":"code","colab":{}},"source":["def model_b(config) :\n","    # load data    \n","    train_image, train_label, \\\n","    valid_image, valid_label, \\\n","    test_image, test_label = load_cifar10()\n","    \n","    # load hyper parameters\n","    load_model    = config['load_model']\n","    save_model    = config['save_model']\n","    learning_rate = config['learning_rate']  \n","    batch_size    = config['batch_size']\n","    n_epoch       = config['epoch']\n","    n_step        = int(len(train_image) // batch_size)\n","    weight_decay  = config['weight_decay']\n","    dropout_ratio = config['dropout_ratio']\n","    \n","    # save directory \n","    if load_model == None :\n","         lode_dir = None\n","    else :\n","         lode_dir = \"./model/vgg_net_model_\" + load_model  \n","    save_dir = \"./model/vgg_net_model_\" + save_model  \n","    log_dir = \"./log/vgg_net_model_\" + save_model\n","    \n","    # tensorboard\n","    tbc = tensorboardcolab.TensorBoardColab(graph_path = log_dir)\n","    train_writer = tf.summary.FileWriter(logdir = log_dir)\n","    train_writer.add_graph(tf.get_default_graph())\n","    merged_all = tf.summary.merge_all()    \n","    \n","    # graph = tf.Graph()\n","    with graph.as_default() :        \n","        \n","        # create Instance\n","        # model_b         = Conv_net(graph, sess)\n","        train_generator = cifar_generator(train_image, train_label, batch_size)\n","        \n","\n","        \n","        # loading pretrained data\n","        trained_dict = get_trained_weights(graph, sess)\n","    \n","        loss_, acc_ = sess.run([rmse, acc], \n","                                          feed_dict = { xs: valid_image, \n","                                                        ys: valid_label,\n","                                                        wd : weight_decay,\n","                                                        is_train : False})\n","        print(\"check loading pretrained data! \\\n","              valid loss = {:.4f}, valid acc = {:.2f}%\".format(loss_, acc_*100))\n","                        \n","        # train_loss = []\n","        # train_acc = []\n","        # valid_loss = []\n","        # valid_acc = []\n","        # cnt = 0\n","        # maximum_acc = 0.5\n","        # for i in tqdm(range(n_epoch)) :\n","        #     for step in range(n_step) :\n","        #         batch_xs, batch_ys = next(train_generator)\n","        #         _, train_loss_, train_acc_ = sess.run([train_op, rmse, acc], \n","        #                                       feed_dict = { xs: batch_xs, \n","        #                                                     ys: batch_ys, \n","        #                                                     lr: learing_rate,\n","        #                                                     wd : weight_decay,\n","        #                                                     dr : dropout_ratio, \n","        #                                                     model : model_type,\n","        #                                                     is_train : True})\n","        #         train_loss.append(train_loss_)\n","        #         train_acc.append(train_acc_)\n","        #     \n","        #         # check validation set\n","        #         if step % 100 == 0 :\n","        #             loss_, acc_ = sess.run([rmse, acc], \n","        #                                   feed_dict = { xs: valid_image, \n","        #                                                 ys: valid_label,\n","        #                                                 wd : weight_decay,\n","        #                                                 is_train : False})\n","        #             valid_loss.append(loss_)\n","        #             valid_acc.append(acc_)\n","        #         \n","        #             # Save the model\n","        #             if acc_ > maximum_acc :\n","        #                 print(\"log current model! valid loss = {:.4f}, \\\n","        #                        valid acc = {:.2f}%\".format(loss_, acc_*100))\n","        #                 maximum_acc = acc_\n","        #                 saver.save(sess, save_path = save_dir)\n","        #     print(\" valid loss = {:.4f}, valid acc = {:.2f}%\". \\\n","        #           format(loss_, acc_*100))\n","        # \n","        # train_writer.flush() # file을 disk에 쓴다"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByDrbiAY81P_","colab_type":"text"},"source":["## main function"]},{"cell_type":"code","metadata":{"id":"iaElNp0ifP1L","colab_type":"code","colab":{}},"source":["def main() :\n","    \n","    config of hyper parameters\n","    config = {\n","        \"model_type\"      : \"VGG\",  # VGG or BN\n","        \"load_model\"      : \"a\",\n","        \"save_model\"      : \"b\",\n","        \"learning_rate\"   : 0.001,\n","        \"batch_size\"      : 1000,\n","        \"epoch\"           : 100,\n","        \"weight_decay\"    : 0.0005,\n","        \"dropout_ratio\"   : 0.5\n","    }\n","    \n","    # call reconstruct model\n","    model_a(config)\n","    \n","    \n","    # # config of hyper parameters\n","    # config = {\n","    #     \"model_type\"      : \"BN\",  # VGG or BN\n","    #     \"load_model\"      : \"a\",\n","    #     \"save_model\"      : \"b\",\n","    #     \"learning_rate\"   : 0.001,\n","    #     \"batch_size\"      : 1000,\n","    #     \"epoch\"           : 100,\n","    #     \"weight_decay\"    : 0.0005,\n","    #     # Cifa10 Dataset은 overfitting이 심하기 때문에 dropout을 제거하는 대신 비율을 줄임\n","    #     \"dropout_ratio\"   : 0.4 \n","    # }\n","    # \n","    # # call reconstruct model\n","    # model_b(config)\n","    \n","    \n","if __name__ == '__main__':\n","    main()           "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvPOw15r8xwt","colab_type":"text"},"source":["## save model"]},{"cell_type":"code","metadata":{"id":"9y81GK9E5vo_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!mkdir gdrive/My\\ Drive/vgg\n","!mv ./model/vgg* gdrive/My\\ Drive/vgg\n","!mv ./model/checkpoint gdrive/My\\ Drive/vgg"],"execution_count":0,"outputs":[]}]}