{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week09_vgg_net_reconstruct_ver1.3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f6lmGly3toQS","colab_type":"text"},"source":["### Version History\n","+ Ver 1.0 : pretrained된 model의 network와 동일하게 구성하고, Optimizer를 Momentum으로 구성함\n","+ Ver 1.1 : train된 weights를 loading하는 부분을 cp mothold를 이용하지 않고 get collection으로 refactoring함\n","+ Ver 1.2 : augmentation code 추가, code 정리\n","+ Ver 1.3 : restore후에 weight, bias를 초기화 해주는 방법 변경(Low->High API 변경)"]},{"cell_type":"code","metadata":{"id":"Dm71BZOKS2dR","colab_type":"code","outputId":"46d7060e-d974-48f4-d293-88c17ae15fed","executionInfo":{"status":"ok","timestamp":1561108023929,"user_tz":-540,"elapsed":5268,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%matplotlib inline\n","!pip install tensorboardcolab\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import tensorboardcolab\n","from tensorflow.python.training import checkpoint_utils as cp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p1Kfmsih8PXF","colab_type":"text"},"source":["## Load Image Data set"]},{"cell_type":"code","metadata":{"id":"y9f2oWW-usPc","colab_type":"code","colab":{}},"source":["def load_cifar10() :\n","    # load cifar10 dataset \n","    from keras.datasets import cifar10\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    \n","    # reshape (None, 1) -> (None)\n","    y_train, y_test = [np.reshape(y_train, [-1]), np.reshape(y_test, [-1])]\n","\n","    # normalization \n","    x_train, x_test = [(x_train-x_train.max()) / (x_train.max()-x_train.min()),\n","                         (x_test-x_test.max()) / (x_test.max()-x_test.min())]\n","\n","    temp = x_train\n","    ratio = int(len(x_train) * 0.7)\n","    ratio_end = int(len(x_train) * 1.0)  \n","    \n","    train_image = temp[0:ratio, :, :, :]\n","    valid_image = temp[ratio:ratio_end , :, :, :]\n","    \n","    train_label = y_train[0:ratio]\n","    valid_label = y_train[ratio:ratio_end ]\n","    \n","    return train_image, train_label, valid_image, valid_label, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnZl2rmnLdKw","colab_type":"text"},"source":["## Data Provider"]},{"cell_type":"code","metadata":{"id":"Tpg4Xv_5Lckl","colab_type":"code","colab":{}},"source":["class DataProvider(object):\n","    def __init__(self, x, y):\n","        self.epoch_count = 0\n","        \n","        self.data = x\n","        self.label = y\n","        \n","        npr.seed(42)\n","        \n","        self.indices = self.generate_indices()\n","        \n","    def generate_indices(self):\n","        indices = list(range(len(self.data)))\n","        npr.shuffle(indices)\n","        \n","        return indices\n","    \n","    def next_batch(self, batch_size):\n","        idx = batch_size\n","        if len(self.indices) < batch_size:\n","            print(\"all data consumed, epoch + 1\")\n","            self.epoch_count += 1\n","            self.indices = self.generate_indices()\n","    \n","        target_indices = self.indices[:batch_size]\n","        del self.indices[:batch_size]\n","        \n","        return self.data[target_indices] , self.label[target_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2bPbJdyLlOS","colab_type":"code","colab":{}},"source":["def cifar_generator(data, labels, batch_size=32):\n","    start_idx = 0\n","    num_step = len(data) // batch_size\n","    indexes = np.arange(0, len(data))\n","    while True:\n","        if start_idx >= num_step-1:\n","            np.random.shuffle(indexes)\n","            start_idx = 0\n","        else:\n","            start_idx += 1            \n","        batch_index = indexes[start_idx*batch_size: (start_idx+1)*batch_size]\n","\n","        batch_data = data[batch_index]\n","        batch_label = labels[batch_index]\n","\n","        yield batch_data, batch_label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aK1rRzBM8Vmb","colab_type":"text"},"source":["## Load pretrained variables"]},{"cell_type":"code","metadata":{"id":"2WbQW6B2vHl8","colab_type":"code","colab":{}},"source":["def get_trained_weights() :\n","    \n","    # loading pretrained files\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    \n","    !mkdir ./model\n","    !cp gdrive/My\\ Drive/vgg/* model/ # from, to 임\n","    \n","    # temperary graph and session\n","    graph = tf.Graph()\n","    with graph.as_default() :\n","        sess = tf.Session(graph=graph)\n","        \n","        lode_dir = \"./model/vgg_net_model_a\"\n","        saver = tf.train.import_meta_graph(lode_dir + '.meta')\n","    \n","        saver.restore(sess, save_path = lode_dir)    \n","        \n","        reuse_vars = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n","        \n","        \n","        # trained variable name : values\n","        reuse_vars_dict = dict([(var.name.replace(':0',''), sess.run(var.name)) \n","                                for var in reuse_vars])\n","        print(\"get_trained_weights -> \", reuse_vars_dict.keys() )\n","        sess.close()\n","        \n","        return reuse_vars_dict       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRcbockb8a9X","colab_type":"text"},"source":["## model functions"]},{"cell_type":"code","metadata":{"id":"C-XRcminN3uJ","colab_type":"code","colab":{}},"source":["def conv2d(input, units, trained_dict, trainable, floor, model, name) :              \n","    kernel_init   = trained_dict[str(name+'_kernel'+str(floor))] \n","    bias_init     = trained_dict[str(name+'_bias'+str(floor))]\n","    initializer_k = tf.constant_initializer(kernel_init)        \n","    initializer_b = tf.constant_initializer(bias_init)   \n","\n","    if model == \"BN\" :\n","        use_bias = False\n","\n","    layer = tf.layers.Conv2D(filters = units, # number of kernels \n","                             kernel_size = [2,2], \n","                             strides = [1,1],\n","                             padding = 'SAME', \n","                             activation = tf.nn.relu, \n","                             kernel_initializer = initializer_k, \n","                             use_bias = True,\n","                             bias_initializer = initializer_b, \n","                             name = name )(input)\n","    \n","    if model == \"BN\" :\n","        layer = tf.layers.BatchNormalization()(layer, training=is_train)\n","        \n","    return layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jFNUz-1TF_2","colab_type":"code","colab":{}},"source":["def max_pooling2d(input) :\n","    pool  = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(input) \n","    \n","    return pool"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-xjFVuPTm8v","colab_type":"code","colab":{}},"source":["def fc(input, units, trained_dict, dr, is_train, model, name) :\n","    kernel_init = trained_dict[str(name+'_kernel')]\n","    bias_init   = trained_dict[str(name+'_bias')]    \n","    initializer_k = tf.constant_initializer(kernel_init)\n","    initializer_b = tf.constant_initializer(bias_init)\n","        \n","    dense = tf.layers.Dense(units = units, \n","                            activation = tf.nn.relu,\n","                            kernel_initializer = initializer_k, \n","                            use_bias = True,\n","                            bias_initializer = initializer_b, \n","                            name = 'fc')(input) # TODO\n","\n","    if model == \"BN\" :\n","        dense = tf.layers.BatchNormalization()(dense, training=is_train)\n","        \n","    dropout = tf.layers.Dropout(dr)(dense, training = is_train)\n","    \n","    # print(trained_dict['VGGBlock-1/conv1_bias1']) \n","    return dropout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2eesMFQUeHv","colab_type":"code","colab":{}},"source":["def softmax_l2_with_loss(ys_true, ys_pred, weight_decay) :  \n","    sce_loss = tf.reduce_mean(\n","        tf.losses.sparse_softmax_cross_entropy(labels=ys_true, logits=ys_pred))\n","    l2_loss  = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n","    loss     = sce_loss + weight_decay * l2_loss\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7-g17VsVBU6","colab_type":"code","colab":{}},"source":["def accuracy(y_true, y_pred) :\n","    pred     = tf.cast(tf.arg_max(y_pred, 1), tf.int32)\n","    correct  = tf.equal(pred, y_true)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","    # add tensor to tensorboard\n","    acc_tb   = tf.summary.scalar(name='accuracy', tensor=accuracy)\n","    \n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOLMQsFi8dS7","colab_type":"text"},"source":["## main model"]},{"cell_type":"code","metadata":{"id":"7iDmvPNkj85g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"outputId":"40325466-d4e3-4e74-8b62-2987d7a4938d","executionInfo":{"status":"ok","timestamp":1561108028627,"user_tz":-540,"elapsed":9825,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}}},"source":["graph = tf.Graph()\n","trained_dict = get_trained_weights()\n"," # print(trained_dict['VGGBlock-1/conv1_kernel1']) "],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","mkdir: cannot create directory ‘./model’: File exists\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0621 09:07:05.579914 139887982245760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["get_trained_weights ->  dict_keys(['VGGBlock-1/conv1_kernel1', 'VGGBlock-1/conv1_bias1', 'VGGBlock-2/conv2_kernel1', 'VGGBlock-2/conv2_bias1', 'VGGBlock-3/conv3_kernel1', 'VGGBlock-3/conv3_bias1', 'VGGBlock-3/conv3_kernel2', 'VGGBlock-3/conv3_bias2', 'VGGBlock-4/conv4_kernel1', 'VGGBlock-4/conv4_bias1', 'VGGBlock-4/conv4_kernel2', 'VGGBlock-4/conv4_bias2', 'fc/fc1_kernel', 'fc/fc1_bias', 'fc/fc2_kernel', 'fc/fc2_bias', 'fc/fc3_kernel', 'fc/fc3_bias', 'y_pred/kernel', 'y_pred/bias'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVMczcqNq6Np","colab_type":"code","outputId":"8e927bae-a05b-41cb-d593-6040f5eff273","executionInfo":{"status":"ok","timestamp":1561108029634,"user_tz":-540,"elapsed":10772,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["with graph.as_default() :\n","    xs       = tf.placeholder(tf.float32, (None, 32, 32, 3), name='xs') \n","    ys       = tf.placeholder(tf.int32, (None), name='ys')\n","    lr       = tf.placeholder_with_default(0.001, (), name='lr')\n","    wd       = tf.placeholder_with_default(0.9, (), name='wd')\n","    is_train = tf.placeholder_with_default(False, (), name='is_train')\n","    m        = tf.placeholder_with_default(0.9, (), name='momentum')\n","    dr       = tf.placeholder_with_default(0.9, (), name='dropout_ratio')   \n","    model    = tf.placeholder_with_default(\"VGG\", (), name='model')  \n","    \n","    with tf.name_scope('VGGBlock-1') :    \n","        layer = conv2d(xs, 32, trained_dict, is_train, 1, model, \n","                       'VGGBlock-1/conv1')\n","        pool  = max_pooling2d(layer) \n","        \n","    with tf.name_scope('VGGBlock-2') :\n","        layer = conv2d(pool, 64, trained_dict, is_train, 1, model, \n","                       'VGGBlock-2/conv2')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-3') :\n","        layer = conv2d(pool, 128, trained_dict, is_train, 1, model, \n","                       'VGGBlock-3/conv3')\n","        layer = conv2d(layer, 128, trained_dict, is_train, 2, model, \n","                       'VGGBlock-3/conv3')\n","        pool = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-4') :\n","        layer = conv2d(pool, 256, trained_dict, is_train, 1, model,\n","                       'VGGBlock-4/conv4')\n","        layer = conv2d(layer, 256,  trained_dict, is_train, 2, model,\n","                       'VGGBlock-4/conv4')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('fc') : \n","        flatten = tf.layers.flatten(pool)\n","        layer = fc(flatten, 1024, trained_dict, dr, is_train, model, 'fc/fc1')  \n","        layer = fc(layer, 1024, trained_dict, dr, is_train, model, 'fc/fc2')                                \n","        layer = fc(layer, 512, trained_dict, dr, is_train, model, 'fc/fc3')\n","        \n","    with tf.name_scope('output') : \n","        y_pred  = tf.layers.Dense(10, \n","                                  activation=None, \n","                                  name='y_pred')(layer)     \n","    \n","    with tf.name_scope('Loss') :\n","        loss = softmax_l2_with_loss(ys, y_pred, wd)\n","    loss = tf.identity(loss, name='loss')\n","        \n","    with tf.name_scope('metric') :\n","        rmse = tf.sqrt(loss)\n","    rmse = tf.identity(rmse, name='rmse')\n","    \n","    with tf.name_scope('accuracy') :\n","        acc = accuracy(ys, y_pred)\n","    acc = tf.identity(acc, name='acc')\n","\n","    with tf.name_scope('train') :\n","        # global_step = tf.train.get_or_create_global_step()\n","        optimizer   = tf.train.MomentumOptimizer(lr, \n","                                                 momentum = m, \n","                                                 use_nesterov = False)\n","        train_op    = optimizer.minimize(loss)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["W0621 09:07:06.834163 139887982245760 deprecation.py:323] From <ipython-input-12-953cfd55700b>:30: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0621 09:07:07.451270 139887982245760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0621 09:07:07.486424 139887982245760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0621 09:07:07.512171 139887982245760 deprecation.py:323] From <ipython-input-10-68f22acdb0a6>:2: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.math.argmax` instead\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lbIszm3083OJ","colab_type":"text"},"source":["## reconstruct model"]},{"cell_type":"code","metadata":{"id":"43KGZAr7ttRO","colab_type":"code","colab":{}},"source":["def model_b(config, graph_) :\n","    # load data    \n","    train_image, train_label, \\\n","    valid_image, valid_label, \\\n","    test_image, test_label = load_cifar10()\n","    \n","    # load hyper parameters\n","    model_type    = config['model_type']\n","    load_model    = config['load_model']\n","    save_model    = config['save_model']\n","    learning_rate = config['learning_rate']  \n","    batch_size    = config['batch_size']\n","    n_epoch       = config['epoch']\n","    n_step        = int(len(train_image) // batch_size)\n","    weight_decay  = config['weight_decay']\n","    dropout_ratio = config['dropout_ratio']\n","    \n","    # save directory \n","    if load_model == None :\n","         lode_dir = None\n","    else :\n","         lode_dir = \"./model/vgg_net_model_\" + load_model  \n","    \n","    if save_model == None :\n","        save_dir = None\n","        log_dir = None\n","    else :\n","        save_dir = \"./model/vgg_net_model_\" + save_model  \n","        log_dir = \"./log/vgg_net_model_\" + save_model\n","        \n","    with graph_.as_default() :        \n","        \n","        saver           = tf.train.Saver()    \n","        sess            = tf.Session(graph=graph_)\n","        sess.run(tf.global_variables_initializer())\n","        \n","        # create Instance\n","        train_generator = cifar_generator(train_image, train_label, batch_size)\n","        \n","                     \n","        loss_, acc_ = sess.run([rmse, acc], feed_dict = { xs: valid_image, \n","                                                          ys: valid_label,\n","                                                          is_train : False})\n","        print(\"check acc! valid loss = {:.4f}, valid acc = {:.2f}%\".format(loss_, acc_*100))\n","                        \n","        train_loss = []\n","        train_acc = []\n","        valid_loss = []\n","        valid_acc = []\n","        cnt = 0\n","        maximum_acc = 0.5\n","        for i in tqdm(range(n_epoch)) :\n","            for step in range(n_step) :\n","                batch_xs, batch_ys = next(train_generator)\n","                _, train_loss_, train_acc_ = sess.run([train_op, rmse, acc], \n","                                              feed_dict = { xs: batch_xs, \n","                                                            ys: batch_ys, \n","                                                            lr: learning_rate,\n","                                                            wd : weight_decay,\n","                                                            dr : dropout_ratio, \n","                                                            model : model_type,\n","                                                            is_train : True})\n","                train_loss.append(train_loss_)\n","                train_acc.append(train_acc_)\n","                \n","                # check validation set\n","                if step % 100 == 0 :\n","                    loss_, acc_ = sess.run([rmse, acc], \n","                                          feed_dict = { xs: valid_image, \n","                                                        ys: valid_label,\n","                                                        wd : weight_decay,\n","                                                        is_train : False})\n","                    valid_loss.append(loss_)\n","                    valid_acc.append(acc_)\n","                \n","                    # Save the model\n","                    if acc_ > maximum_acc :\n","                        print(\"log current model! valid loss = {:.4f}, \\\n","                               valid acc = {:.2f}%\".format(loss_, acc_*100))\n","                        maximum_acc = acc_\n","                        saver.save(sess, save_path = save_dir)\n","\n","            print(\" valid loss = {:.4f}, valid acc = {:.2f}%\". \\\n","                  format(loss_, acc_*100))\n","        \n","        train_writer.flush() # file을 disk에 쓴다\n","        \n","    return valid_loss, valid_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J31YhzJjTDSv","colab_type":"code","colab":{}},"source":["def show_result(vgg_loss, vgg_acc, BN_loss, BN_acc) :\n","    plt.plot(vgg_acc, linestyle = \"--\", color = \"red\", label = \"vgg_acc\")\n","    plt.plot(BN_acc, linestyle = \":\", color = \"blue\", label = \"BN_acc\")\n","    plt.legend()\n","    plt.show()\n","    \n","    plt.plot(vgg_loss, linestyle = \"--\", color = \"red\", label = \"vgg_loss\")\n","    plt.plot(BN_loss, linestyle = \":\", color = \"blue\", label = \"BN_loss\")\n","    plt.legend()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByDrbiAY81P_","colab_type":"text"},"source":["## main function"]},{"cell_type":"code","metadata":{"id":"iaElNp0ifP1L","colab_type":"code","outputId":"90824c21-0201-4c9d-b6ef-e25cc7492c92","executionInfo":{"status":"ok","timestamp":1561108035235,"user_tz":-540,"elapsed":16333,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["def main() :\n","    \n","    # config of hyper parameters\n","    config = {\n","        \"model_type\"      : \"VGG\",  # VGG or BN\n","        \"load_model\"      : \"a\",\n","        \"save_model\"      : \"b\",\n","        \"learning_rate\"   : 0.001,\n","        \"batch_size\"      : 1000,\n","        \"epoch\"           : 100,\n","        \"weight_decay\"    : 0.0005,\n","        \"dropout_ratio\"   : 0.5\n","    }\n","    \n","    # call reconstruct model\n","    vgg_loss, vgg_acc = model_b(config, graph)\n","    \n","    \n","    # # config of hyper parameters\n","    # config = {\n","    #     \"model_type\"      : \"BN\",  # VGG or BN\n","    #     \"load_model\"      : \"a\",\n","    #     \"save_model\"      : None,\n","    #     \"learning_rate\"   : 0.001,\n","    #     \"batch_size\"      : 1000,\n","    #     \"epoch\"           : 100,\n","    #     \"weight_decay\"    : 0.0005,\n","    #     # Cifa10 Dataset은 overfitting이 심하기 때문에 dropout을 제거하는 대신 비율을 줄임\n","    #     \"dropout_ratio\"   : 0.4 \n","    # }\n","    # \n","    # # call reconstruct model\n","    # BN_loss, BN_acc = model_b(config)\n","# \n","    # show_result(vgg_loss, vgg_acc, BN_loss, BN_acc)\n","    \n","    \n","if __name__ == '__main__':\n","    main()        "],"execution_count":14,"outputs":[{"output_type":"stream","text":["check acc! valid loss = 32.7248, valid acc = 10.77%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvPOw15r8xwt","colab_type":"text"},"source":["## save model"]},{"cell_type":"code","metadata":{"id":"9y81GK9E5vo_","colab_type":"code","colab":{}},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# \n","# !mkdir gdrive/My\\ Drive/vgg\n","# !mv ./model/vgg* gdrive/My\\ Drive/vgg\n","# !mv ./model/checkpoint gdrive/My\\ Drive/vgg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF42DO2epD7j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}