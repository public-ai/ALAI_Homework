{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week09_vgg_net_reconstruct_ver1.2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f6lmGly3toQS","colab_type":"text"},"source":["### Version History\n","+ Ver 1.0 : pretrained된 model의 network와 동일하게 구성하고, Optimizer를 Momentum으로 구성함\n","+ Ver 1.1 : train된 weights를 loading하는 부분을 cp mothold를 이용하지 않고 get collection으로 refactoring함\n","+ Ver 1.2 : (예정) augmentation code 테스트예정 , class화, batch normalization test"]},{"cell_type":"code","metadata":{"id":"Dm71BZOKS2dR","colab_type":"code","outputId":"95b1aa07-82f2-4482-86d6-10b2d0a92cf4","executionInfo":{"status":"ok","timestamp":1561088611345,"user_tz":-540,"elapsed":5417,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%matplotlib inline\n","!pip install tensorboardcolab\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import tensorboardcolab\n","from tensorflow.python.training import checkpoint_utils as cp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p1Kfmsih8PXF","colab_type":"text"},"source":["## Load Image Data set"]},{"cell_type":"code","metadata":{"id":"y9f2oWW-usPc","colab_type":"code","colab":{}},"source":["def load_cifar10() :\n","    # load cifar10 dataset \n","    from keras.datasets import cifar10\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    \n","    # reshape (None, 1) -> (None)\n","    y_train, y_test = [np.reshape(y_train, [-1]), np.reshape(y_test, [-1])]\n","\n","    # normalization \n","    x_train, x_test = [(x_train-x_train.max()) / (x_train.max()-x_train.min()),\n","                         (x_test-x_test.max()) / (x_test.max()-x_test.min())]\n","\n","    temp = x_train\n","    ratio = int(len(x_train) * 0.7)\n","    ratio_end = int(len(x_train) * 1.0)  \n","    \n","    train_image = temp[0:ratio, :, :, :]\n","    valid_image = temp[ratio:ratio_end , :, :, :]\n","    \n","    train_label = y_train[0:ratio]\n","    valid_label = y_train[ratio:ratio_end ]\n","    \n","    return train_image, train_label, valid_image, valid_label, x_test, y_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnZl2rmnLdKw","colab_type":"text"},"source":["## Data Provider"]},{"cell_type":"code","metadata":{"id":"Tpg4Xv_5Lckl","colab_type":"code","colab":{}},"source":["class DataProvider(object):\n","    def __init__(self, x, y):\n","        self.epoch_count = 0\n","        \n","        self.data = x\n","        self.label = y\n","        \n","        npr.seed(42)\n","        \n","        self.indices = self.generate_indices()\n","        \n","    def generate_indices(self):\n","        indices = list(range(len(self.data)))\n","        npr.shuffle(indices)\n","        \n","        return indices\n","    \n","    def next_batch(self, batch_size):\n","        idx = batch_size\n","        if len(self.indices) < batch_size:\n","            print(\"all data consumed, epoch + 1\")\n","            self.epoch_count += 1\n","            self.indices = self.generate_indices()\n","    \n","        target_indices = self.indices[:batch_size]\n","        del self.indices[:batch_size]\n","        \n","        return self.data[target_indices] , self.label[target_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2bPbJdyLlOS","colab_type":"code","colab":{}},"source":["def cifar_generator(data, labels, batch_size=32):\n","    start_idx = 0\n","    num_step = len(data) // batch_size\n","    indexes = np.arange(0, len(data))\n","    while True:\n","        if start_idx >= num_step-1:\n","            np.random.shuffle(indexes)\n","            start_idx = 0\n","        else:\n","            start_idx += 1            \n","        batch_index = indexes[start_idx*batch_size: (start_idx+1)*batch_size]\n","\n","        batch_data = data[batch_index]\n","        batch_label = labels[batch_index]\n","\n","        yield batch_data, batch_label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aK1rRzBM8Vmb","colab_type":"text"},"source":["## Load pretrained variables"]},{"cell_type":"code","metadata":{"id":"2WbQW6B2vHl8","colab_type":"code","colab":{}},"source":["def get_trained_weights() :\n","    \n","    # loading pretrained files\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    \n","    !mkdir ./model\n","    !cp gdrive/My\\ Drive/vgg/* model/ # from, to 임\n","    \n","    # temperary graph and session\n","    graph = tf.Graph()\n","    with graph.as_default() :\n","        sess = tf.Session(graph=graph)\n","        \n","        lode_dir = \"./model/vgg_net_model_a\"\n","        saver = tf.train.import_meta_graph(lode_dir + '.meta')\n","    \n","        saver.restore(sess, save_path = lode_dir)    \n","        # Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist.\n","        # The operation, 'save/Const', does not exist in the graph.\n","        \n","        reuse_vars = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n","        \n","        \n","        # trained variable name : values\n","        reuse_vars_dict = dict([(var.name.replace(':0',''), sess.run(var.name)) \n","                                for var in reuse_vars])\n","        \n","        sess.close()\n","        return reuse_vars_dict       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRcbockb8a9X","colab_type":"text"},"source":["## model functions"]},{"cell_type":"code","metadata":{"id":"C-XRcminN3uJ","colab_type":"code","colab":{}},"source":["def conv2d(input, trained_dict, trainable, floor, model, name) :\n","    for i in range(floor) :                \n","        kernel_init = trained_dict[str(name+'_kernel'+str(i+1))]  # 'conv1/kernel1'\n","        bias_init   = trained_dict[str(name+'_bias'+str(i+1))]\n","        kernel      = tf.Variable(kernel_init, name='kernel'+str(i+1))               \n","        bias        = tf.Variable(bias_init, name='bias'+str(i+1))        \n","        layer       = tf.nn.conv2d(input, kernel, strides=[1,1,1,1], \n","                                   padding='SAME')\n","        layer       = layer + bias\n","        layer       = tf.nn.relu(layer)\n","        \n","        if model == \"BN\" :\n","            layer = tf.layers.BatchNormalization()(layer, training=is_train)\n","            \n","    return layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jFNUz-1TF_2","colab_type":"code","colab":{}},"source":["def max_pooling2d(input) :\n","    pool  = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(input) \n","    \n","    return pool"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-xjFVuPTm8v","colab_type":"code","colab":{}},"source":["def fc(input, trained_dict, is_train, model, name) :\n","    kernel_init = trained_dict[str(name+'_kernel1')]\n","    bias_init   = trained_dict[str(name+'_bias1')]\n","    kernel      = tf.Variable(kernel_init, name = \"kernel1\")\n","    bias        = tf.Variable(bias_init, name = \"bias1\")\n","    z           = tf.matmul(input, kernel) + bias \n","    logits      = tf.nn.relu(z)\n","    \n","    if model == \"BN\" :\n","        logits = tf.layers.BatchNormalization()(logits, training=is_train)\n","        \n","    dropout     = tf.layers.Dropout(0.5)(logits, training = is_train)\n","    \n","    return dropout"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2eesMFQUeHv","colab_type":"code","colab":{}},"source":["def softmax_l2_with_loss(ys_true, ys_pred, weight_decay) :  \n","    sce_loss = tf.reduce_mean(\n","        tf.losses.sparse_softmax_cross_entropy(labels=ys_true, logits=ys_pred))\n","    l2_loss  = tf.add_n([tf.nn.l2_loss(var) for var in tf.global_variables()])\n","    loss     = sce_loss + weight_decay * l2_loss\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7-g17VsVBU6","colab_type":"code","colab":{}},"source":["def accuracy(y_true, y_pred) :\n","    pred     = tf.cast(tf.arg_max(y_pred, 1), tf.int32)\n","    correct  = tf.equal(pred, y_true)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","    # add tensor to tensorboard\n","    acc_tb   = tf.summary.scalar(name='accuracy', tensor=accuracy)\n","    \n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOLMQsFi8dS7","colab_type":"text"},"source":["## main model"]},{"cell_type":"code","metadata":{"id":"5OKwIlxabiGO","colab_type":"code","colab":{}},"source":["graph = tf.Graph()\n","# temp = tf.Variable(0)\n","# saver = tf.train.Saver() "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbIszm3083OJ","colab_type":"text"},"source":["## reconstruct model"]},{"cell_type":"code","metadata":{"id":"43KGZAr7ttRO","colab_type":"code","colab":{}},"source":["def model_b(config, graph_) :\n","    # load data    \n","    train_image, train_label, \\\n","    valid_image, valid_label, \\\n","    test_image, test_label = load_cifar10()\n","    \n","    # load hyper parameters\n","    load_model    = config['load_model']\n","    save_model    = config['save_model']\n","    learning_rate = config['learning_rate']  \n","    batch_size    = config['batch_size']\n","    n_epoch       = config['epoch']\n","    n_step        = int(len(train_image) // batch_size)\n","    weight_decay  = config['weight_decay']\n","    dropout_ratio = config['dropout_ratio']\n","    \n","    # save directory \n","    if load_model == None :\n","         lode_dir = None\n","    else :\n","         lode_dir = \"./model/vgg_net_model_\" + load_model  \n","    save_dir = \"./model/vgg_net_model_\" + save_model  \n","    log_dir = \"./log/vgg_net_model_\" + save_model\n","    \n","    # tensorboard\n","    # tbc = tensorboardcolab.TensorBoardColab(graph_path = log_dir)\n","    train_writer = tf.summary.FileWriter(logdir = log_dir)\n","    train_writer.add_graph(tf.get_default_graph())\n","    merged_all = tf.summary.merge_all()    \n","    \n","    # graph = tf.Graph()\n","    with graph_.as_default() :        \n","        \n","        # create Instance\n","        # model_b         = Conv_net(graph, sess)\n","        train_generator = cifar_generator(train_image, train_label, batch_size)\n","        \n","        sess  = tf.Session(graph=graph_)   \n","        sess.run(tf.global_variables_initializer())\n","\n","        \n","        # loading pretrained data\n","        trained_dict = get_trained_weights()\n","    \n","        print(valid_image[0])\n","        loss_, acc_ = sess.run([rmse, acc], feed_dict = { xs: valid_image, \n","                                                          ys: valid_label,\n","                                                          wd : weight_decay,\n","                                                          is_train : False})\n","        print(\"check loading pretrained data! \\\n","              valid loss = {:.4f}, valid acc = {:.2f}%\".format(loss_, acc_*100))\n","                        \n","        # train_loss = []\n","        # train_acc = []\n","        # valid_loss = []\n","        # valid_acc = []\n","        # cnt = 0\n","        # maximum_acc = 0.5\n","        # for i in tqdm(range(n_epoch)) :\n","        #     for step in range(n_step) :\n","        #         batch_xs, batch_ys = next(train_generator)\n","        #         _, train_loss_, train_acc_ = sess.run([train_op, rmse, acc], \n","        #                                       feed_dict = { xs: batch_xs, \n","        #                                                     ys: batch_ys, \n","        #                                                     lr: learing_rate,\n","        #                                                     wd : weight_decay,\n","        #                                                     dr : dropout_ratio, \n","        #                                                     model : model_type,\n","        #                                                     is_train : True})\n","        #         train_loss.append(train_loss_)\n","        #         train_acc.append(train_acc_)\n","        #     \n","        #         # check validation set\n","        #         if step % 100 == 0 :\n","        #             loss_, acc_ = sess.run([rmse, acc], \n","        #                                   feed_dict = { xs: valid_image, \n","        #                                                 ys: valid_label,\n","        #                                                 wd : weight_decay,\n","        #                                                 is_train : False})\n","        #             valid_loss.append(loss_)\n","        #             valid_acc.append(acc_)\n","        #         \n","        #             # Save the model\n","        #             if acc_ > maximum_acc :\n","        #                 print(\"log current model! valid loss = {:.4f}, \\\n","        #                        valid acc = {:.2f}%\".format(loss_, acc_*100))\n","        #                 maximum_acc = acc_\n","        #                 saver.save(sess, save_path = save_dir)\n","        #     print(\" valid loss = {:.4f}, valid acc = {:.2f}%\". \\\n","        #           format(loss_, acc_*100))\n","        # \n","        # train_writer.flush() # file을 disk에 쓴다"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVMczcqNq6Np","colab_type":"code","outputId":"815b015a-abd7-4cad-eb5f-c28dbfd1043e","executionInfo":{"status":"ok","timestamp":1561088618119,"user_tz":-540,"elapsed":12042,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":373}},"source":["# class Conv_net() :\n","#     def __init__(self, graph, sess) :\n","#         # super().__init__()\n","#         self.graph = graph\n","#         self.sess = sess\n","#         self.sess.run(tf.global_variables_initializer())\n","#         print(\"initial is complete!\")\n","#         \n","#     def train(self, image, label, lr) :\n","with graph.as_default() :\n","    xs       = tf.placeholder(tf.float32, (None, 32, 32, 3), name='xs') \n","    ys       = tf.placeholder(tf.int32, (None), name='ys')\n","    lr       = tf.placeholder_with_default(0.001, (), name='lr')\n","    wd       = tf.placeholder_with_default(0.9, (), name='wd')\n","    is_train = tf.placeholder_with_default(False, (), name='is_train')\n","    m        = tf.placeholder_with_default(0.9, (), name='momentum')\n","    dr       = tf.placeholder_with_default(0.9, (), name='dropout_ratio')   \n","    model    = tf.placeholder_with_default(\"VGG\", (), name='model')  \n","    \n","    trained_dict = get_trained_weights()\n","    # print(trained_dict)    \n","    \n","    with tf.name_scope('VGGBlock-1') :    \n","        layer = conv2d(xs, trained_dict, is_train, 1, model, 'VGGBlock-1/conv1')\n","        pool  = max_pooling2d(layer)   \n","        \n","    with tf.name_scope('VGGBlock-2') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, model, 'VGGBlock-2/conv2')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-3') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, model, 'VGGBlock-3/conv3')\n","        layer = conv2d(layer, trained_dict, is_train, 2, model, 'VGGBlock-3/conv3')\n","        pool = max_pooling2d(layer)\n","        \n","    with tf.name_scope('VGGBlock-4') :\n","        layer = conv2d(pool, trained_dict, is_train, 1, model,'VGGBlock-4/conv4')\n","        layer = conv2d(layer, trained_dict, is_train, 2, model,'VGGBlock-4/conv4')\n","        pool  = max_pooling2d(layer)\n","        \n","    with tf.name_scope('fc1') : \n","        flatten = tf.layers.flatten(pool)\n","        layer = fc(flatten, trained_dict, is_train, model, 'fc/fc1')\n","        \n","    with tf.name_scope('fc2') :     \n","        layer = fc(layer, trained_dict, is_train, model, 'fc/fc2')\n","\n","    with tf.name_scope('fc3') :                                  \n","        layer = fc(layer, trained_dict, is_train, model, 'fc/fc3')\n","    \n","    with tf.name_scope('output') : \n","        y_pred  = tf.layers.Dense(10, \n","                                  activation=None, \n","                                  name='y_pred')(layer)     \n","    \n","    with tf.name_scope('Loss') :\n","        loss = softmax_l2_with_loss(ys, y_pred, wd)\n","    loss = tf.identity(loss, name='loss')\n","        \n","    with tf.name_scope('metric') :\n","        rmse = tf.sqrt(loss)\n","    rmse = tf.identity(rmse, name='rmse')\n","    \n","    with tf.name_scope('accuracy') :\n","        acc = accuracy(ys, y_pred)\n","    acc = tf.identity(acc, name='acc')\n","\n","    with tf.name_scope('train') :\n","        # global_step = tf.train.get_or_create_global_step()\n","        optimizer   = tf.train.MomentumOptimizer(lr, \n","                                                 momentum = m, \n","                                                 use_nesterov = False)\n","        train_op    = optimizer.minimize(loss)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","mkdir: cannot create directory ‘./model’: File exists\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0621 03:43:34.169649 140431447394176 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","W0621 03:43:35.267228 140431447394176 deprecation.py:323] From <ipython-input-13-5754eecf7188>:33: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0621 03:43:35.643105 140431447394176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","W0621 03:43:35.945533 140431447394176 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0621 03:43:35.978850 140431447394176 deprecation.py:323] From <ipython-input-10-68f22acdb0a6>:2: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.math.argmax` instead\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ByDrbiAY81P_","colab_type":"text"},"source":["## main function"]},{"cell_type":"code","metadata":{"id":"iaElNp0ifP1L","colab_type":"code","outputId":"2fe0c4a4-5406-4bed-845a-e09d9cddc4b5","executionInfo":{"status":"ok","timestamp":1561088627216,"user_tz":-540,"elapsed":21082,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":988}},"source":["def main() :\n","    \n","    # config of hyper parameters\n","    config = {\n","        \"model_type\"      : \"VGG\",  # VGG or BN\n","        \"load_model\"      : \"a\",\n","        \"save_model\"      : \"b\",\n","        \"learning_rate\"   : 0.001,\n","        \"batch_size\"      : 1000,\n","        \"epoch\"           : 100,\n","        \"weight_decay\"    : 0.0005,\n","        \"dropout_ratio\"   : 0.5\n","    }\n","    \n","    # call reconstruct model\n","    model_b(config, graph)\n","    \n","    \n","    # # config of hyper parameters\n","    # config = {\n","    #     \"model_type\"      : \"BN\",  # VGG or BN\n","    #     \"load_model\"      : \"a\",\n","    #     \"save_model\"      : \"b\",\n","    #     \"learning_rate\"   : 0.001,\n","    #     \"batch_size\"      : 1000,\n","    #     \"epoch\"           : 100,\n","    #     \"weight_decay\"    : 0.0005,\n","    #     # Cifa10 Dataset은 overfitting이 심하기 때문에 dropout을 제거하는 대신 비율을 줄임\n","    #     \"dropout_ratio\"   : 0.4 \n","    # }\n","    # \n","    # # call reconstruct model\n","    # model_b(config)\n","    \n","    \n","if __name__ == '__main__':\n","    main()        "],"execution_count":14,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","mkdir: cannot create directory ‘./model’: File exists\n","[[[0.2627451  0.21176471 0.14509804]\n","  [0.27843137 0.23137255 0.16470588]\n","  [0.29019608 0.23921569 0.17254902]\n","  ...\n","  [0.18039216 0.05882353 0.02745098]\n","  [0.21960784 0.06666667 0.00784314]\n","  [0.2627451  0.09803922 0.02352941]]\n","\n"," [[0.28627451 0.23529412 0.16862745]\n","  [0.30588235 0.25490196 0.18823529]\n","  [0.31764706 0.26666667 0.2       ]\n","  ...\n","  [0.14509804 0.0745098  0.03529412]\n","  [0.21568627 0.08235294 0.03529412]\n","  [0.2745098  0.09803922 0.04705882]]\n","\n"," [[0.30980392 0.25882353 0.19215686]\n","  [0.3254902  0.2745098  0.20784314]\n","  [0.3372549  0.28627451 0.21960784]\n","  ...\n","  [0.06666667 0.07058824 0.02352941]\n","  [0.14901961 0.07843137 0.03921569]\n","  [0.22352941 0.09411765 0.05490196]]\n","\n"," ...\n","\n"," [[0.46666667 0.32941176 0.24705882]\n","  [0.48627451 0.35294118 0.2627451 ]\n","  [0.50588235 0.37254902 0.27058824]\n","  ...\n","  [0.99607843 0.71764706 0.71372549]\n","  [0.94509804 0.58823529 0.62352941]\n","  [0.97254902 0.58431373 0.65490196]]\n","\n"," [[0.4745098  0.3372549  0.25098039]\n","  [0.49411765 0.35686275 0.26666667]\n","  [0.51372549 0.38039216 0.27843137]\n","  ...\n","  [0.99607843 0.69803922 0.71372549]\n","  [0.94117647 0.59215686 0.63529412]\n","  [0.95294118 0.62745098 0.68235294]]\n","\n"," [[0.47058824 0.33333333 0.25098039]\n","  [0.49019608 0.35686275 0.27058824]\n","  [0.50588235 0.37254902 0.2745098 ]\n","  ...\n","  [0.97254902 0.65490196 0.68627451]\n","  [0.91764706 0.59215686 0.63921569]\n","  [0.94901961 0.75686275 0.79607843]]]\n","check loading pretrained data!               valid loss = 1.7287, valid acc = 1.75%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvPOw15r8xwt","colab_type":"text"},"source":["## save model"]},{"cell_type":"code","metadata":{"id":"9y81GK9E5vo_","colab_type":"code","colab":{}},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# \n","# !mkdir gdrive/My\\ Drive/vgg\n","# !mv ./model/vgg* gdrive/My\\ Drive/vgg\n","# !mv ./model/checkpoint gdrive/My\\ Drive/vgg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF42DO2epD7j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}