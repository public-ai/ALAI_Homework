{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week09_vgg_net_modelB.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Dm71BZOKS2dR","colab_type":"code","outputId":"1bf807de-cafb-45f2-bc6e-372f64ad38e8","executionInfo":{"status":"ok","timestamp":1560693536300,"user_tz":-540,"elapsed":5719,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%matplotlib inline\n","!pip install tensorboardcolab\n","\n","import numpy as np \n","import tensorflow as tf \n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import tensorboardcolab"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fMckkTBoTDxV","colab_type":"code","outputId":"11f30f99-86d0-4b2c-9366-7bf3e7f9fd0f","executionInfo":{"status":"ok","timestamp":1560693540168,"user_tz":-540,"elapsed":9440,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# load cifar10 dataset \n","from keras.datasets import cifar10\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-_00tFXVTIkn","colab_type":"code","outputId":"3e27c0f5-eff8-4037-f555-09ecf676e5bd","executionInfo":{"status":"ok","timestamp":1560693541672,"user_tz":-540,"elapsed":10814,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# reshape (None, 1) -> (None)\n","y_train, y_test = [np.reshape(y_train, [-1]), np.reshape(y_test, [-1])]\n","\n","# normalization \n","x_train, x_test = [(x_train - x_train.max()) / (x_train.max() - x_train.min()),\n","                   (x_test - x_test.max()) / (x_test.max() - x_test.min())]\n","\n","# N class\n","n_classes = 10\n","print('image shape : {}, label shape : {} '.format(x_train.shape, y_train.shape))\n","print('image shape : {}, label shape : {} '.format(x_test.shape, y_test.shape))\n","print('train minimun : {}, train_maximum : {} '.format(x_train.min(), x_train.max()))\n","print('tests minimun : {}, test_maximum : {} '.format(x_test.min(), x_test.max()))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["image shape : (50000, 32, 32, 3), label shape : (50000,) \n","image shape : (10000, 32, 32, 3), label shape : (10000,) \n","train minimun : 0.0, train_maximum : 1.0 \n","tests minimun : 0.0, test_maximum : 1.0 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fDdQkH56bJMO","colab_type":"code","outputId":"0a88d86f-636c-44dc-a670-df9ae2548fdb","executionInfo":{"status":"ok","timestamp":1560693541674,"user_tz":-540,"elapsed":9792,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["temp = x_train\n","ratio = int(len(x_train) * 0.7)\n","ratio_end = int(len(x_train) * 1.0)\n","\n","x_train = temp[0:ratio, :, :, :]\n","x_validation = temp[ratio:ratio_end , :, :, :]\n","\n","y_train_label = y_train[0:ratio]\n","y_validation_label = y_train[ratio:ratio_end ]\n","\n","print(y_train_label[:10]) # label이 one_hot encoding상태가 아니다.\n","\n","print(x_train.shape, y_train_label.shape)\n","print(x_validation.shape, y_validation_label.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[6 9 9 4 1 1 2 7 8 3]\n","(35000, 32, 32, 3) (35000,)\n","(15000, 32, 32, 3) (15000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mnZl2rmnLdKw","colab_type":"text"},"source":["## Data Provider"]},{"cell_type":"code","metadata":{"id":"Tpg4Xv_5Lckl","colab_type":"code","colab":{}},"source":["class DataProvider(object):\n","    def __init__(self, x, y):\n","        self.epoch_count = 0\n","        \n","        self.data = x\n","        self.label = y\n","        \n","        npr.seed(42)\n","        \n","        self.indices = self.generate_indices()\n","        \n","    def generate_indices(self):\n","        indices = list(range(len(self.data)))\n","        npr.shuffle(indices)\n","        \n","        return indices\n","    \n","    def next_batch(self, batch_size):\n","        idx = batch_size\n","        if len(self.indices) < batch_size:\n","            print(\"all data consumed, epoch + 1\")\n","            self.epoch_count += 1\n","            self.indices = self.generate_indices()\n","    \n","        target_indices = self.indices[:batch_size]\n","        del self.indices[:batch_size]\n","        \n","        return self.data[target_indices] , self.label[target_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2bPbJdyLlOS","colab_type":"code","colab":{}},"source":["def cifar_generator(data, labels, batch_size=32):\n","    start_idx = 0\n","    num_step = len(data) // batch_size\n","    indexes = np.arange(0, len(data))\n","    while True:\n","        if start_idx >= num_step-1:\n","            np.random.shuffle(indexes)\n","            start_idx = 0\n","        else:\n","            start_idx += 1            \n","        batch_index = indexes[start_idx*batch_size:\n","                              (start_idx+1)*batch_size]\n","\n","        batch_data = data[batch_index]\n","        batch_label = labels[batch_index]\n","\n","        yield batch_data, batch_label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tppzG1VKq6kj","colab_type":"text"},"source":["## Model B : Build  --> Model A와 동일한 Structure이고, Optimizer만 Momentum으로 변경함"]},{"cell_type":"code","metadata":{"id":"SVMczcqNq6Np","colab_type":"code","outputId":"aba447d4-5725-493d-c61a-5c4008f8c4ec","executionInfo":{"status":"ok","timestamp":1560696498329,"user_tz":-540,"elapsed":1583,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["graph = tf.Graph()\n","with graph.as_default() :\n","    xs = tf.placeholder(tf.float32, (None, 32, 32, 3), name='xs') # 32*32 = 1024\n","    ys = tf.placeholder(tf.int32, (None), name='ys')\n","    lr = tf.placeholder_with_default(0.001, (), name='lr')\n","    \n","    with tf.name_scope('convolution_layer_1') :   ## filters is number of channels\n","        # kernel_init = tf.random.normal(shape=[2, 2, 3, 16], mean=0.0, stddev=0.1, dtype=tf.float32, name='kernel_init') # random normal \n","        kernel_init = tf.zeros([2, 2, 3, 16])\n","        kernel= tf.Variable(kernel_init, name='kernel')               \n","        bias_init = tf.zeros([16])\n","        bias = tf.Variable(bias_init, name='bias')\n","    \n","        layer = tf.nn.conv2d(xs, kernel, strides=[1,1,1,1], padding='SAME') # convolution and add bias\n","        layer = layer + bias\n","        layer = tf.nn.relu(layer)\n","        \n","        pool = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)\n","\n","    with tf.name_scope('convolution_layer_2') :   ## filters is number of channels\n","        kernel_init = tf.zeros([2, 2, 16, 32]) \n","        kernel= tf.Variable(kernel_init, name='kernel')               \n","        bias_init = tf.zeros([32])\n","        bias = tf.Variable(bias_init, name='bias')\n","    \n","        layer = tf.nn.conv2d(pool, kernel, strides=[1,1,1,1], padding='SAME') # convolution and add bias   \n","        layer = layer + bias\n","        layer = tf.nn.relu(layer)\n","           \n","        pool = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)\n","        \n","            \n","    with tf.name_scope('convolution_layer_3') :   ## filters is number of channels\n","        kernel_init = tf.zeros([3, 3, 32, 64])\n","        kernel= tf.Variable(kernel_init, name='kernel')               \n","        bias_init = tf.zeros([64])\n","        bias = tf.Variable(bias_init, name='bias')    \n","        layer = tf.nn.conv2d(pool, kernel, strides=[1,1,1,1], padding='SAME') # convolution and add bias       \n","        layer = layer + bias\n","        layer = tf.nn.relu(layer)\n","\n","        kernel_init2 = tf.zeros(shape=[3, 3, 64, 64]) \n","        kernel2= tf.Variable(kernel_init2, name='kernel')               \n","        bias_init2 = tf.zeros([64])\n","        bias2 = tf.Variable(bias_init2, name='bias')    \n","        layer = tf.nn.conv2d(layer, kernel2, strides=[1,1,1,1], padding='SAME') # convolution and add bias   \n","        layer = layer + bias2\n","        layer = tf.nn.relu(layer)\n","    \n","        pool = tf.layers.MaxPooling2D(pool_size=[2,2], strides=[2,2])(layer)        \n","        \n","    # receptive filed = 27, images size = 32, output imagesize = (4, 4)\n","    \n","    with tf.name_scope('Reconstruct') : \n","        flatten = tf.layers.flatten(pool)\n","        layer = tf.layers.Dense(1024, activation=tf.nn.relu)(flatten) # 32*32\n","        layer = tf.layers.Dense(1024, activation=tf.nn.relu)(layer)\n","        y_pred = tf.layers.Dense(10, activation=None, name='y_pred')(layer)\n","        \n","    with tf.name_scope('Loss') :\n","        loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=ys, logits=y_pred))\n","        print(loss.shape)\n","        \n","    with tf.name_scope('metric') :\n","        rmse = tf.sqrt(loss)\n","    \n","    with tf.name_scope('accuracy') :\n","        pred = tf.cast(tf.arg_max(y_pred, 1), tf.int32)\n","        correct = tf.equal(pred, ys)\n","        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","        \n","        # add tensor to tensorboard\n","        acc_tb = tf.summary.scalar(name='accuracy', tensor=accuracy)\n","\n","    with tf.name_scope('train') :\n","        # train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n","        train_op = tf.train.MomentumOptimizer(lr, momentum=0.9, use_nesterov=False).minimize(loss)\n","        "],"execution_count":18,"outputs":[{"output_type":"stream","text":["()\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nyn0SDbfrGRV","colab_type":"text"},"source":["## Model B : Train"]},{"cell_type":"markdown","metadata":{"id":"G0hYYOQxyonM","colab_type":"text"},"source":["### Load trained weight and bias"]},{"cell_type":"code","metadata":{"id":"0YTr5PuHZaK3","colab_type":"code","colab":{}},"source":["from tensorflow.python.training import checkpoint_utils as cp\n","\n","# 변수 불러오기\n","var_names = cp.list_variables('./model/vgg_net')   \n","\n","# 값 불러오기\n","con_layer1_kernel  = cp.load_variable('./model/vgg_net' , 'convolution_layer_1/kernel')   \n","con_layer1_bias    = cp.load_variable('./model/vgg_net' , 'convolution_layer_1/bias')\n","\n","con_layer2_kernel  = cp.load_variable('./model/vgg_net' , 'convolution_layer_2/kernel')   \n","con_layer2_bias    = cp.load_variable('./model/vgg_net' , 'convolution_layer_2/bias')\n","\n","con_layer3_kernel1 = cp.load_variable('./model/vgg_net' , 'convolution_layer_3/kernel')   \n","con_layer3_bias1   = cp.load_variable('./model/vgg_net' , 'convolution_layer_3/bias')\n","con_layer3_kernel2 = cp.load_variable('./model/vgg_net' , 'convolution_layer_3/kernel_1')   \n","con_layer3_bias2   = cp.load_variable('./model/vgg_net' , 'convolution_layer_3/bias_1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTazVoc9zcd3","colab_type":"text"},"source":["## Model B : Train"]},{"cell_type":"code","metadata":{"id":"ZEShi1e3zefJ","colab_type":"code","outputId":"13a31797-ff2c-40d8-d6f4-9b1b1b4f2105","executionInfo":{"status":"error","timestamp":1560696516842,"user_tz":-540,"elapsed":14000,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":3385}},"source":["with graph.as_default() :\n","    \n","    log_dir = \"./log/vgg_net_model_b\"   \n","    lode_dir = \"./model/vgg_net\"\n","    save_dir = \"./model/vgg_net_model_b\"    \n","    \n","    # 텐서보드에 연결합니다.\n","    tbc = tensorboardcolab.TensorBoardColab(graph_path = log_dir)\n","    train_writer = tf.summary.FileWriter(logdir = log_dir)\n","    train_writer.add_graph(tf.get_default_graph())\n","    merged_all = tf.summary.merge_all()    \n","    \n","    sess = tf.Session()\n","    # weight를 불러왔기 때문에 초기화를 하면 안됨!\n","    sess.run(tf.global_variables_initializer()) \n","    \n","    # Step2 : 기록\n","    saver = tf.train.Saver()\n","    \n","    # loading parameters\n","    saver.restore(sess, save_path = lode_dir)\n","    # convolution_layer_1/kernel/Momentum <-- 얘를 초기화 하지 못함\n","    \n","    # Training\n","    batch_size = 50\n","    n_epoch = 30\n","    n_step = int(len(x_train) // batch_size)  # //은 몫이다.\n","    learing_rate = 0.0001\n","    \n","    # instance 생성\n","    train_generator = cifar_generator(x_train, y_train_label, batch_size)\n","    \n","    train_loss = []\n","    valid_loss = []\n","    valid_acc = []\n","    cnt = 0\n","    minimum_loss = 1.1\n","    for i in tqdm(range(n_epoch)) :\n","        for step in range(n_step) :\n","            batch_xs, batch_ys = next(train_generator)\n","            _, train_loss_, tbs_train_ = sess.run([train_op, rmse, merged_all], feed_dict = { xs: batch_xs, \n","                                                                                              ys: batch_ys, \n","                                                                                              lr: learing_rate})\n","            train_writer.add_summary(tbs_train_, global_step=cnt) # 흠 되야 하는데 안된다.\n","            cnt += 1\n","            train_loss.append(train_loss_)\n","            \n","            # check validation set\n","            if step % 100 == 0 :\n","                loss_, acc_ = sess.run([rmse, accuracy], feed_dict = { xs: x_validation, \n","                                                                       ys: y_validation_label})\n","                valid_loss.append(loss_)\n","                valid_acc.append(acc_)\n","                \n","                # Save the model\n","                if loss_ < minimum_loss :\n","                    print(\"log current model!\")\n","                    minimum_loss = loss_\n","                    saver.save(sess, save_path = save_dir)\n","        print(\"loss = {:.4f}, acc = {:.2f}%\".format(loss_, acc_*100))\n","    print(\"loss = {:.4f}, acc = {:.2f}%\".format(loss_, acc_*100))\n","    \n","    train_writer.flush() # file을 disk에 쓴다"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","https://479415ea.ngrok.io\n"],"name":"stdout"},{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: 2 root error(s) found.\n  (0) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[{{node save/RestoreV2}}]]\n\t [[save/RestoreV2/_49]]\n  (1) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[{{node save/RestoreV2}}]]\n0 successful operations.\n0 derived errors ignored.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1286\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: 2 root error(s) found.\n  (0) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-19-ef2dc52334d7>:18) ]]\n\t [[save/RestoreV2/_49]]\n  (1) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-19-ef2dc52334d7>:18) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-ef2dc52334d7>\", line 18, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m   \u001b[0mobject_graph_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_GRAPH_PROTO_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m   \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableObjectGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader_GetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-ef2dc52334d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# loading parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m# convolution_layer_1/kernel/Momentum <-- 얘를 초기화 하지 못함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1302\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\n2 root error(s) found.\n  (0) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-19-ef2dc52334d7>:18) ]]\n\t [[save/RestoreV2/_49]]\n  (1) Not found: Key convolution_layer_1/bias/Momentum not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-19-ef2dc52334d7>:18) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-ef2dc52334d7>\", line 18, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"]}]},{"cell_type":"code","metadata":{"id":"9y81GK9E5vo_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}