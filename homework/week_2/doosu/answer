\\<!-- 2주차 과제 첫번째 숙제 -->
-- 2주차 과제 첫번쨰 숙제 --
import numpy as np
import matplotlib.pyplot as plt

dataset = np.array([
    [120, 3],
    [105, 2],
    [25, 12],
    [32, 15],
    [17, 9],
    [98, 5],
    [130, 1],
    [0, 16],
    [40, 20],
    [100, 10]
])

labels = np.array(["comedy", "comedy", "drama",
                   "drama","drama","comedy","comedy",
                   "drama","drama","comedy"])

def classify_knn_modify(inX, dataset, labels, K):
    # (1) 우리가 분류항목을 알고자 하는 점 (inX)와
    # 알고 있는 점들(dataset)과의 모든 점 거리를 계산

    dists = np.abs(np.sum((inX - dataset), axis=1))
    #dists = np.sqrt(np.sum((inX - dataset) ** 2, axis=1))
    #dists = np.max(np.abs((inX - dataset)))

    print("dists 를 출력합니다.")
    print(dists)

    # (2) 오름 차순으로 거리의 길이를 정렬
    sorted_index = dists.argsort()

    # (3) inX와의 거리가 가장짧은 K개의 아이템 추출
    sorted_labels = labels[sorted_index]
    K_nearest_labels = sorted_labels[:K]

    # (4) K개의 아이템에서 가장 많은 분류 항목 찾기
    _labels, count_labels = np.unique(K_nearest_labels,
                                      return_counts=True)

    # (5) 해당 항목 반환
    return _labels[count_labels.argmax()]

inX = np.array([60,0])

plt.title("The Category of Movie")
plt.scatter(dataset[labels=="comedy",0],dataset[labels=="comedy",1],
            label='comedy', c='g')
plt.scatter(dataset[labels=="drama",0],dataset[labels=="drama",1],
            label='drama', c='r')
plt.scatter(inX[0],inX[1],label="?",
            c='b')

plt.xlim(-10,140)
plt.ylim(-10,40)

plt.xlabel('The number of smile')
plt.ylabel('The number of cry')
plt.legend()
plt.show()

dataset = (dataset - dataset.mean()) / dataset.std()
answer = classify_knn_modify(inX,dataset,labels,4)
print(answer)



\*\* knn 알고리즘 숙제를 하면서 느낀점 \*\*
처음에 숙제에서 무엇을 의도하는지를 몰랐다. 사실 처음나의 생각은 웃는 횟수가 우는 횟수보다 많으면 코미디이고, 우는 횟수가 웃는 횟수보다 많으면 드라마일것이라고 생각하였다.
그런 알고자 하는 inX에 대하여 데이터셋과 거리계산을 하였는대 문제에서 말한거처럼 한번도 울지도 않았는대 드라마로 답이 나왔었다. 데이터를 자세히보니웃는횟수가 40이고 우는횟수가20인 feature는
label이 drama였다. 어떻게 문제를 처리하기위해 고민한 결과 수업중의 선생님의 말씀이 생각났다. 범주가 큰 데이터는 범주가 작은 데이터에 영향을 끼칠수 있다 라는말씀이셨는대...
다시말하면, 아래와 같은 문제점이 도출된다.
1. smile의 데이터 범주는 0 ~ 130 수치를 가지고 있다.
2. cry의 데이터 범주는 1 ~ 20 수치를 가지고 있다.
3. 각각의 data들은 수치의 단위가 다르다
4. 기존 knn 알고리즘을 사용시 잘못된 결과값이 나온다.

- 잘못된 결과값이 나온 이유 -
우리가 구하고자, 알고자 하는 점 inX 와 dataset에 feture들의 데이터를 이용하여 거리계산을 하였을때 단위가 큰 수치는 단위가 작은 수치의 결과 값에 영향을 준다는 것이다? 
이렇게 말하는게 맞는지 모르겠지만 예를 들어설명하자면 아래와 같다
코미디의 데이터 수치 범위가 300 ~ 500 까지있고 드라마의 데이터 수치 범위가 0 ~ 10 까지있다고 하자. 극단적이긴 하지만 이해하기 쉽게 설명을 정리하고 싶다.
위와 같은 데이터 수치는 코미디가 나올 확률이 극단적으로 높다고 할 수 있다. 즉, 이런 이유로 거리계산시 올바르지 않은  거리계산을 하게된다.

- 해결 방법 -
거리계산시 수치가 큰 값만 결과에 반영이 되기때문에 이를 해결하기위해서는 표준화(normalization), 정규화(standardization)을 통해 데이터의 전처리 과정을 거쳐야 한다.
데이터의 전처리 과정은 위와 같은 이유때문에 꼭 거쳐야할 과정이라고 느꼈다. 
데이터 정규화 과정의 식은 아래와 같이 하였다(normalization은 시도하였는대 실패하였다)
dataset = (dataset - dataset.mean()) / dataset.std()
편차에 표준편차를 나누어 줌으로써 데이터들이 평균을 기준으로 얼마나 떨어져 있는지를 나타냄으로써 2개 이상의 데이터 범주의 단위가 다를때 각각의 데이터들을 같은 기준으로 볼 수 있게? 해주었다.
그 후 테스트를 통해 답을 확인할 수 있었다.
정규화를 통해 나온 데이터들은 -0.850 ~ 2.115 로분포되었다 확실히 데이터의 간격이 줄어든것을 볼 수 있었다. 이로써 거리계산을 할때 올바르지 않은 결과를 낼 확률이 줄어든다는 것을 알수있었다.
또한 k의갯수에 따라서 결과 값에 영향을 줄수있다. 짝수보다는 홀수 1보다는 1이상의 값을 설정해야 할거 같다.




-- 2주차 과제 2번째 숙제 --

def cluster_kmeans(dataset, k):
    # (1) 중심점 초기화
    min_x = dataset[:, 0].min()
    max_x = dataset[:, 0].max()
    min_y = dataset[:, 1].min()
    max_y = dataset[:, 1].max()

    center_x = np.random.uniform(low=min_x, high=max_x, size=k)
    center_y = np.random.uniform(low=min_y, high=max_y, size=k)
    centroids = np.stack([center_x, center_y], axis=-1)
    print("centroids 값")
    print(centroids)

    # (2) ~ (5) 순회
    num_data = dataset.shape[0]
    cluster_per_point = np.zeros((num_data))  # 각 점 별 군집

    counter = 0
    while True:
        prev_cluster_per_point = cluster_per_point

        # (2) 거리 계산 |x_1-x_2| + |y_1-y_2|
        dists = []
        for center in centroids:
            # 각 중심점 2개 기준으로 각데이터 간 거리 계산
            dist_from_center = []
            for point in dataset:
                dist = np.sum(np.abs(center - point))
                dist_from_center.append(dist)
            dists.append(dist_from_center)
        dists = np.array(dists)
       
        # (3) 각 데이터를 거리가 가장 가까운 군집으로 할당
        cluster_per_point = dists.argmin(axis=0)
        # (4) 각 군집 별 점들의 평균을 계산 후, 군집의 중심점을 다시 계산
        for i in range(k):
             centroids[i] = np.median(dataset[cluster_per_point == i], axis=0)
        print("cluster_per_point")
        print(cluster_per_point)

        if np.all(prev_cluster_per_point == cluster_per_point):
            break

        counter += 1
        plt.title("{}th Distribution of Dataset".format(counter))
        for idx, color in enumerate(['r', 'g', 'b', 'y']):
            mask = (cluster_per_point == idx)
            plt.scatter(dataset[mask, 0], dataset[mask, 1],
                        label='dataset', c=color)
            plt.scatter(centroids[:, 0], centroids[:, 1],
                        s=200, label="centroid", marker='+')
        plt.show()

    return centroids

print(cluster_kmeans(dataset,2))

\*\* k-means 알고리즘 숙제를 하면서 느낀점 \*\*
데이터의 퍼져있는 정도를 육안으로 볼때와 실제 컴퓨터가 알고리즘을 통해서 나오는
군집의 결과를 볼때 결과값이 달랐다. k-means 알고리즘은 평균값을 대표값 즉 군집으로 설정하기 때문에 이상값들에 취약한 경향을 뛰지만 k-median 알고리즘은 중앙값을 대표값으로 가져가기 때문에 오차가 줄어든거 같다. 
정확히 어떤 데이터에 어떤 올바른 거리계산 및 알고리즘을 적용할지는 배우는 단계라서 감을 잡을 수 없었지만 이 숙제를 하면서 깨달은 것은
중앙값은 평균에 비해 이상값(outlier)에 영향을 확실히 덜받는 것으로 결과가 나왔다.
궁금한점은 거리계산에서 median의 거리계산 속도가 means의 거리속도와 같지 않고 연산속도가 더 걸린다면 사용해야 할가?? 결과는 올바르게 나오는대.. 라는 궁금증이 들긴했었다.
