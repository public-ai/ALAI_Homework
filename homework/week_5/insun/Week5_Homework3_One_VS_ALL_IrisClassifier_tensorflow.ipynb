{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week5_Homework3_One_VS_ALL_IrisClassifier_tensorflow","version":"0.3.2","provenance":[{"file_id":"1OAJSCIgm1TBnZMd2N-d30l9ZXeN7mNNn","timestamp":1557919746950},{"file_id":"https://github.com/public-ai/ALAI_Homework/blob/master/homework/week_5/One_VS_ALL_IrisClassifier.ipynb","timestamp":1556353306582}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"je1nCDIAtS2e"},"source":[" ╔══<i><b>&nbsp;Alai-DeepLearning&nbsp;</b></i>══════════════════════════════════╗\n","###  &nbsp;&nbsp; **✎&nbsp;&nbsp;Week 5. machine learning basis**\n","# Homework 2. Multi classification\n","\n","### _Objective_\n","Tensorflow 을 이용해서 One Vs All 전략을 이용해 다중 classification 모델을 생성합니다.\n","\n","아래 순서에 맞게 프로그램을 작성해 주세요.\n","1. Setosa  Vs Versicolour, Virginica 모델 구현\n"," - 학습시 Tensorboard 을 이용해 loss 와 accuracy 을 추적해 주세요. \n"," - 학습 동안 acc 가 가장 높은 모델을 저장해 주세요.\n","2. Versicolour Vs Setosa, Virginica 모델 구현\n","  - 학습시 Tensorboard 을 이용해 loss 와 accuracy 을 추적해 주세요. \n"," - 학습 동안 acc 가 가장 높은 모델을 저장해 주세요.\n","3. Virginica Vs Setosa, Versicolour 모델 구현\n","  - 학습시 Tensorboard 을 이용해 loss 와 accuracy 을 추적해 주세요. \n"," - 학습 동안 acc 가 가장 높은 모델을 저장해 주세요.\n","4. 저장된 3개의 모델을 불러온 후 가장 높은 확률이 나오는 값을 선택합니다. <br>\n","가령 아래의 경우 **Versicolour** 을 선택합니다.\n","\n","| class        | probabilty |\n","|--------------|------------|\n","| Setosa       | 0.7        |\n","| Versicolour  | 0.9        |\n","| Virginica    | 0.3        |\n","\n","학습이 끝난후 모든 데이터를 평가한 후 accuracy 을 측정합니다.\n","\n","╚═══════════════════════════════════════════════╝"]},{"cell_type":"code","metadata":{"id":"Z-Ah8f58KJ0j","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.datasets import load_iris"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDtFr5kC5i_W","colab_type":"code","colab":{}},"source":["class Layer() :\n","    def forward(X) :\n","        raise NotImprementedError\n","        \n","    def backward(y_pred, y_label) :\n","        raise NotImprementedError\n","        \n","    def update(learning_rate) :\n","        pass\n","    \n","    \n","class LogitsLayer(Layer) :\n","    def __init__(self, weights, bias) :\n","        self.weights = weights\n","        self.bias = bias\n","        \n","        self.d_w = 0\n","        self.d_b = 0\n","        self.X = 0\n","        \n","    def forward(self, X) :\n","        self.X = X\n","        logits = tf.matmul(X, self.weights) + self.bias\n","        return logits\n","    \n","    def backward(self, grad) :\n","        self.d_w = tf.matmul(tf.transpose(self.X), grad)\n","        self.d_b = tf.reduce_sum(grad, axis=0)\n","        self.d_x = tf.matmul(grad, tf.transpose(self.weights))\n","        return self.d_x\n","        \n","    def update(self, learning_rate) :\n","        tf.assign_sub(self.weights, self.d_w * learning_rate)\n","        tf.assign_sub(self.bias, self.d_b * learning_rate)\n","        \n","        \n","class ReluLayer(Layer) :\n","    def __init__(self) :\n","        self.X = 0\n","        \n","    def forward(self, X) :\n","        self.X = tf.cast(X > 0, tf.float64)\n","        logits = self.X\n","        return logits\n","    \n","    def backward(self, grad) :\n","        grad = grad * self.X \n","        return grad    \n","\n","    def update(self, learning_rate) :\n","        pass\n","    \n","    \n","class SigmoidWithLossLayer(Layer) :\n","    def __init__(self) :\n","        self.y_pred = 0\n","        self.y_label = 0\n","        \n","    def forward(self, logits, y_label) : \n","        y_pred = 1 / ( 1 + tf.exp(-logits))\n","        loss = -tf.reduce_mean( y_label*tf.log(y_pred) + (1-y_label)*(tf.log(1-y_pred)) ) # element wise\n","        self.y_pred = y_pred\n","        self.y_label = y_label\n","        return loss\n","        \n","    def backward(self, grad) : # dataset_size를 잘못 구했었음.\n","        ## input shape : y_pred, y.lable -> (None, 1)\n","        ## output shape : (None, 1)\n","        grad = tf.reshape(self.y_pred, [-1, 1]) - tf.reshape(self.y_label, [-1, 1]) # broad casting방지\n","        dataset_size = tf.cast(self.y_pred.shape[0], tf.float64)\n","        loss_gradient = grad / dataset_size   \n","        return grad * loss_gradient   \n","\n","    def update(self, learning_rate) :\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol3u09WU5mfu","colab_type":"code","colab":{}},"source":["class dataFrame() :\n","    def __init__(self) :\n","        np.random.seed(1)\n","        iris = load_iris()\n","        xs = iris['data']\n","        ys = iris['target']\n","        ys_name = iris['target_names']\n","        \n","        concat = np.concatenate([xs, np.reshape(ys, [-1, 1])], axis=1)\n","        column = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"target\"]\n","        df = pd.DataFrame(concat, columns=column) \n","        \n","        temp = df.loc[:,\"sepal_length\":\"petal_width\"]\n","        df.loc[:,\"sepal_length\":\"petal_width\"] = (temp - temp.min()) / (temp.max() - temp.min())\n","        self.xs = df.loc[:,\"sepal_length\":\"petal_width\"].values\n","        self.ys = df.loc[:,\"target\"].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsQFzsXim3aU","colab_type":"code","colab":{}},"source":["class Classifier(dataFrame) : # dataFrame 상속받음.\n","    def __init__(self, one):  \n","        \n","        self.graph = tf.Graph()\n","        \n","        with self.graph.as_default() :\n","            \n","            ## dataFrame 상속받음 \n","            super(Classifier, self).__init__()\n","            \n","            with tf.name_scope('variables_initial') :\n","                # target을 받아서 label을 치환한다. (1vs 2,3)    \n","                mask_ys_label = tf.equal(self.ys, (tf.ones_like(self.ys, dtype=tf.float64) * one)) # mask_ys_label.shape = (150, )\n","                self.ys_label = tf.cast(mask_ys_label, tf.float64) \n","                self.X = self.xs # dataFrame에서 상속받은 xs임.\n","\n","                # variable initialize\n","                num_in = 4\n","                num_out = 100\n","                self.weights1 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_in, num_out)), dtype=tf.float64, name='weights1')\n","                self.bias1 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_out)), dtype=tf.float64, name='bias1')\n","                \n","                num_in = num_out\n","                num_out = 100                \n","                self.weights2 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_in, num_out)), dtype=tf.float64, name='weights2')\n","                self.bias2 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_out)), dtype=tf.float64, name='bias2')\n","\n","                num_in = num_out\n","                num_out = 1                \n","                self.weights3 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_in, num_out)), dtype=tf.float64, name='weights3')\n","                self.bias3 = tf.Variable(np.random.normal(loc=0.0, scale=0.1, size=(num_out)), dtype=tf.float64, name='bias3')\n","                \n","            with tf.name_scope('graph_initial') :\n","                self.sess = tf.Session()\n","                self.sess.run(tf.global_variables_initializer())\n","            \n","            with tf.name_scope('make_layers') : \n","                # instance 생성(initial)\n","                self.layers = []\n","                self.layers.append(LogitsLayer(self.weights1, self.bias1)) \n","                self.layers.append(ReluLayer())            \n","                self.layers.append(LogitsLayer(self.weights2, self.bias2)) \n","                self.layers.append(ReluLayer())            \n","                self.layers.append(LogitsLayer(self.weights3, self.bias3)) \n","                \n","                self.loss_layer = SigmoidWithLossLayer()\n","        \n","    def forward_Layer(self) :\n","        X_ = self.X \n","        for layer in self.layers :\n","            X_ = layer.forward(X_)\n","        return X_\n","    \n","    def backward_Layer(self, grad) :\n","        grad_ = grad\n","        for layer in self.layers[::-1] :\n","            grad_ = layer.backward(grad_)\n","        \n","    def update_Layer(self, learning_rate) :\n","        for layer in self.layers :\n","            layer.update(learning_rate)\n","        return tf.constant(0, dtype=tf.float64) # 아... 이걸 할필요가 없는데 에러가 발생한다. 함수 밖에 놔도 될듯..\n","            \n","    def prediction(self) :\n","        logits = self.forward_Layer()\n","        pred = tf.reshape(1 / ( 1 + tf.exp(-logits)), [-1])\n","        cut_value = tf.ones_like(pred, dtype=tf.float64) * 0.7        \n","        mask_pred = tf.cast(pred > cut_value, tf.float64)        \n","        acc = tf.reduce_mean(tf.cast(tf.equal(mask_pred, self.ys_label), tf.float64))\n","        return acc\n","        \n","    def train(self, learning_rate = 0.01) :\n","        with self.graph.as_default() :\n","\n","            # forward propagation\n","            logits = self.forward_Layer()\n","            loss = self.loss_layer.forward(logits, self.ys_label) # logits = (150, 1), loss=() ys_label=()\n","            \n","            # back propagation\n","            grad = 1\n","            grad = self.loss_layer.backward(grad)\n","            self.backward_Layer(grad)    \n","            \n","            # update weights\n","            update = self.update_Layer(learning_rate)\n","\n","            # calcurate accuracy\n","            acc = self.prediction()   \n","            \n","            # train\n","            train_op = tf.group([logits, loss, grad, update, acc], name='train_op')\n","            _, acc_, loss_= self.sess.run([train_op, acc, loss])\n","            \n","            loss_mean = self.sess.run(tf.reduce_mean(loss_, axis=0))\n","            acc_loss = np.stack( [acc_, loss_mean])            \n","\n","            return acc_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdcKry8N5qhP","colab_type":"code","colab":{}},"source":["class OVA() :\n","    def __init__(self, df) :\n","        self.df = df\n","        self.c1 = Classifier(0)\n","        self.c2 = Classifier(1)\n","        self.c3 = Classifier(2)\n","        \n","    def train_all(self, learning_rate=0.01, iteration=1) :\n","        self.iteration = iteration\n","        \n","        self.acc_list1 = []\n","        self.acc_list2 = []\n","        self.acc_list3 = []\n","        self.loss_list1 = []\n","        self.loss_list2 = []\n","        self.loss_list3 = []\n","    \n","        for i in range(iteration) :\n","            self.acc_loss1 = self.c1.train(learning_rate)\n","            self.acc_loss2 = self.c2.train(learning_rate)\n","            self.acc_loss3 = self.c3.train(learning_rate)\n","            self.acc_list1.append(self.acc_loss1[0])\n","            self.acc_list2.append(self.acc_loss2[0])\n","            self.acc_list3.append(self.acc_loss3[0])\n","            self.loss_list1.append(self.acc_loss1[1])\n","            self.loss_list2.append(self.acc_loss2[1])\n","            self.loss_list3.append(self.acc_loss3[1])\n","        \n","        \n","    def show_result(self) :\n","        \n","        fig = plt.figure(figsize=(15, 15))\n","        \n","        ax = fig.add_subplot(5,5,1)\n","        ax.plot(np.arange(0, self.iteration, 1), self.acc_list1, label=\"accuracy\", color='blue')\n","        ax.plot(np.arange(0, self.iteration, 1), self.loss_list1, label=\"loss\", color='red')\n","        ax.legend()\n","        ax.set_ylim(0, 1.2)\n","        ax.set_title(\"Setosa\")\n","        \n","        ax = fig.add_subplot(5,5,2)\n","        ax.plot(np.arange(0, self.iteration, 1), self.acc_list2, label=\"accuracy\", color='blue')\n","        ax.plot(np.arange(0, self.iteration, 1), self.loss_list2, label=\"loss\", color='red')\n","        ax.legend()\n","        ax.set_ylim(0, 1.2)\n","        ax.set_title(\"Versicolour\")\n","    \n","        ax = fig.add_subplot(5,5,3)\n","        ax.plot(np.arange(0, self.iteration, 1), self.acc_list3, label=\"accuracy\", color='blue')\n","        ax.plot(np.arange(0, self.iteration, 1), self.loss_list3, label=\"loss\", color='red')\n","        ax.legend()\n","        ax.set_ylim(0, 1.2)\n","        ax.set_title(\"Virginica\")\n","        \n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbGsDjfY5rvw","colab_type":"code","outputId":"d781cdb0-d6e9-4576-c65b-242728562933","executionInfo":{"status":"ok","timestamp":1557922347187,"user_tz":-540,"elapsed":8835,"user":{"displayName":"황인선","photoUrl":"","userId":"02350663787870167266"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["def main() :\n","    df = dataFrame()   \n","    ova = OVA(df)\n","    ova.train_all(learning_rate=0.01, iteration=10)\n","    ova.show_result()\n","    \n","if __name__ == '__main__':\n","    main()           "],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAhMAAAC7CAYAAADfcxyOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHJZJREFUeJzt3X+cVXW97/HXO8BIMESdUBlULDqK\nyA8d0TyFdM1CM8E6lhzLwB/k6do9Hk2PlpXXfJxKT+m1S9fmnkip/MHR7MFNiuKmB7uCMhj4CzUk\nPAx5EgFRUoofn/vHWnA20wx7z6y1fyzm/Xw85sHstb57rc8s3mw+811rr62IwMzMzKyn3lLvAszM\nzKzY3EyYmZlZJm4mzMzMLBM3E2ZmZpaJmwkzMzPLxM2EmZmZZeJmwsy6RdLTkiZm3MZDki7KqSTr\nZSTdJulLtXh+1n31Fn3rXcDeRtJ7gRuBY4DtwArgsohYUuZ5AYyIiJXVr9L2NpJ+DjwWEV/usHwy\n8F2gOSK25bGviDgmj+2YdaXaeY6IS6oxtjfzzESOJL0d+CnwbeAAYCjw34E/1bMu6xXuAD4pSR2W\nfwr4UXdeeCUV9peMItduu+lxnp2B+nAzka93A0TEXRGxPSLejIhfRMQTAJIukLRC0kZJ8yUdni5f\nmD5/uaTNkj6RLr9Y0kpJGyTNlXRoulySbpb0sqTXJD0paVS67sOSfpMuXyPpulofBKuLnwAHAu/b\nuUDSYOBMYLakt0r6Z0n/LukP6dTt29JxEyW1S/pHSf8BfF/SQZJ+KunVNH8PS3pLOn61pA+k3/eR\n9AVJL0h6XdJSScPSdSdLWiJpU/rnyZ0VLuktkq6V9GKa6dmSBpXW1mF86f6vk3SvpB9Keg2YlutR\ntXopl+fbJd2QLv+L/KbLr5L0kqTfS7pIUkh6V7qus+dfkebvJUnTS/a7a2z6eLKkZelr7AuSJqXL\np6ev769LWiXpM1U/Sg3EzUS+nge2S7pD0ulp+IFd03NfAD4KNAEPA3cBRMSEdNiYiBgYEfdI+i/A\n14CPA4cALwJ3p+M+CEwgaV4GpWPWp+v+CJwP7A98GPg7SVOq9PNag4iIN4E5JH/3O30ceDYilgNf\nJ8nLWOBdJLNmpVPIB5PMph0OzACuANpJsjqEJLud3Xv/cmAqcAbwduAC4A1JBwAPALeS/KfwLeAB\nSQd2so1p6df7gSOBgcD/7MaPPxm4lyTzP+rG86xBVZDnjnbLb/of/OXAB0jyPrHMLg8meS0dClwI\nzCx9/d5J0nhgNnAlSd4mAKvT1S+TNDtvB6YDN0s6rsx+9xpuJnIUEa8B7yV50f3fwLp0RmEIcAnw\ntYhYkU7R/RMwdufsRCfOA2ZFxOMR8SfgGuA9ko4AtgL7AUcBSrf5UlrDQxHxZETsSGdE7gJOqdbP\nbA3lDuBvJPVPH58P3JFOFc8A/iEiNkTE6yT5O7fkuTuAr0TEn9IX8q0kTezhEbE1Ih6Ozj/I5yLg\n2oh4LhLLI2I9SSP724j4QURsi4i7gGeBj3SyjfOAb0XEqojYTJL1c7sxXb0oIn6SZv7NCp9jja/T\nPHcxtmN+Pw58PyKejog3gOvK7GsrcH2a9XnAZuCvOhl3Icnr8i/TvK2NiGcBIuKBiHgh/Xfwb8Av\nKJlZ2du5mchZ+h/7tIhoBkYBhwK3kHTM/yOdNn4V2ACIpBPuzKEksxE7t7uZZPZhaET8iuQ3t5nA\ny5JalVyvgaQTJT0oaZ2kTSRNzEFV+WGtoUTEr4FXgCmS3gmMB+4kmV3YF1hakr+fp8t3WhcRW0oe\n3wSsBH6RTtle3cVuhwEvdLJ8t/ymXqTzvHcc+yLJxeFDuthnR2sqHGcFsoc8d6Zjfg9l91yUy8j6\nDtdhvEEyQ9ZRV3knnY1enJ4WfJVktq7XvPa6maiitGO9naSpWAN8JiL2L/l6W0Q80sXTf0/SgAAg\naQDJdPHadNu3RsTxwEiS6esr06F3AnOBYRExCLiNpGmx3mE2yW9wnwTmR8QfSF6Q3wSOKcneoIgo\nfbHcbdYhIl6PiCsi4kjgLOBySad2sr81wDs7Wb5bflOHkea3zNjDgG3AH0hO2+27c4WkPuzeBP1F\n7bZX6SzPnemYgZeA5pLHw3Kqp9O8S3orcB/wz8CQiNgfmEcveu11M5EjSUelF/E0p4+HkZxPXkzy\nn/o1ko5J1w2SdE7J0/9Acr54p7uA6ZLGpkH9J+DRiFgt6YR0BqIfyYvtFpJpPkhOf2yIiC3p+b2/\nrd5PbA1oNsl54otJp4QjYgfJabebJb0DQNJQSR/qaiOSzpT0rvQUySaStznv6GTovwBflTRCidHp\ndRHzgHdL+ltJfZVcVDyS5N1OHd0F/IOk4ZIGkmT9nvQ3xeeB/kouLO4HXAu8tfuHxQrqL/JcoTkk\nr59HS9oXyOs+Ed9Lt3uqkguHh0o6CtiHJJfrgG2STie5tq3XcDORr9eBE4FHJf2RpIl4CrgiIu4H\nvgHcnV51/hRweslzryM5v/2qpI9HxAKSfwD3kXTZ7+Q/z3G/neQ/h40kU8LrSaalAT4LXC/pdZIL\n7OZU6We1BhQRq4FHgAEkM1Q7/SPJaYvFaf4W0Pk54Z1GpGM2A4uA70TEg52M+xZJxn4BvEbyYvu2\n9LqJM0ku5FwPXAWcGRGvdLKNWcAPgIXA70ia48+lP88mkkz/C8msxh9JLgy1XmAPeS73vJ+RXPz7\nIGnu01WZ3qYfEY+RXlxJ0mT/G8l1Ra8D/43k38JGkl/iKq53b6DOr6kyMzPbO0g6muQXuLfmdfM2\n251nJszMbK8j6Wwl91cZTDIr/H/cSFRP2WZC0qz0Rh5PdbH+PElPKLlx0iOSxuRfplnlnFkrIuc2\nd58huffDCyTX/PxdfcvZu5U9zSFpAsl509kRMaqT9ScDKyJiY3rRyXURcWJVqjWrgDNrReTcWpGV\nvSlMRCxMb5TU1frStzYuZve345jVnDNrReTcWpHl/YEoFwI/62qlpBkkd+JjwIABxx911FE57956\nk6VLl74SER3vOdBdzqzVTE6ZBefWaqTSzObWTEh6P0nA39vVmIhoBVoBWlpaoq2tLa/dWy8kqeMd\nFrv7fGfWaiprZtNtOLdWM5VmNpdmQtJokveBn56+v9ysoTmzVkTOrTWqzG8NlXQY8GPgUxHxfPaS\nzKrLmbUicm6tkZWdmZB0F8nHtx4kqR34CtAPICJuI7nL4oHAd5I777ItIlqqVbBZOc6sFZFza0VW\nybs5ppZZfxHJxxBbJ7Zu3Up7eztbtmwpP9g61b9/f5qbm+nXr19F453Z7JzbbLqbWXBus3Jms+lJ\nZkvl/W4O66C9vZ399tuPI444gvS3CeuGiGD9+vW0t7czfPjwepfTazi3PefM1ocz23N5ZNa3066y\nLVu2cOCBBzrcPSSJAw880L9t1Jhz23PObH04sz2XR2bdTNSAw52Nj199+Lj3nI9dffi491zWY+dm\nwszMzDJxM2FmZmaZuJmw3Gzb5k/3tWJxZq1oGjWzbiZ6iSlTpnD88cdzzDHH0NraCsDPf/5zjjvu\nOMaMGcOpp54KwObNm5k+fTrHHnsso0eP5r777gNg4MCBu7Z17733Mm3aNACmTZvGJZdcwoknnshV\nV13FY489xnve8x7GjRvHySefzHPPPQfA9u3b+fznP8+oUaMYPXo03/72t/nVr37FlClTdm33l7/8\nJWeffXYtDocVgDNrRdObM+u3htbQZZfBsmX5bnPsWLjllvLjZs2axQEHHMCbb77JCSecwOTJk7n4\n4otZuHAhw4cPZ8OGDQB89atfZdCgQTz55JMAbNy4sey229vbeeSRR+jTpw+vvfYaDz/8MH379mXB\nggV84Qtf4L777qO1tZXVq1ezbNky+vbty4YNGxg8eDCf/exnWbduHU1NTXz/+9/nggsuyHQ8LH/1\nyq0zaz3lzNY+s24meolbb72V+++/H4A1a9bQ2trKhAkTdr2n+IADDgBgwYIF3H333bueN3jw4LLb\nPuecc+jTpw8AmzZt4tOf/jS//e1vkcTWrVt3bfeSSy6hb9++u+3vU5/6FD/84Q+ZPn06ixYtYvbs\n2Tn9xFZ0zqwVTW/OrJuJGqpkBqEaHnroIRYsWMCiRYvYd999mThxImPHjuXZZ5+teBulbxvq+F7k\nAQMG7Pr+S1/6Eu9///u5//77Wb16NRMnTtzjdqdPn85HPvIR+vfvzznnnLPrH4E1jnrk1pm1LJzZ\n3dUis75mohfYtGkTgwcPZt999+XZZ59l8eLFbNmyhYULF/K73/0OYNf022mnncbMmTN3PXfn9NuQ\nIUNYsWIFO3bs2NV5d7WvoUOHAnD77bfvWn7aaafx3e9+d9fFQzv3d+ihh3LooYdyww03MH369Px+\naCs0Z9aKprdn1s1ELzBp0iS2bdvG0UcfzdVXX81JJ51EU1MTra2tfPSjH2XMmDF84hOfAODaa69l\n48aNjBo1ijFjxvDggw8C8PWvf50zzzyTk08+mUMOOaTLfV111VVcc801jBs3brerji+66CIOO+ww\nRo8ezZgxY7jzzjt3rTvvvPMYNmwYRx99dJWOgBWNM2tF09szq4ioyobLaWlpiba2trrsu5ZWrFjh\nF5wyLr30UsaNG8eFF17Y5ZjOjqOkpbX81MTekllwbsspSmah9+TWmd2zamfWJ/usro4//ngGDBjA\nN7/5zXqXYlYRZ9aKphaZdTNhdbV06dJ6l2DWLc6sFU0tMlv2mglJsyS9LOmpLtZL0q2SVkp6QtJx\n+Zdp1j3OrRWNM2tFVskFmLcDk/aw/nRgRPo1A/hf2csyy+x2nFsrlttxZq2gyjYTEbEQ2LCHIZOB\n2ZFYDOwvqevLUM1qwLm1onFmrcjyeGvoUGBNyeP2dNlfkDRDUpuktnXr1uWwa7Meqyi3zqw1EL/W\nWsOq6X0mIqI1IloioqWpqamWu+7VSj88xrrHma0f57bnnNv66M2ZzaOZWAsMK3ncnC4za2TOrRWN\nM2sNK49mYi5wfnql8UnApoh4KYftWs4igiuvvJJRo0Zx7LHHcs899wDw0ksvMWHCBMaOHcuoUaN4\n+OGH2b59O9OmTds19uabb65z9blzbgvCud3FmS2I3pjZsveZkHQXMBE4SFI78BWgH0BE3AbMA84A\nVgJvAL5ZfVfq+RnkwI9//GOWLVvG8uXLeeWVVzjhhBOYMGECd955Jx/60If44he/yPbt23njjTdY\ntmwZa9eu5amnknepvfrqq/nWXWXObY6c25pwZnPkzNZc2WYiIqaWWR/Af82tIquaX//610ydOpU+\nffowZMgQTjnlFJYsWcIJJ5zABRdcwNatW5kyZQpjx47lyCOPZNWqVXzuc5/jwx/+MB/84AfrXX63\nOLd7j96SW2d279FbMlvKd8CspXp9BnkZEyZMYOHChTzwwANMmzaNyy+/nPPPP5/ly5czf/58brvt\nNubMmcOsWbPqXarVg3NrRePM1pw/NbQXed/73sc999zD9u3bWbduHQsXLmT8+PG8+OKLDBkyhIsv\nvpiLLrqIxx9/nFdeeYUdO3bwsY99jBtuuIHHH3+83uVbL+XcWtH0xsx6ZqIXOfvss1m0aBFjxoxB\nEjfeeCMHH3wwd9xxBzfddBP9+vVj4MCBzJ49m7Vr1zJ9+nR27NgBwNe+9rU6V2+9lXNrRdMbM+uP\nIK8yfyxuPhrh45x7S2bBuc1DI2QWek9undnssmTWpznMzMwsEzcTZmZmlombiRqo16mkvYWPX334\nuPecj119+Lj3XNZj52aiyvr378/69esd8h6KCNavX0///v3rXUqv4tz2nDNbH85sz+WRWb+bo8qa\nm5tpb2/Hn9zXc/3796e5ubneZfQqzm02zmztObPZZM2sm4kq69evH8OHD693GWbd4txa0Tiz9eXT\nHGZmZpaJmwkzMzPLxM2EmZmZZeJmwszMzDKpqJmQNEnSc5JWSrq6k/WHSXpQ0m8kPSHpjPxLNauc\nM2tF48xakZVtJiT1AWYCpwMjgamSRnYYdi0wJyLGAecC38m7ULNKObNWNM6sFV0lMxPjgZURsSoi\n/gzcDUzuMCaAt6ffDwJ+n1+JZt3mzFrROLNWaJU0E0OBNSWP29Nlpa4DPimpHZgHfK6zDUmaIalN\nUptvLGJV5Mxa0eSWWXBurfbyugBzKnB7RDQDZwA/kPQX246I1ohoiYiWpqamnHZt1iPOrBVNRZkF\n59Zqr5JmYi0wrORxc7qs1IXAHICIWAT0Bw7Ko0CzHnBmrWicWSu0SpqJJcAIScMl7UNy4c/cDmP+\nHTgVQNLRJCH33JrVizNrRePMWqGVbSYiYhtwKTAfWEFyNfHTkq6XdFY67ArgYknLgbuAaeGPbrM6\ncWataJxZK7qKPugrIuaRXPBTuuzLJd8/A/x1vqWZ9Zwza0XjzFqR+Q6YZmZmlombCTMzM8vEzYSZ\nmZll4mbCzMzMMnEzYWZmZpm4mTAzM7NM3EyYmZlZJm4mzMzMLBM3E2ZmZpZJRXfArKXLLoNly+pd\nhTWKsWPhllvqXcWeObNWypm1oskjs56ZMDMzs0wabmbiFi4D3DLbTmOBxv41z5m13TmzVjTZM+uZ\nCTMzM8uk4WYmGv5ko1lHzqwVjTNrOatoZkLSJEnPSVop6eouxnxc0jOSnpZ0Z75lmnWPM2tF48xa\nkZWdmZDUB5gJnAa0A0skzY2IZ0rGjACuAf46IjZKeke1CjYrx5m1onFmregqmZkYD6yMiFUR8Wfg\nbmByhzEXAzMjYiNARLycb5lm3eLMWtE4s1ZolTQTQ4E1JY/b02Wl3g28W9L/k7RY0qS8CjTrAWfW\nisaZtULL6wLMvsAIYCLQDCyUdGxEvFo6SNIMYAbAYYcdltOuzXrEmbWiqSiz4Nxa7VUyM7EWGFby\nuDldVqodmBsRWyPid8DzJKHfTUS0RkRLRLQ0NTX1tGazcpxZK5rcMgvOrdVeJc3EEmCEpOGS9gHO\nBeZ2GPMTkm4ZSQeRTMetyrFOs+5wZq1onFkrtLLNRERsAy4F5gMrgDkR8bSk6yWdlQ6bD6yX9Azw\nIHBlRKyvVtFme+LMWtE4s1Z0ioi67LilpSXa2trqsm/bO0haGhEttdqfM2tZ1Tqz4NxaNpVm1rfT\nNjMzs0zcTJiZmVkmbibMzMwsEzcTZmZmlombCTMzM8vEzYSZmZll4mbCzMzMMnEzYWZmZpm4mTAz\nM7NM3EyYmZlZJm4mzMzMLBM3E2ZmZpaJmwkzMzPLxM2EmZmZZeJmwszMzDKpqJmQNEnSc5JWSrp6\nD+M+Jikklf3sc7NqcmataJxZK7KyzYSkPsBM4HRgJDBV0shOxu0H/D3waN5FmnWHM2tF48xa0VUy\nMzEeWBkRqyLiz8DdwOROxn0V+AawJcf6zHrCmbWicWat0CppJoYCa0oet6fLdpF0HDAsIh7Y04Yk\nzZDUJqlt3bp13S7WrELOrBVNbplNxzq3VlOZL8CU9BbgW8AV5cZGRGtEtERES1NTU9Zdm/WIM2tF\n053MgnNrtVdJM7EWGFbyuDldttN+wCjgIUmrgZOAub44yOrImbWicWat0CppJpYAIyQNl7QPcC4w\nd+fKiNgUEQdFxBERcQSwGDgrItqqUrFZec6sFY0za4VWtpmIiG3ApcB8YAUwJyKelnS9pLOqXaBZ\ndzmzVjTOrBVd30oGRcQ8YF6HZV/uYuzE7GWZZePMWtE4s1ZkvgOmmZmZZeJmwszMzDJxM2FmZmaZ\nuJkwMzOzTNxMmJmZWSZuJszMzCwTNxNmZmaWiZsJMzMzy8TNhJmZmWXiZsLMzMwycTNhZmZmmbiZ\nMDMzs0zcTJiZmVkmbibMzMwsk4qaCUmTJD0naaWkqztZf7mkZyQ9Ien/Sjo8/1LNKufMWtE4s1Zk\nZZsJSX2AmcDpwEhgqqSRHYb9BmiJiNHAvcCNeRdqViln1orGmbWiq2RmYjywMiJWRcSfgbuByaUD\nIuLBiHgjfbgYaM63TLNucWataJxZK7RKmomhwJqSx+3psq5cCPyssxWSZkhqk9S2bt26yqs06x5n\n1oomt8yCc2u1l+sFmJI+CbQAN3W2PiJaI6IlIlqampry3LVZjzizVjTlMgvOrdVe3wrGrAWGlTxu\nTpftRtIHgC8Cp0TEn/Ipz6xHnFkrGmfWCq2SmYklwAhJwyXtA5wLzC0dIGkc8F3grIh4Of8yzbrF\nmbWicWat0Mo2ExGxDbgUmA+sAOZExNOSrpd0VjrsJmAg8K+Slkma28XmzKrOmbWicWat6Co5zUFE\nzAPmdVj25ZLvP5BzXWaZOLNWNM6sFZnvgGlmZmaZuJkwMzOzTNxMmJmZWSZuJszMzCwTNxNmZmaW\niZsJMzMzy8TNhJmZmWXiZsLMzMwycTNhZmZmmbiZMDMzs0zcTJiZmVkmbibMzMwsEzcTZmZmlomb\nCTMzM8vEzYSZmZllUlEzIWmSpOckrZR0dSfr3yrpnnT9o5KOyLtQs+5wZq1onFkrsrLNhKQ+wEzg\ndGAkMFXSyA7DLgQ2RsS7gJuBb+RdqFmlnFkrGmfWiq6SmYnxwMqIWBURfwbuBiZ3GDMZuCP9/l7g\nVEnKr0yzbnFmrWicWSu0vhWMGQqsKXncDpzY1ZiI2CZpE3Ag8ErpIEkzgBnpw82Snutinwd1fG6d\nNVo90Hg11aOew7tY7sw2Xj3QeDXtlZmFinPbaH8f0Hg1uZ6uM7ubSpqJ3EREK9BabpyktohoqUFJ\nFWm0eqDxamq0evLizOan0WpqtHryVEluG/Hnb7SaXE/lKjnNsRYYVvK4OV3W6RhJfYFBwPo8CjTr\nAWfWisaZtUKrpJlYAoyQNFzSPsC5wNwOY+YCn06//xvgVxER+ZVp1i3OrBWNM2uFVvY0R3pu7lJg\nPtAHmBURT0u6HmiLiLnA94AfSFoJbCD5h5BF2WnlGmu0eqDxamqYepxZoPHqgcarqWHqcWZ3abSa\nXE+F5MbWzMzMsvAdMM3MzCwTNxNmZmaWSV2biUa6faykYZIelPSMpKcl/X0nYyZK2iRpWfr15WrV\nU7LP1ZKeTPfX1sl6Sbo1PUZPSDquirX8VcnPvkzSa5Iu6zCm5seolpzZiupyZhuIM1tRXc5sVhFR\nly+Si4xeAI4E9gGWAyM7jPkscFv6/bnAPVWs5xDguPT7/YDnO6lnIvDTGh+n1cBBe1h/BvAzQMBJ\nwKM1/Pv7D+Dweh+jGv5dOLOV1eXMNsiXM1txXc5sxq96zkw01O1jI+KliHg8/f51YAXJHeca3WRg\ndiQWA/tLOqQG+z0VeCEiXqzBvhqFM5sPZ7Z2nNl8OLNl1LOZ6Oz2sR1DtdvtY4Gdt4+tqnSabxzw\naCer3yNpuaSfSTqm2rUAAfxC0lIlt8jtqJLjWA3nAnd1sa7Wx6hWnNnKOLONw5mtjDObUU1vp10E\nkgYC9wGXRcRrHVY/TjLdtFnSGcBPgBFVLum9EbFW0juAX0p6NiIWVnmfe6TkpjpnAdd0sroex6hX\nc2bLc2YbizNbXtEyW8+ZiYa7faykfiQB/1FE/Ljj+oh4LSI2p9/PA/pJOqha9aT7WZv++TJwP8m0\nZalKjmPeTgcej4g/dFxRj2NUQ85sBZzZhuLMVsCZza6ezURD3T42PUf4PWBFRHyrizEH7zyXKGk8\nyfGr5j+6AZL22/k98EHgqQ7D5gLnp1cbnwRsioiXqlVTaipdTL3V+hjVmDNbviZntrE4s+Vrcmbz\nUI+rPnd+kVwh+zzJ1cZfTJddD5yVft8f+FdgJfAYcGQVa3kvyXmzJ4Bl6dcZwCXAJemYS4GnSa6I\nXgycXOXjc2S6r+Xpfnceo9KaBMxMj+GTQEuVaxpAEtpBJcvqdoycWWfWmXVmndn6Z9a30zYzM7NM\nfAdMMzMzy8TNhJmZmWXiZsLMzMwycTNhZmZmmbiZMDMzs0zcTJiZmVkmbibMzMwsk/8PEE8Wne43\nk28AAAAASUVORK5CYII=\n","text/plain":["<Figure size 1080x1080 with 3 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8AeNTYRt8VXw"},"source":["#  \n","\n","---\n","\n","    Copyright(c) 2019 by Public AI. All rights reserved.\n","    Writen by PAI, SangJae Kang ( rocketgrowthsj@publicai.co.kr )  last updated on 2019/03/22\n","\n","---"]},{"cell_type":"code","metadata":{"id":"k9ul19Sx767I","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}